Early Childhood Longitudinal
Study, Kindergarten Class
of 2010–11 (ECLS-K:2011)
User’s Manual for the ECLS-K:2011 Kindergarten–Fifth Grade
Data File and Electronic Codebook, Public Version
NCES 2019-051
U.S. DEPARTMENT OF EDUCATIONEarly Childhood Longitudinal
Study, Kindergarten Class
of 2010–11 (ECLS-K:2011)
User’s Manual for the ECLS-K:2011 Kindergarten–Fifth Grade
Data File and Electronic Codebook, Public Version
JULY 2019
Karen Tourangeau
Christine Nord
Thanh Lê
Kathleen Wallner-Allen
Nancy Vaden-Kiernan
Lisa Blaker
Westat
Michelle Najarian
Educational Testing Service
Gail M. Mulligan
Project Officer
National Center for Education Statistics
NCES 2019-051
U.S. DEPARTMENT OF EDUCATIONU.S. Department of Education
Betsy DeVos
Secretary
Institute of Education Sciences
Mark Schneider
Director
National Center for Education Statistics
James L. Woodworth
Commissioner
The National Center for Education Statistics (NCES) is the primary federal entity for collecting,
analyzing, and reporting data related to education in the United States and other nations. It fulfills a
congressional mandate to collect, collate, analyze, and report full and complete statistics on the condition
of education in the United States; conduct and publish reports and specialized analyses of the meaning
and significance of such statistics; assist state and local education agencies in improving their statistical
systems; and review and report on education activities in foreign countries.
NCES activities are designed to address high-priority education data needs; provide consistent, reliable,
complete, and accurate indicators of education status and trends; and report timely, useful, and high-
quality data to the U.S. Department of Education, the Congress, the states, other education policymakers,
practitioners, data users, and the general public. Unless specifically noted, all information contained
herein is in the public domain.
We strive to make our products available in a variety of formats and in language that is appropriate to a
variety of audiences. You, as our customer, are the best judge of our success in communicating
information effectively. If you have any comments or suggestions about this or any other NCES product
or report, we would like to hear from you. Please direct your comments to
NCES, IES, U.S. Department of Education
550 12th Street SW
Washington, DC 20202
July 2019
The NCES Home Page address is https://nces.ed.gov.
The NCES Publications and Products address is https://nces.ed.gov/pubsearch.
This publication is only available online. To download, view, and print the report as a PDF file, go to the
NCES Publications and Products address shown above.
The Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011) has been
funded by the U.S. Department of Education, Institute of Education Sciences, under contract number ED-
IES-14-C-0119 with Westat. This report was produced under that contract. Mention of trade names,
commercial products, or organizations does not imply endorsement by the U.S. Government.
Suggested Citation
Tourangeau, K., Nord, C., Lê, T., Wallner-Allen, K., Vaden-Kiernan, N., Blaker, L. and Najarian, M.
(2019). Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011) User’s
Manual for the ECLS-K:2011 Kindergarten–Fifth Grade Data File and Electronic Codebook, Public
Version (NCES 2019-051). U.S. Department of Education. Washington, DC: National Center for
Education Statistics.
Content Contact
Gail M. Mulligan
(202) 245-8413
Gail.Mulligan@ed.govCONTENTS
Chapter Page
LIST OF TABLES ............................................................................................................................. x
LIST OF EXHIBITS ....................................................................................................................... xvi
1. INTRODUCTION ..................................................................................................... 1-1
1.1 Background ................................................................................................... 1-2
1.2 1.3 1.4 Periods of Data Collection ............................................................................ 1-3
Overview of the Fifth-Grade Round of Data Collection ............................... 1-4
ECLS-K:2011 Kindergarten–Fifth Grade (K–5) Public-Use Data
File ................................................................................................................ 1-5
1.5 Contents of Manual ....................................................................................... 1-6
2. DATA COLLECTION INSTRUMENTS AND METHODS ................................... 2-1
2.1 Data Collection Instruments ......................................................................... 2-1
2.2 2.1.1 Direct Child Assessment ............................................................... 2-3
2.1.2 Child Questionnaire ...................................................................... 2-8
2.1.3 Parent Interview .......................................................................... 2-11
2.1.4 General Classroom Teacher Questionnaires ............................... 2-14
2.1.5 Special Education Teacher Questionnaires ................................. 2-22
2.1.6 School Administrator Questionnaires ......................................... 2-24
2.1.7 Copyrighted Materials ................................................................. 2-27
Data Collection Methods ............................................................................ 2-28
2.2.1 Comparison of Data Collection Methods Used in Fifth
Grade to Those Used in Earlier Data Collection Rounds ............ 2-28
3. ECLS-K:2011 DIRECT AND INDIRECT ASSESSMENT DATA ......................... 3-1
3.1 Direct Cognitive Assessment: Reading, Mathematics, and Science ............. 3-2
3.1.1 IRT-Based Scores Developed for the ECLS-K:2011 .................... 3-2
3.1.1.1 Theta and the Standard Error of Measurement
(SEM) of Theta .............................................................. 3-3
3.1.1.2 Scale Scores ................................................................... 3-8
...................................................................
3.1.2 3.1.3 Variables Indicating Exclusion from the Direct Assessment
Due to Disability ......................................................................... 3-11
Choosing the Appropriate Score for Analysis ............................. 3-11
iiiCONTENTS—Continued
Chapter Page
4. 3.1.4 3.1.5 3.1.6 Analytic Considerations for Measuring Gains in the
ECLS-K:2011 ............................................................................... 3-12
Reliability of the ECLS-K:2011 Scores ....................................... 3-13
Validity of the ECLS-K:2011 Scores ........................................... 3-14
3.2 Direct Cognitive Assessment: Executive Function ...................................... 3-15
3.2.1 Dimensional Change Card Sort .................................................... 3-16
3.2.1.1 Dimensional Change Card Sort Data Flags .................. 3-24
3.2.2 Numbers Reversed ....................................................................... 3-26
3.2.2.1 Numbers Reversed Data Flags ..................................... 3-33
3.2.3 The NIH Toolbox Flanker Inhibitory Control and
Attention Task (Flanker) .............................................................. 3-34
3.2.3.1 Flanker Data Flag ......................................................... 3-40
3.3 Child Questionnaire ..................................................................................... 3-41
3.4 Teacher- and Parent-Reported Measures of Child Behavior and
Peer Relationships ........................................................................................ 3-44
3.4.1 3.4.2 3.4.3 3.4.4 3.4.5 Teacher-Reported Social Skills .................................................... 3-45
Teacher-Reported Approaches to Learning Items and
Scale ............................................................................................. 3-50
Teacher-Reported Attentional Focusing and Inhibitory
Control: Children’s Behavior Questionnaire (CBQ) and
Temperament in Middle Childhood Questionnaire
(TMCQ) ....................................................................................... 3-51
Teacher- and Parent-Reports of Children’s Peer
Relationships ................................................................................ 3-56
Teacher- and Parent-Reports of Children’s School
Liking and Avoidance .................................................................. 3-60
SAMPLE DESIGN AND SAMPLING WEIGHTS ................................................... 4-1
4.1 Sample Design ............................................................................................... 4-1
4.1.1 4.1.2 ECLS-K:2011 School Sample ........................................................ 4-2
The Sample of Children ................................................................. 4-4
ivCONTENTS—Continued
Chapter Page
4.2 Sample Design for the First- Through Fifth-Grade Years ............................. 4-7
4.2.1 4.2.2 Fall First Grade and Fall Second Grade ......................................... 4-7
Spring First Grade Through Spring Fifth Grade .......................... 4-14
4.2.3 Following Movers ........................................................................ 4-25
4.3 Calculation and Use of Sample Weights ..................................................... 4-28
4.3.1 4.3.2 Types of Sample Weights ............................................................ 4-29
Computation of Sample Weights ................................................. 4-39
4.3.2.1 4.3.2.2 Student Base Weights ................................................... 4-40
Student Weights Adjusted for Mover
Subsampling ................................................................. 4-40
4.3.2.3 Student Nonresponse-Adjusted Weights ...................... 4-41
4.3.2.4 Raking to Sample Control Totals ................................. 4-41
4.3.3 Characteristics of Sample Weights .............................................. 4-42
4.3.4 Variance Estimation ..................................................................... 4-44
4.3.4.1 Jackknife Method ......................................................... 4-44
4.3.4.2 Taylor Series Method ................................................... 4-45
4.3.4.3 Specifications for Computing Standard
Errors ............................................................................ 4-46
4.3.5 Use of Design Effects ................................................................... 4-47
5. RESPONSE RATES ................................................................................................... 5-1
5.1 Study Instruments .......................................................................................... 5-1
5.2 5.3 Unit Response Rates and Overall Response Rates ........................................ 5-2
Nonresponse Bias Analysis ......................................................................... 5-31
5.3.1 5.3.2 5.3.3 5.3.4 5.3.5 Effect of Nonresponse on Child Assessment Data ....................... 5-32
Effect of Nonresponse on Parent Interview Data ......................... 5-37
Effect of Nonresponse on Teacher Questionnaire Data ............... 5-43
Effect of Nonresponse on School Administrator
Questionnaire Data ....................................................................... 5-49
Effect of Nonresponse on Characteristics from the
Base Year ..................................................................................... 5-53
.....................................................................................
vCONTENTS—Continued
Chapter Page
6. DATA PREPARATION ............................................................................................. 6-1
7. 6.1 Coding Text Responses ................................................................................. 6-2
6.1.1 6.1.2 Household Roster Review .............................................................. 6-4
Partially Complete Parent Interviews ............................................. 6-5
6.2 Receipt, Coding, and Editing of Hard-Copy Questionnaires ......................... 6-5
6.2.1 Receipt Control .............................................................................. 6-5
6.2.2 6.2.3 Scanning of Hard-Copy Questionnaires ......................................... 6-5
Coding for Hard-Copy Questionnaires .......................................... 6-5
6.2.4 Data Editing ................................................................................... 6-6
DATA FILE CONTENT AND COMPOSITE VARIABLES .................................... 7-1
7.1 Variable Naming Conventions ....................................................................... 7-3
7.2 Identification Variables .................................................................................. 7-4
7.3 Missing Values .............................................................................................. 7-7
7.4 Data Flags .................................................................................................... 7-11
7.4.1 Child Assessment Flags (X9RDGFLG, X9MTHFLG,
X9SCIFLG, X9NRFLG, X9NRGEST, X9DCCSFLG,
X9FLNKFLG, X9HGTFLG, X9WGTFLG,
X9ASMTST, X9EXDIS, X9CQFLG) ......................................... 7-11
7.4.1.1 Child Theta Score Outlier (X_RTOFLG,
X
MTOFLG, X
_
_
STOFLG) .......................................... 7-13
7.4.2 Parent Data Flags (X9PARDAT, X9EDIT,
X9BRKFNL) ................................................................................ 7-14
7.4.3 Teacher Flags (X9TQTDAT, X9TQTZDAT,
X9TQRDGDAT, X9TQMTHDAT, X9TQSCIDAT,
X9MSFLAG, X9SETQA, X9SETQC) ........................................ 7-15
7.4.4 7.4.5 School Administrator Data Flag (X9INSAQ) .............................. 7-16
Child Destination School Flag (X9DEST) ................................... 7-17
7.5 Composite Variables .................................................................................... 7-17
7.5.1 Child Composite Variables .......................................................... 7-18
7.5.1.1 Child’s Date of Birth (X_DOBYY_R and
X_DOBMM_R) ............................................................ 7-18
viCONTENTS—Continued
Chapter Page
7.5.2 7.5.1.2 Child’s Age at Assessment and the Date of
Assessment (X9AGE, X9ASMTDD,
X9ASMTMM, X9ASMTYY) ...................................... 7-18
7.5.1.3 Child’s Sex (X_CHSEX_R) ......................................... 7-19
7.5.1.4 Race/Ethnicity (X_AMINAN_R,
X_ASIAN_R, X_HAWPI_R, X_BLACK_R,
X_WHITE_R, X_HISP_R, X_MULTR_R,
X_RACETHP_R, X_RACETH_R) .............................. 7-20
7.5.1.5 Child’s Height (X9HEIGHT) ....................................... 7-22
7.5.1.6 Child’s Weight (X9WEIGHT ....................................... 7-23
7.5.1.7 Child’s Body Mass Index (X9BMI) ............................. 7-24
7.5.1.8 Child’s Disability Status (X9DISABL2,
X9DISABL) ................................................................. 7-24
7.5.1.9 Primary Language in the Child’s Home
(X9LANGST) ............................................................... 7-26
7.5.1.10 Student Grade Level (X9GRDLVL) ............................ 7-26
7.5.1.11 Child Linked to a Special Education Teacher
(X9SPECS) ................................................................... 7-27
Family and Household Composite Variables ............................... 7-27
7.5.2.1 Household Counts (X9HTOTAL,
X9NUMSIB, X9LESS18, X9OVER18) ...................... 7-28
7.5.2.2 Household Rosters ........................................................ 7-29
7.5.2.3 Parent Identifiers and Type in the Household
(X9IDP1, X9IDP2, X9HPAR1, X9HPAR2,
X9HPARNT) ................................................................ 7-32
7.5.2.4 Parent Demographic Variables
(X9PAR1AGE, X9PAR2AGE,
X9PAR1RAC, X9PAR2RAC) ..................................... 7-37
7.5.2.5 Parent Education Variables (X9PAR1ED_I,
X9PAR2ED_I) ............................................................. 7-38
7.5.2.6 Parent Occupation Variables
(X9PAR1EMP_I, X9PAR2EMP_I,
X9PAR1OCC_I, X9PAR2OCC_I,
X9PAR1SCR_I, X9PAR2SCR_I) ................................ 7-39
7.5.2.7 Household Income and Poverty
(X9INCCAT_I, X9POVTY_I) ..................................... 7-47
7.5.2.8 7.5.2.9 Socioeconomic Status (SES) (X9SESL_I) ................... 7-50
Respondent ID and Relationship to Focal
Child (X9RESID, X9RESREL2) ................................. 7-54
viiCONTENTS—Continued
Chapter Page
7.5.3 7.5.4 7.5.5 7.5.6 7.5.2.10 Food Security Status ..................................................... 7-55
7.5.2.10.1 Food Security Status: Raw
Scores (X9FSRAW2,
X9FSADRA2, and X9FSCHRA) ............... 7-56
7.5.2.10.2 Food Security Status: Continuous
Measures (X9FSSCAL2,
X9FSADSC2, and X9FSCHSC) ............... 7-57
7.5.2.10.3 Food Security Status: Categorical
Measures (X9FSSTAT2,
X9FSADST2, and X9FSCHST) ............... 7-58
Teacher Composite Variables ...................................................... 7-59
School Composite Variables ........................................................ 7-60
7.5.4.1 7.5.4.2 7.5.4.3 7.5.4.4 7.5.4.5 7.5.4.6 7.5.4.7 School Type (X9SCTYP) ............................................. 7-60
Public or Private School (X9PUBPRI) ......................... 7-61
School Enrollment (X9ENRLS) ................................... 7-61
Percent Non-White Students in the School
(X9RCETH) ................................................................. 7-62
Highest and Lowest Grade at the School
(X9LOWGRD, X9HIGGRD) ....................................... 7-64
Students Eligible for Free or Reduced-Price
School Meals (X9FRMEAL_I) .................................... 7-64
Geographic Region and Locality of the
Child’s School (X9REGION, X9LOCALE) ................ 7-67
Field Management System (FMS) Composite
Variables ...................................................................................... 7-69
7.5.5.1 School Year Start and End Dates
(X9SCHBDD, X9SCHBMM, X9SCHBYY,
X9SCHEDD, X9SCHEMM, X9SCHEYY) ................. 7-69
7.5.5.2 Year-Round Schools (X9YRRND) .............................. 7-70
School District Poverty (X9DISTPOV) ....................................... 7-70
7.6 Methodological Variables ............................................................................ 7-70
viiiCONTENTS—Continued
8. Chapter Page
ELECTRONIC CODEBOOK (ECB) ......................................................................... 8-1
8.1 Introduction .................................................................................................... 8-1
8.1.1 Hardware and Software Requirements ........................................... 8-1
8.2 Installing, Starting, and Exiting the ECB ....................................................... 8-2
8.2.1 8.2.2 Installing the ECB Program on Your Personal
Computer ........................................................................................ 8-2
How to Start the ECB ..................................................................... 8-5
REFERENCES ..................................................................................................................................... R-1
Appendixes
A. Data Addenda, Anomalies, Errata, and Data Considerations .................................... A-1
B. Supplemental Guide for the Kindergarten-Fifth Grade Public-Use Data
File ............................................................................................................................. B-1
ixLIST OF TABLES
Table Page
3-1. 3-2. 3-3. 3-4. 3-5. 3-6. 3-7. 3-8. Direct cognitive assessment: Item Response Theory (IRT) theta scores,
fall and spring kindergarten, fall and spring first-grade, fall and spring
second-grade, spring third-grade, spring fourth-grade, and spring fifth-
grade assessments: School years 2010–11, 2011–12, 2012–13; spring
2014; spring 2015; and spring 2016 ............................................................................ 3-7
Direct cognitive assessment: Item Response Theory (IRT) standard
errors of measurement (SEM), fall and spring kindergarten, fall and
spring first-grade, fall and spring second-grade, spring third-grade,
spring fourth-grade, and spring fifth-grade assessments: School years
2010–11, 2011–12, 2012–13; spring 2014; spring 2015; and spring 2016 ................. 3-8
Direct cognitive assessment: Item Response Theory (IRT) scale scores,
fall and spring kindergarten, fall and spring first-grade, fall and spring
second-grade, spring third-grade, spring fourth-grade, and spring fifth-
grade assessments: School years 2010–11, 2011–12, 2012–13; spring
2014; spring 2015; and spring 2016 .......................................................................... 3-10
Reliability of Item Response Theory (IRT)-based scores (theta and scale
scores), by round of data collection and domain, for fall and spring
kindergarten, fall and spring first grade, fall and spring second grade,
spring third grade, spring fourth grade, and spring fifth grade: School
years 2010–11, 2011–12, 2012–13; spring 2014; spring 2015; and spring
2016 .......................................................................................................................... 3-14
Dimensional Change Card Sort variable names, descriptions, value
ranges, weighted means, and standard deviations for fall and spring
second grade, spring third grade, spring fourth grade, and spring fifth
grade: School year 2012–13, spring 2014, spring 2015, and spring 2016 ................ 3-23
Numbers Reversed variable names, descriptions, value ranges, weighted
means, and standard deviations for fall and spring kindergarten, fall and
spring first grade, fall and spring second grade, spring third grade, spring
fourth grade, and spring fifth grade: School years 2010–11, 2011–12,
2012–13; spring 2014; spring 2015; and spring 2016 ............................................... 3-32
Flanker variable names, descriptions, value ranges, weighted means, and
standard deviations for spring fourth grade and spring fifth grade: Spring
2015 and spring 2016 ................................................................................................ 3-40
Teacher-reported social skills scales variable names, descriptions, value
ranges, weighted means, and standard deviations for fall and spring
kindergarten, fall and spring first grade, fall and spring second grade,
spring third grade, spring fourth grade, and spring fifth grade: School
years 2010–11, 2011–12, 2012–13; spring 2014; spring 2015; and spring
2016 .......................................................................................................................... 3-47
xLIST OF TABLES—Continued
Table Page
3-9. 3-10. 3-11. 3-12. 3-13. 4-1. 4-2. 4-3. 4-4. 4-5. Teacher-reported social skill scales reliability estimates for fall and
spring kindergarten, fall and spring first grade, and fall and spring second
grade, spring third grade, spring fourth grade, and spring fifth grade:
School years 2010–11, 2011–12, 2012–13; spring 2014; spring 2015; and
spring 2016 ............................................................................................................... 3-49
Teacher-reported Approaches to Learning scale variable names,
descriptions, value ranges, weighted means, and standard deviations for
fall and spring kindergarten, fall and spring first grade, fall and spring
second grade, spring third grade, spring fourth grade, and spring fifth
grade: School years 2010–11, 2011–12, 2012–13; spring 2014; spring
2015; and spring 2016 ............................................................................................... 3-51
Children’s Behavior Questionnaire variable names, descriptions, value
ranges, weighted means, and standard deviations for fall and spring
kindergarten and spring first grade: School year 2010–11 and spring
2012 .......................................................................................................................... 3-54
Temperament in Middle Childhood Questionnaire variable names,
descriptions, value ranges, weighted means, and standard deviations for
spring second grade, spring third grade, spring fourth grade, and spring
fifth grade: Spring 2013, spring 2014, spring 2015, and spring 2016 ...................... 3-54
Reliability estimates for the teacher-reported Attentional Focus and
Inhibitory Control scales for fall and spring kindergarten, spring first
grade, spring second grade, spring third grade, spring fourth grade, and
spring fifth grade: School year 2010–11, spring 2012, spring 2013,
spring 2014, spring 2015, and spring 2016 ............................................................... 3-56
The ECLS-K:2011 school sample after school substitution ....................................... 4-3
Number (unweighted) of eligible children sampled for the ECLS-
K:2011, by selected characteristics: School year 2010–11 ......................................... 4-5
Number (unweighted) of sampled children who are base-year
respondents, by selected characteristics: School year 2010–11 .................................. 4-6
Number (unweighted) of original sampled schools in the 30 PSUs
selected for the fall data collections, by selected characteristics: Fall
2011 and fall 2012 ...................................................................................................... 4-8
Number (unweighted) of original sampled schools with base-year
respondents at the start of the fall data collections, by selected
characteristics: Fall 2011 and fall 2012 ...................................................................... 4-9
xiLIST OF TABLES—Continued
Table Page
4-6. 4-7. 4-8. 4-9. 4-10. 4-11. 4-12. 4-13. 4-14. 4-15. 4-16. 4-17. 4-18. 4-19. Number (unweighted) of base-year respondents in the fall first- and
second-grade sample, by selected characteristics: Fall 2011 and fall 2012 ................. 4-10
Number (unweighted) of base-year respondents in fall first grade, by
type of sampled school and selected characteristics: Fall 2011 ................................ 4-12
Number (unweighted) of base-year respondents in the fall second grade,
by type of sampled school and selected characteristics: Fall 2012 ........................... 4-13
Number (unweighted) of original sampled schools in the 90 PSUs
selected for the spring data collections with base-year respondents, by
selected characteristics: Spring 2012, spring 2013, spring 2014, spring
2015, and spring 2016 ............................................................................................... 4-14
Number (unweighted) of base-year respondents in spring first grade, by
type of sampled school and selected characteristics: Spring 2012 ........................... 4-16
Number (unweighted) of base-year respondents in spring second grade,
by type of sampled school and selected characteristics: Spring 2013 ...................... 4-17
Number (unweighted) of base-year respondents in spring third grade, by
type of sampled school and selected characteristics: Spring 2014 ........................... 4-18
Number (unweighted) of base-year respondents in spring fourth grade, by
type of sampled school and selected characteristics: Spring 2015 .............................. 4-19
Number (unweighted) of base-year respondents in spring fourth grade
who were selected for the mathematics teacher questionnaire, by type of
sampled school and selected characteristics: Spring 2015 ........................................ 4-20
Number (unweighted) of base-year respondents in spring fourth grade
who were selected for the science teacher questionnaire, by type of
sampled school and selected characteristics: Spring 2015 ........................................ 4-21
Number (unweighted) of base-year respondents in spring fifth grade, by
type of sampled school and selected characteristics: Spring 2016 ........................... 4-22
Number (unweighted) of base-year respondents in spring fifth grade who
were selected for the mathematics teacher questionnaire, by type of
sampled school and selected characteristics: Spring 2016 ........................................ 4-23
Number (unweighted) of base-year respondents in spring fifth grade who
were selected for the science teacher questionnaire, by type of sampled
school and selected characteristics: Spring 2016 ...................................................... 4-24
Characteristics of the fifth-grade weights: Spring 2016 ........................................... 4-43
xiiLIST OF TABLES—Continued
Table Page
4-20. 4-21. 5-1. 5-2. 5-3. 5-4. 5-5. 5-6. 5-7. 5-8. 5-9. 5-10. 5-11. 5-12. 5-13. 5-14. Standard errors and design effects for selected survey items, fifth grade:
Spring 2016 ............................................................................................................... 4-49
Median design effects for the spring fifth-grade survey items, by school
characteristic: Spring 2016 ....................................................................................... 4-52
Response rates for child assessment and parent interview, by selected
school characteristics, fifth grade: Spring 2016 .......................................................... 5-6
Response rates for child assessment and parent interview, by selected
student characteristics, fifth grade: Spring 2016 ......................................................... 5-8
Response rates for reading teacher questionnaires, by selected school
characteristics, fifth grade: Spring 2016 ..................................................................... 5-9
Response rates for reading teacher questionnaires, by selected student
characteristics, fifth grade: Spring 2016 ................................................................... 5-11
Response rates for mathematics teacher questionnaires, by selected
school characteristics, fifth grade: Spring 2016 ........................................................ 5-12
Response rates for mathematics teacher questionnaires, by selected
student characteristics, fifth grade: Spring 2016 ....................................................... 5-14
Response rates for science teacher questionnaires, by selected school
characteristics, fifth grade: Spring 2016 ................................................................... 5-15
Response rates for science teacher questionnaires, by selected student
characteristics, fifth grade: Spring 2016 ................................................................... 5-17
Overall response rates for child assessment and parent interview, by
selected school characteristics, fifth grade: Spring 2016 .......................................... 5-18
Overall response rates for reading teacher questionnaires, by selected
school characteristics, fifth grade: Spring 2016 ........................................................ 5-20
Overall response rates for mathematics teacher questionnaires, by
selected school characteristics, fifth grade: Spring 2016 .......................................... 5-22
Overall response rates for science teacher questionnaires, by selected
school characteristics, fifth grade: Spring 2016 ........................................................ 5-24
Response rates for special education teacher questionnaires, fifth grade:
Spring 2016 ............................................................................................................... 5-26
Response rates for school administrator questionnaire, by selected school
characteristics, fifth grade: Spring 2016 ................................................................... 5-27
xiiiLIST OF TABLES—Continued
Table Page
5-15. 5-16. 5-17. 5-18. 5-19. 5-20. 5-21. 5-22. 5-23. 5-24. 5-25. 5-26. Response rates for school administrator questionnaire, by selected
student characteristics, fifth grade: Spring 2016 ....................................................... 5-29
Overall response rates for school administrator questionnaire, by selected
school characteristics, fifth grade: Spring 2016 ........................................................ 5-30
Weighted and unweighted response rates for all instruments, fifth grade:
Spring 2016 ............................................................................................................... 5-31
Estimates using unadjusted and nonresponse-adjusted weights, child
assessment, spring fifth grade: Spring 2016 ............................................................. 5-34
Differences between unweighted and weighted estimates, and between
unadjusted and adjusted estimates, child assessment, spring fifth grade:
Spring 2016 ............................................................................................................... 5-35
Estimates using unadjusted and nonresponse-adjusted weights, parent
interview, spring fifth grade: Spring 2016 ................................................................ 5-38
Differences between unweighted and weighted estimates, and between
unadjusted and adjusted estimates, parent interview, spring fifth grade:
Spring 2016 ............................................................................................................... 5-40
Estimates using unadjusted and nonresponse-adjusted weights, teacher
questionnaire data, spring fifth grade: Spring 2016 .................................................. 5-43
Differences between unweighted and weighted estimates, and between
unadjusted and adjusted estimates, teacher questionnaire data, spring
fifth grade: Spring 2016 ............................................................................................ 5-46
Estimates using unadjusted and nonresponse-adjusted weights, school
administrator questionnaire data, spring fifth grade: Spring 2016 ............................ 5-49
Differences between unweighted and weighted estimates, and between
unadjusted and adjusted estimates, school administrator questionnaire
data, spring fifth grade: Spring 2016 ........................................................................ 5-51
Differences between unadjusted base-year estimates from kindergarten
respondents and unadjusted base-year estimates from fifth-grade
respondents: Spring 2011 and spring 2016 ............................................................... 5-54
xivLIST OF TABLES—Continued
Table Page
5-27. 7-1. 7-2. 7-3. 7-4. 7-5. 7-6. 7-7. Differences between adjusted base-year estimates from kindergarten
respondents and adjusted base-year estimates from fifth-grade
longitudinal respondents: Spring 2011 and spring 2016 ........................................... 5-57
Pointers to parent figure questions: School year 2015–16 ........................................ 7-35
Occupation categories and assigned prestige scores ................................................. 7-45
Detailed income range categories used in the parent interview: Spring
2016 .......................................................................................................................... 7-47
Criteria for reporting income to the nearest $1,000 in the spring parent
interview and 2015 thresholds for 200 percent of poverty: Spring 2016 .................. 7-49
ECLS-K:2011 poverty composite and 2015 census poverty thresholds:
Spring 2016 ............................................................................................................... 7-50
Missing data for socioeconomic status (SES) source variables, fifth grade
year: School year 2015–16 ........................................................................................ 7-51
Number and percent of public and private schools and study students
with missing data for the percent of children in the school eligible for
free or reduced-price lunch (S9PCTFLN): Spring 2016 ........................................... 7-66
xvLIST OF EXHIBITS
Exhibit Page
1-1. 2-1. 2-2. 2-3. 2-4. 2-5. 2-6. 2-7. 2-8. 3-1. 3-2. 3-3. 3-4. Data collection schedule: School years 2010–11 through 2015–16 ........................... 1-4
Instruments used in the ECLS-K:2011 kindergarten, first-, second-, third,
fourth-, and fifth-grade rounds of data collection: School years 2010–11,
2011–12, 2012–13; spring 2014; spring 2015; and spring 2016 ................................. 2-2
Child questionnaire topics by round of data collection in the ECLS-
K:2011: Spring 2014, spring 2015, and spring 2016 ................................................ 2-10
Parent interview topics, by round of data collection in the ECLS-K:2011:
School years 2010–11, 2011–12, 2012–13; spring 2014; spring 2015; and
spring 2016 ............................................................................................................... 2-12
General classroom teacher teacher-level questionnaire topics, by round
of data collection in the ECLS-K:2011: School years: 2010–11, 2011–12,
2012–13; spring 2014; spring 2015; and spring 2016 ............................................... 2-19
General classroom teacher child-level questionnaire topics, by round of
data collection in the ECLS-K:2011: School years 2010–11, 2011–12,
2012–13; spring 2014; spring 2015; and spring 2016 ............................................... 2-21
Special education teacher questionnaire topics, by round of data
collection in the ECLS-K:2011: Spring 2011, spring 2012, spring 2013,
spring 2014, spring 2015, and spring 2016 ............................................................... 2-23
School administrator questionnaire topics, by round of data collection in
the ECLS-K:2011: Spring 2011, spring 2012, spring 2013, spring 2014,
spring 2015, and spring 2016 .................................................................................... 2-26
Copyright-protected instruments in ECLS-K:2011 .................................................. 2-27
Data flag description for the computerized Dimensional Change Card
Sort for fall and spring second grade, spring fourth grade, and spring fifth
grade: School year 2012–13, spring 2015, and spring 2016 ..................................... 3-24
Data flag description for the computerized the computerized
Dimensional Change Card Sort (DCCS) for spring third grade: Spring
2014 .......................................................................................................................... 3-25
Data flag description for the Flanker for the spring of fourth grade and
spring of fifth grade: Spring 2015 and spring 2016 .................................................. 3-41
Child questionnaire topics and item-level variables for spring third grade,
spring fourth grade, and spring fifth grade: Spring 2014, spring 2015, and
spring 2016 ................................................................................................................ 3-43
xviLIST OF EXHIBITS—Continued
Exhibit Page
3-5. 3-6. 3-7. 4-1. 4-2. 4-3. 5-1. 7-1. 7-2. 7-3. 7-4. 7-5. Teacher-reported item-level variables on peer relationships in spring
second grade, spring third grade, spring fourth grade, and spring fifth
grade: Spring 2013, spring 2014, spring 2015, and spring 2016 .............................. 3-58
Parent-reported item-level variables on peer relationships and friendships
in spring second grade, spring third grade, spring fourth grade, and
spring fifth grade: Spring 2013, spring 2014, spring 2015, and spring
2016 .......................................................................................................................... 3-59
Teacher- and parent-reported item-level variables on school liking and
avoidance in spring fourth grade and spring fifth grade: Spring 2015 and
spring 2016 ............................................................................................................... 3-60
ECLS-K:2011 fifth-grade main sampling weights for analysis not
including data from teachers ..................................................................................... 4-31
ECLS-K:2011 fifth-grade main sampling weights associated with data
from teachers ............................................................................................................. 4-32
Weights developed for use with the ECLS-K:2011 fifth-grade data, by
components for which nonresponse adjustments were made: Spring 2016 ................... 4-38
ECLS-K:2011 survey instruments and definition of completed
instrument: Spring 2016 .............................................................................................. 5-2
Prefixes for fifth-grade variables ................................................................................ 7-3
Identification variables included in the ECLS-K:2011 kindergarten–fifth
grade restricted-use data file ....................................................................................... 7-5
Missing value codes used in the ECLS-K:2011 data file ............................................ 7-7
Industry and occupation codes used in the ECLS-K:2011 ....................................... 7-41
Definitions of negative and affirmed values for the food security items in
the ECLS-K:2011 kindergarten–fifth grade restricted-use data file ......................... 7-57
8-1. InstallShield Wizard ................................................................................................... 8-3
8-2. Welcome window ....................................................................................................... 8-3
8-3. Choose Destination Location ...................................................................................... 8-4
8-4. Setup Status ................................................................................................................. 8-4
8-5. InstallShield Wizard Complete ................................................................................... 8-5
xviiLIST OF EXHIBITS—Continued
Exhibit Page
8-6a. Desktop icon ............................................................................................................... 8-5
8-6b. 8-7. 8-8. 8-9. 8-10. B-1. B-2. B-3. B-4. B-5. B-6. B-7. B-8. Desktop screen—click start ........................................................................................ 8-6
First-time user dialog box ........................................................................................... 8-6
ECB splash screen ...................................................................................................... 8-7
Select Catalog screen .................................................................................................. 8-7
Main ECB screen ........................................................................................................ 8-8
ECLS-K:2011 masked variables, spring 2016 child assessment ............................... B-3
ECLS-K:2011 masked variables, spring 2016 parent interview ................................ B-4
ECLS-K:2011 masked variables, spring 2016 teacher-level reading
teacher questionnaire ............................................................................................... B-14
ECLS-K:2011 masked variables, spring 2016 teacher-level
mathematics/science teacher questionnaire ............................................................. B-16
ECLS-K:2011 masked variables, spring 2016 child-level teacher
questionnaires .......................................................................................................... B-17
ECLS-K:2011 masked variables, spring 2016 school administrator
questionnaire ............................................................................................................ B-18
ECLS-K:2011 masked variables, spring 2016 composite variables ........................ B-21
ECLS-K:2011 masked variables, spring 2016 field management system
and identification variables ...................................................................................... B-22
xviii1. INTRODUCTION
This manual provides guidance and documentation for users of the longitudinal kindergarten–
fifth grade (K-5) data file of the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11
(ECLS-K:2011). It mainly provides information specific to the fifth-grade round of data collection. Users
should refer to the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011),
User’s Manual for the ECLS-K:2011 Kindergarten Data File and Electronic Codebook, Public Version
(NCES 2015-074) (Tourangeau et al. 2015a), hereinafter referred to as the base-year User’s Manual, for
information about the general study methodology and the kindergarten rounds of data collection; to the
Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), User’s Manual for
the ECLS-K:2011 Kindergarten–First Grade Data File and Electronic Codebook, Public Version (NCES
2015-078) (Tourangeau et al. 2015b); for information about the first-grade rounds of data collection; to the
Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), User’s Manual for
the ECLS-K:2011 Kindergarten–Second Grade Data File and Electronic Codebook, Public Version (NCES
2017-285) (Tourangeau et al. 2017) for information about the second-grade rounds of data collection; to
the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), User’s Manual
for the ECLS-K:2011 Kindergarten–Third Grade Data File and Electronic Codebook, Public Version
(NCES 2018-034) (Tourangeau et al. 2018a) for information about the third-grade round of data collection;
and to the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), User’s
Manual for the ECLS-K:2011 Kindergarten–Fourth Grade Data File and Electronic Codebook, Public
Version (NCES 2018-032) (Tourangeau et al. 2018b) for information about the fourth-grade round of data
collection.
This chapter provides an overview of the ECLS-K:2011. Subsequent chapters provide details
on the fifth-grade data collection instruments and methods, including a description of how the fifth-grade
data collection differs from the earlier rounds, the direct and indirect child assessments, the sample design,
weighting procedures, response rates, and data file content, including composite variables.
Data for the ECLS-K:2011 are released in both a restricted-use and a public-use version. This
manual, which has been developed for public dissemination and use with the public version of the data, is
almost identical to the manual released with the kindergarten-fifth-grade restricted-use file.
1 Edits have
been made to round or remove unweighted sample sizes that cannot be generated with the public-use
file (PUF). Estimates such as means that are presented in the tables throughout the manual were calculated
1 Early Childhood Longitudinal Study, Kindergarten Class of 2010–11(ECLS-K:2011) User’s Manual for the ECLS-K:2011 Kindergarten-Fifth
Grade Data File and Electronic Codebook, Restricted Version (NCES 2019-101) (Tourangeau et al. 2019b).
1-1with the restricted-use file. Some estimates may not be able to be reproduced exactly with variables in the
PUF because the variables have been masked to make them suitable for public release. Appendix B
provides information about the ways in which data were masked on the PUF and includes tables that
list all variables that have been masked or suppressed. Also, throughout this manual references are made
to materials that are on the restricted-use DVD. Public-release versions of these materials are available
under “Data Products” on the ECLS-K:2011 website, https://nces.ed.gov/ecls/kindergarten2011.asp.
The ECLS-K:2011 followed a nationally representative sample of children from kindergarten
through their elementary school years. It is a multisource, multimethod study that focuses on children’s
early school experiences. It includes interviews with parents; self-administered questionnaires completed
by teachers and school administrators; one-on-one assessments of children; and beginning in third grade, a
computer-assisted self-administered questionnaire for children. During the kindergarten year, the ECLS-
K:2011 also included self-administered questionnaires for nonparental before- and after-school care
providers. The ECLS-K:2011 is sponsored by the National Center for Education Statistics (NCES) within
the Institute of Education Sciences (IES) of the U.S. Department of Education.
1.1 Background
The ECLS-K:2011 is the third and latest study in the Early Childhood Longitudinal Study
(ECLS) program, which at present comprises three longitudinal studies of young children: the Early
Childhood Longitudinal Study, Kindergarten Class of 1998–99 (ECLS-K); the Early Childhood
Longitudinal Study, Birth Cohort (ECLS-B); and the ECLS-K:2011. The ECLS program is broad in its
scope and coverage of child development, early learning, and school progress. It draws together information
from multiple sources, including children, parents, teachers, school administrators, and early care and
education providers, to provide data for researchers and policymakers to use to answer questions regarding
children’s early educational experiences and address important policy questions. The ECLS-K:2011
provides current information about today’s elementary school children. Also, coming more than a decade
after the inception of the ECLS-K, the ECLS-K:2011 allows for cross-cohort comparisons of two nationally
representative kindergarten classes experiencing different policy, educational, and demographic
environments.
The three studies in the ECLS program provide national data on children’s developmental
status at birth and at various points thereafter; children’s transitions to nonparental care, early education
programs, and school; and children’s home and school experiences, growth, and learning. The ECLS
program also provides data that enable researchers to analyze how a wide range of child, family, school,
1-2classroom, nonparental care and education provider, and community characteristics relate to children’s
development and to their experiences and success in school. Together, these three studies provide the range
and breadth of data needed to more fully describe and understand children’s education experiences, early
learning, development, and health in the late 1990s, 2000s, and 2010s.
More information about all three of these studies can be found on the ECLS website
(https://nces.ed.gov/ecls).
1.2 Periods of Data Collection
The ECLS-K:2011 followed a cohort of children from their kindergarten year (the
2010–11 school year, referred to as the base year) through the 2015–16 school year, when most of the
children were in fifth grade (exhibit 1-1). The sample included both children who were in kindergarten for
the first time and those who were repeating kindergarten during 2010–11. Although the study refers to later
rounds of data collection by the grade the majority of children were expected to be in (that is, the modal
grade for children who were in kindergarten in the 2010–11 school year), children were included in
subsequent data collections regardless of their grade level.2 During the 2010–11 school year, when both a
fall and a spring data collection were conducted, approximately 18,170 kindergartners from about 1,310
schools3 and their parents, teachers, school administrators, and before- and after-school care providers
participated in the study. Fall and spring data collections were also conducted during the first-grade year.
While the fall kindergarten collection included the full ECLS-K:2011 sample, the fall first-grade collection
was conducted with children in one-third of the sample of primary sampling units (PSUs) selected for the
study. These children are referred to as the fall subsample. The data collection schedule for second grade
was similar to the schedule for first grade, with a fall second-grade collection that included the same
subsample of children from the fall of first grade and a spring collection that included the entire sample of
children who participated in at least one of the two base-year data collection rounds. In the third, fourth,
and fifth grades, a spring data collection was conducted with the entire sample of children who participated
in the base year.4
2 Children may not be in the modal grade due to retention in a grade or promotion to a higher grade ahead of schedule.
3 This number includes both schools that were part of the original sample of schools selected for the study (approximately 970) and schools to
which children transferred during the base year (approximately 340).
4 Beginning with the fall first-grade data collection, children who moved away from their original base-year schools were subsampled for follow-
up. More information about the sample for fifth grade, including the subsampling of movers, is provided in chapter 4.
1-3Exhibit 1-1. Data collection schedule: School years 2010–11 through 2015–16
School year Grade1 Data collections2
2010–11 Kindergarten Fall 2010
Spring 2011
2011–12 First grade Fall 2011
Spring 2012
2012–13 Second grade Fall 2012
Spring 2013
2013–14 Third grade Spring 2014
2014–15 Fourth grade Spring 2015
2015–16 Fifth grade Spring 2016
1 Grade indicates the modal grade for children who were in kindergarten in the 2010–11 school year. After the kindergarten rounds of data
collection, children were included in data collection regardless of their grade level.
2 All but two rounds of data collection include the entire sample of children. The fall first-grade data collection included approximately one-third
of the total ECLS-K:2011 sample of children. The fall second-grade data collection included the same subsample selected for the fall of first
grade.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011).
1.3 Overview of the Fifth-Grade Round of Data Collection
As described in chapter 1 of the base-year User’s Manual, the ECLS-K:2011 collected
information from children, parents, classroom teachers, special education teachers, and school
administrators. In the base year, information was also collected from children’s before- and after-school
care providers. Data collection instruments for all these different respondent types were included in the
fifth-grade round of data collection, except for the care provider questionnaires. The care provider
component was included in the base year to obtain more information about young children’s activities
outside of school, which is particularly important for understanding differences in the educational
environments of children attending full-day kindergarten and of those attending part-day kindergarten.
The assessments and instruments used in fifth grade were largely the same as those used in
earlier rounds to allow for longitudinal analysis. However, the earlier assessments and instruments were
revised, as necessary, to make them appropriate for the fifth-grade data collections. For example, questions
in the school administrator questionnaire asking about the school’s fourth-graders were revised to ask about
the school’s fifth-graders. As in third and fourth grades, fifth-grade instruments included a child
questionnaire. Specifically, children completed an audio computer-assisted self-administered questionnaire
about themselves. For the fifth-grade collection, the direct child assessment included a similar battery of
assessments as previous rounds and also included the third measure of executive function added in fourth
grade to the existing two measures used in the previous rounds. More detailed information about the fifth-
1-4grade study instruments, including how they differ from the instruments used in the earlier rounds, is
provided in chapter 2.
1.4 ECLS-K:2011 Kindergarten–Fifth Grade (K-5) Public-Use Data File
The ECLS-K:2011 kindergarten–fifth grade (K-5) public-use data file includes the base-year,
first-grade, second-grade, third-grade, fourth-grade, and fifth-grade data encompassing both the fall and
spring rounds of data collection in kindergarten, first grade, and second grade and the spring round of data
collection in third, fourth, and fifth grades. The data file includes information for all students who
participated during the kindergarten year even if they did not participate during later rounds. Fifth-grade
data for students who did not participate in the fifth-grade round are set to “system missing.” The K-5
public-use file (PUF) is intended to replace the previously released PUFs; the K-5 PUF includes all of the
cases included in prior PUFs and has some important corrections and updates to previously released data,
including the child assessment scores.
In preparing data files for release, NCES takes steps to minimize the likelihood that individual
schools, teachers, parents, or students participating in the study can be identified. Every effort is made to
protect the identity of individual respondents. The process of preparing the files for release includes a formal
disclosure risk analysis. Small percentages of values are swapped across cases with similar characteristics
to make it very difficult to identify a respondent with certainty. The modifications used to reduce the
likelihood that any respondent could be identified in the data do not affect the overall data quality.
Analysts should be aware that the ECLS-K:2011 data file is provided as a child-level data file
containing one record for each child who participated in the base year. The record for each child contains
information from each of the study respondents: the child, as well as his or her parent, teacher(s), school
administrator and, if applicable, before- or after-school care provider.
The ECLS-K:2011 K-5 data are provided with an electronic codebook (ECB) that permits
analysts to view the variable frequencies, tag selected variables, and prepare data extract files for analysis
with SAS, SPSS, or Stata. The public-use version of the data will be available online.
1-51.5 Contents of Manual
The remainder of this manual contains more detailed information on the fifth-grade data
collection instruments (chapter 2) and the direct and indirect child assessments (chapter 3). It also describes
the ECLS-K:2011 sample design and weighting procedures (chapter 4), response rates and bias analysis
(chapter 5), and data preparation procedures (chapter 6). In addition, this manual describes the structure of
the K-5 data file and the composite variables that have been developed for the file (chapter 7). The last
chapter of this manual contains a short introduction to the ECLS-K:2011 ECB and how to use it (chapter 8).
Additional information about the ECLS-K:2011 study design, methods, and measures can be
found in earlier round user’s manuals noted above, as well as in the Early Childhood Longitudinal Study,
Kindergarten Class of 2010–11 (ECLS-K:2011), Kindergarten Psychometric Report (NCES 2018-182)
(Najarian et al. 2018a), the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11
(ECLS-K:2011), First-Grade and Second-Grade Psychometric Report (NCES 2018-183) (Najarian et al.
2018b), and the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011),
Third-Grade, Fourth-Grade, and Fifth-Grade Psychometric Report (NCES 2019-023) (Najarian et al.
forthcoming). Also, as noted earlier, additional information about the ECLS program can be found online
at https://nces.ed.gov/ecls.
1-62. DATA COLLECTION INSTRUMENTS AND METHODS
This chapter describes the data collection instruments used in the Early Childhood
Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011) fifth-grade round of data collection,
including the child assessments, child questionnaire, parent interview, school administrator
questionnaires, and teacher questionnaires.1 Differences between earlier rounds of data collection and the
fifth-grade round in the study instruments and data collection procedures are discussed. For more
information on the earlier data collection instruments and methods, consult the user’s manuals for those
rounds.
2.1 Data Collection Instruments
The design of the ECLS-K:2011 and its survey instruments was guided by a conceptual
framework of children’s development and learning that emphasizes the interaction among the various
environments in which children live and learn and the resources within those environments to which
children have access. A comprehensive picture of children’s environments and experiences is created by
combining information from children themselves, their parents, their school administrators, their teachers,
and their kindergarten before- and after-school care providers.
Exhibit 2-1 presents a listing of the ECLS-K:2011 data collection instruments and the rounds
of data collection in which they were used. The instruments for the kindergarten, first-grade, second-
grade, third-grade, fourth-grade, and fifth-grade collections are included on the ECLS-K:2011
kindergarten–fifth grade (K–5) restricted-use DVD and are available online at https://nces.ed.gov/ecls,
with the exception of copyrighted materials or items adapted from copyrighted materials that cannot be
publicly distributed without copyright holder and National Center for Education Statistics (NCES)
permission. Study instruments and items for which copyright permissions are needed are discussed
further in section 2.1.7.
The information collected in the ECLS-K:2011 instruments can be used to answer a wide
variety of research questions about how child, home, school, and neighborhood factors relate to children’s
cognitive, social, emotional, and physical development. Sections 2.1.1 through 2.1.6 describe the major
topics covered in each instrument.
1 For ease of presentation, this chapter refers to all students as “fifth-grade students.” However, the reader should keep in mind that some
children had been retained in a grade and a very small number of students had been advanced to a higher grade. These children are included in the
group being referred to as fifth-graders.
2-1Exhibit 2-1. Instruments used in the ECLS-K:2011 kindergarten, first-, second-, third-, fourth-, and
fifth-grade rounds of data collection: School years 2010–11, 2011–12, 2012–13; spring
2014; spring 2015; and spring 2016
Instrument
Fall
kinder-
garten
Spring
kinder-
garten
Fall
first
grade
Spring
first
grade
Fall
second
grade
Spring
second
grade
Spring
third
grade
Spring
fourth
grade
Spring
fifth
grade
Child assessment
Language screener X X X X
Reading X X X X X X X X X
Mathematics X X X X X X X X X
Executive function X X X X X X X X X
Science X X X X X X X X
Height and weight X X X X X X X X X
Hearing evaluation X X X1
Child questionnaire X X X
Parent interview X X X X X X X X X
Classroom teacher
questionnaires –
grades K, 1, 2, and 3
Teacher level X X X X X
Teacher level – subject
area
X
Teacher background (new
teacher supplement)
X
Child level X X X X X X X
Classroom teacher
questionnaires –
grades 4 and 5
Teacher Background
Questionnaire
X X
Reading and Language
Arts Teacher
Questionnaire
X X
Mathematics Teacher
Questionnaire
X X
Science Teacher
Questionnaire
X X
Special education teacher
questionnaires
Teacher level X X X X X X
Child level X X X X X X
See notes at end of exhibit.
2-2Exhibit 2-1. Instruments used in the ECLS-K:2011 kindergarten, first-, second-, third-, fourth-, and
fifth-grade rounds of data collection: School years 2010–11, 2011–12, 2012–13; spring
2014; spring 2015; and spring 2016—Continued
Instrument
Fall
kinder-
garten
Spring
kinder-
garten
Fall
first
grade
Spring
first
grade
Fall
second
grade
Spring
second
grade
Spring
third
grade
Spring
fourth
grade
Spring
fifth
grade
School administrator
questionnaires
X X X X X X
Before- and after-school care
questionnaires
Center director X
Center-based care
provider
X
Home-based care
provider
X
Child level X
1 In spring fifth grade, children who completed the hearing evaluation also completed a short language impairment screener. For more
information on this assessment and on the hearing evaluations component, see the Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011) User’s Manual for the Fifth-Grade Hearing Evaluations Component Data File (NCES 2019-019) (Tourangeau et al.
2019a).
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
2.1.1 Direct Child Assessment
In the fifth-grade data collection, children were assessed in the spring in reading,
mathematics, science, and on their executive function skills, and their height and weight were measured.
The majority of the items included in the fifth-grade assessments in reading, mathematics, and science
had been included in the earlier assessments. However, to ensure that these assessments adequately
measured the knowledge and skills of the children as they progressed through school, new, more difficult
items were added to the assessments in fifth grade, and easier items reflecting lower level skills were
omitted. All children received the assessments designed for the fifth-grade collection, regardless of their
actual grade level. The reading, mathematics, and science assessments were administered directly to the
sampled children on an individual basis by trained and certified child assessors. This battery of
assessments was designed to be administered within about 60 minutes per child.2 Child responses were
entered by the assessor into a computer-assisted interviewing (CAI) program. Executive function skills
were assessed through computer-administered tasks completed by children and an oral task in which child
responses were input into the computer using the CAI program. In addition, a subsample of study
children had their hearing evaluated.
2 Together the fifth-grade reading, mathematics, and science assessments took an average of 61 minutes. The executive function assessments
averaged 13 minutes. The measurement of height and weight took about 5 minutes.
2-3Two-stage assessment. The fifth-grade direct cognitive assessment included two-stage
assessments for reading, mathematics, and science. For each assessment domain, the first stage of the
assessment was a routing section that included items covering a broad range of difficulty. A child’s
performance on the routing section of a domain determined which one of three second-stage tests (low,
middle, or high difficulty) the child was next administered for that domain. The second-stage tests varied
by level of difficulty so that a child would be administered questions appropriate for his or her
demonstrated level of ability for each of the cognitive domains. The purpose of this adaptive assessment
design was to maximize accuracy of measurement while minimizing administration time.
Language screener for children whose home language was not English. In kindergarten
and first grade, a language screener was used for children whose home language was not English. By the
spring of first grade, nearly all children (99.9 percent) were routed through the assessment in English;
therefore, the language screener was not administered beyond the spring of first grade.
Cognitive domains. The fifth-grade cognitive assessment focused on four domains: reading
(language use and literacy), mathematics, science, and executive function (working memory, cognitive
flexibility, and inhibitory control). For the reading, mathematics, and science assessments, assessors
asked the children questions related to images or text that were presented on a small easel, such as words,
short sentences, or items associated with passages for reading; numbers and number problems for
mathematics; and predictions based on observations and cause-and-effect relationships for science. For
the reading assessment, children were also asked questions about short reading selections they were asked
to read in a passages booklet developed for the assessment. These questions were also presented on the
easel. Children were not required to explain their reasoning. The executive function component included a
computer-administered card sort task, for which children entered responses in the assessor’s laptop
computer; a backward digit span task, for which children provided verbal responses to the assessor; and a
computer-administered inhibitory control task, for which children entered responses in the assessor’s
laptop computer. A brief description of each of the cognitive assessment components follows.
Reading (language and literacy). The reading assessment included questions measuring
basic skills (e.g., word recognition), vocabulary knowledge, and reading comprehension. Reading
comprehension questions asked the child to identify information specifically stated in text (e.g.,
definitions, facts, supporting details); to make complex inferences within texts; and to consider the text
objectively and judge its appropriateness and quality. The reading assessment began with a set of 12
routing items, with the child’s score on these items determining which second-stage form (low, middle, or
high difficulty) the child received.
2-4Mathematics. The mathematics assessment was designed to measure skills in conceptual
knowledge, procedural knowledge, and problem solving. The assessment consisted of questions on
number sense, properties, and operations; measurement; geometry and spatial sense; data analysis,
statistics, and probability; and patterns, algebra, and functions. A set of 18 routing items was administered
to all children, and the score on these items determined which second-stage test (low, middle, or high
difficulty) a child received. Most of the text that the children could see on the easel pages, for example,
question text for word problems or graph labels, was read to them by the assessor to reduce the likelihood
that the children’s reading ability would affect their mathematics assessment performance.3 Paper and
pencil were offered to the children for use during the mathematics assessment, and children were
periodically reminded of the availability of paper and pencil as part of the assessment protocol.
Science. The science assessment included questions about physical sciences, life sciences,
Earth and space sciences, and scientific inquiry. The science assessment included 15 routing items that all
children received, followed by one of three second-stage forms (low, middle, or high difficulty). As with
reading and mathematics, the second-stage form children received depended on their responses to the
routing items. The questions, response options, and any text the children could see on the easel pages (for
example, graph labels) were read to the children to reduce the likelihood that their reading ability would
affect their science assessment score.
Executive function. The executive function component of the cognitive assessment obtained
information on cognitive processes associated with learning: cognitive flexibility, working memory, and
inhibitory control. To measure cognitive flexibility, children were administered the Dimensional Change
Card Sort (DCCS) (Zelazo 2006). Different versions of the DCCS were used in different rounds of data
collection because there was no single task that was age appropriate across all rounds of data collection
when the study began. During the kindergarten and first-grade rounds, the hard-copy or physical version
of the DCCS, as described in Zelazo 2006, was administered using cards that children were asked to sort
into piles. Because the physical version of the DCCS would have been too easy for the majority of the
study children during the second-grade rounds, beginning in the fall second-grade round, children were
administered a new, age-appropriate, computerized version of the DCCS in which the “cards” were
presented on a computer screen and children sorted them into “piles” on the computer screen using keys
on the keyboard to indicate where to place each card. The computerized task was developed as part of the
National Institutes of Health Toolbox for the Assessment of Neurological and Behavioral Function (NIH
Toolbox) and is appropriate for ages 3–85 (Zelazo et al. 2013). The NIH Toolbox DCCS has two different
3 Numbers were read to the child only when the question text referenced the number.
2-5administrations based on the age of the child: one for children 7 years and younger and one for children 8
years and older. The task had been under development during the kindergarten and first-grade rounds of
data collection but became available in time to be incorporated into the second-grade data collections. The
ECLS-K:2011 used the version for children 8 years and older beginning in the fall second-grade round.
Although the physical and the computer versions assess the same construct, the scoring and the way by
which the construct is assessed differ across the two tasks (for information on scoring, see chapter 3,
section 3.2.1).
Like the physical version of the DCCS administered in the kindergarten and first-grade data
collections, the computerized version asked children to sort cards either by shape or color. However,
rather than administer the cards in sections with a consistent sorting rule (with cards first sorted only by
color, then only by shape, and finally by color or shape depending on whether a card had a black border),
in the computerized DCCS the sorting rules were intermixed across the 30 trials of the task. In the
computerized DCCS, one rule was more common than the other to build a response tendency (i.e., a
response that was “preferred” because it happened more frequently, resulting in a predisposition to
respond in that manner). Also, whereas performance on the physical version was measured by sorting
accuracy, performance on the computerized version was measured as a function of both accuracy and
reaction time. Reaction time was calculated based on reaction time only for trials using the sorting rule
that was presented less often and only when there was a correct response. The reaction time of the less
frequent trials or nondominant trials was of most interest because when a child is predisposed to respond
in a particular way, it is harder and takes more time to inhibit that response tendency and switch the
response to maintain accuracy. As children get older, it is important to incorporate reaction time into the
DCCS score because older children and adults tend to slow down in order to respond accurately. Younger
children do not tend to show a speed/accuracy tradeoff, and therefore accuracy is a better metric of
performance for young children (Davidson et al. 2006). Performance on the computerized version of the
DCCS was derived from a formula that takes into consideration both accuracy and reaction time (Zelazo
et al. 2013; Slotkin, Nowinski et al. 2012).
After the card sort, children were administered the Numbers Reversed task, which is a
measure of working memory. In this task, children were asked to repeat strings of orally presented
numbers in reverse order. The sequence of numbers became increasingly longer, up to a maximum of
eight numbers. The task was ended when children responded incorrectly to three consecutive number
sequences of the same length, so that they would not be asked to continue at a level that was too difficult,
or when all number sequences had been completed.
2-6Beginning in fourth grade, children were administered a task that measured inhibitory
control in the context of selective visual attention. The NIH Toolbox Flanker Inhibitory Control and
Attention Task (Flanker) is a computerized task that was developed as part of the NIH Toolbox for the
Assessment of Neurological and Behavioral Function (NIH Toolbox) and is appropriate for ages 3–85
(Zelazo et al. 2013). The ECLS-K:2011 used the version of the NIH Toolbox Flanker task that is for
children 8 years and older.
The Flanker task measures both inhibitory control and attention. Children must inhibit an
automatic response tendency that may interfere with achieving a goal and use selective attention to
consciously direct sensory or thought processes to a stimulus in the visual field in the service of goal-
directed behavior. In the Flanker task, children were asked to focus attention on a central stimulus while
ignoring or inhibiting attention to stimuli presented on either side of the central stimulus. The stimuli used
for children 8 years or older are a series of five arrows, pointing either left of right. The stimuli that
“flank” the central stimulus either point in the same direction as the central stimulus (congruent) or in the
opposite direction as the central stimulus (incongruent). Children were presented with 20 trials and were
asked to press a button on the computer to indicate the direction the central stimulus was pointing. Like
the DCCS, performance on the Flanker was derived from a formula that takes into consideration both
accuracy and reaction time (Zelazo et al. 2013; Slotkin, Nowinski et al. 2012). Performance on the
incongruent trials was used to derive a score that is a measure of inhibitory control in the context of
selective visual attention.
Height and weight measurement. In addition to the cognitive domains described above,
children’s height and weight were measured during each data collection. A Shorr board (a tall wooden
stand with a ruled edge used for measuring height) and a digital scale were used to obtain the
measurements.
4 Assessors recorded the children’s height (in inches to the nearest one-quarter inch) and
weight (in pounds to the nearest half pound) on a height and weight recording form and then entered the
measurements into a laptop computer. Each measurement was taken and recorded twice to ensure reliable
measurement.
Hearing evaluations. In the spring fifth-grade data collection, a subsample of the children
also had their hearing evaluated by specially trained health technicians. Study protocol called for the
health technicians to conduct the 15-minute hearing evaluations immediately after each selected child’s
4 The Shorr board that was used is manufactured by Weigh and Measure, LLC, and is model ICA. The digital scale used was Seca Bella model
840.
2-7assessment and height and weight measurement.5 For the hearing evaluation, the health technician first
asked the child a few questions about his or her hearing and recent experiences that could affect the
results of the evaluation, including whether the child had an earache or recent cold or had recently heard
any loud noises. Next, the child’s ears were visually examined to see if there was any blockage that could
affect the evaluation. The child’s responses to the questions and the results of the visual examination of
the child’s ears were entered into a laptop computer. Then, the child listened to short tones of various
pitches and decibel levels that were presented through headphones connected to an audiometer in order to
determine hearing thresholds (the softest sounds the child could hear) for each ear. Next, the health
technician used a tympanometer to measure inner-ear functioning. The data collected from the audiometer
and the tympanometer were automatically transferred from the hearing equipment and saved to the health
technician’s laptop. Finally, new to the fifth-grade round, children used an Apple iPod Touch® MP3
player to complete a short grammar test that could indicate language impairment. The scores produced by
the Grammaggio language screening application were entered into the laptop by the health technician. For
detailed information about the language screening application and each part of the hearing evaluations
component, see the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011)
User’s Manual for the Fifth-Grade Hearing Evaluations Component Data File (NCES 2019-019)
(Tourangeau et al., 2019). The data collected during the hearing evaluation are available in a separate,
restricted-use data file, the ECLS-K:2011 Fifth-Grade Hearing Evaluations Component Restricted-Use
Data File (NCES 2019-018) (U.S. Department of Education 2019).
2.1.2 Child Questionnaire
Beginning in the spring of third grade, a child questionnaire was administered to children
prior to the cognitive assessment components. The fifth-grade questionnaire had 48 questions and took
approximately 10 minutes to complete.
Unlike the hard-copy child questionnaires that were administered during the Early
Childhood Longitudinal Study, Kindergarten Class of 1998–99 (ECLS-K) by assessors who read the
questions/items to the children, the ECLS-K:2011 child questionnaire was administered on a computer
using audio computer-assisted self-interview (audio-CASI) technology and headphones. Children listened
as the software system read the instructions and questionnaire items. One questionnaire item at a time was
displayed on the laptop’s screen, and in fourth and fifth grades a computer-generated voice read each
5 In some instances, it was not possible to follow this standard protocol because after the assessment/measurement, the child had to return to the
classroom for a scheduled activity, for example, a recess or lunch break. For those children, the evaluation was completed as soon as possible
after the activity or break.
2-8question and the response options to the child. The child responded by selecting the desired response on
the laptop’s touch screen. The audio-CASI questionnaire standardized administration and accommodated
the variation in children’s reading ability levels. It also allowed the child privacy to respond to the
questions and limited distractions because the headphones worn during the administration minimized
extraneous noise.
Exhibit 2-2 shows the content areas included in the third-, fourth-, and fifth-grade child
questionnaires. The fifth-grade child questionnaire included both new items and items that were also
included in earlier questionnaires. In both the third- and fourth-grade questionnaires, children were asked
about social anxiety, specifically fear of negative evaluation by peers, and about peer victimization. The
peer victimization questions were parallel to questions asked of teachers in third and fourth grades and of
parents in third grade. New questions that were part of the fourth-grade questionnaire asked children
about their behavioral engagement in school, peer social support, feelings of loneliness at school, media
usage and family rules about media usage, and pets. In contrast to the third-grade child questionnaire, the
content of the fourth-grade questionnaire did not overlap with the content of the child questionnaires that
were administered in the prior cohort study, the ECLS-K. The fifth-grade questionnaire included the peer
victimization questions and fear of negative evaluation by peer items that were also asked in third and
fourth grades. A subset of the life satisfaction items asked in third grade was also asked in fifth grade. The
fifth-grade questionnaire included questions on behavioral engagement in school, peer social support,
feelings of loneliness at school and media usage that were asked in fourth grade. It also included
additional, new items on media usage. New questions were added to the fifth-grade questionnaire on
school belonging, grit (i.e., perseverance over the long term in pursuit of a goal), worry about school, and
parental monitoring. The questions about school belonging were originally asked in the Grade 8 Student
Questionnaire from the ECLS-K, and questions about worry about school were selected from a larger set
of items on internalizing problem behaviors that were developed and used in grades 3, 5, and 8 in the
ECLS-K.
2-9Exhibit 2-2. Child questionnaire topics by round of data collection in the ECLS-K:2011: Spring 2014,
spring 2015, and spring 2016
Child questionnaire topics
Spring
third grade
Spring
fourth grade
Spring
fifth grade
Perceived Interest/Competence in Reading1 X
Perceived Interest/Competence in Math1 X
Perceived Interest/Competence in Science1 X
Perceived Interest/Competence in Peer Relationships1 X
Peer Victimization2 X X X
Social Anxiety/Fear of Negative Evaluation3 X X X
Prosocial Behavior4 X
Life Satisfaction5 X X
Behavioral Engagement6 X X
Peer Social Support7 X X
Loneliness8 X X
Media Usage9 X X
Pets10 X
School Belonging11 X
Grit12 X
Worry/Stress About School13 X
Parental Monitoring14 X
1 Adapted from the Self Description Questionnaire I (SDQI) © Herbert Marsh. SELF Research Centre (Bankstown Campus) University of
Western Sydney, Australia. Used with permission.
2 Peer victimization items were adapted from a 21-item scale by Espelage, D. L. and Holt, M. (2001). Bullying and victimization during early
adolescence: Peer influences and psychosocial correlates. Journal of Emotional Abuse, 2: 123–142.
3 Adapted from the Social Anxiety Scale for Children—Revised ©1993 Annette M. La Greca, University of Miami. Used with permission. La
Greca, A. M. and Stone, W. L. (1993). Social anxiety scale for children—revised: Factor structure and concurrent validity. Journal of Clinical
Child Psychology, 22(1): 17–27.
4 Adapted from the Children’s Social Behavior Scale—Self Report (CSBS-S). Crick, N.R. and Grotpeter, J.K. (1995). Relational aggression,
gender, and social psychological adjustment. Child Development, 66: 710–722.
5 Adapted from the NIH Toolbox for Assessment of Neurological and Behavioral Function (version 1.0): Domain-Specific Life Satisfaction
Survey from the NIH Toolbox Emotion Battery (www.NIHToolbox.org) © 2012 Northwestern University and the National Institutes of Health.
Used with permission.
6 Adapted from Skinner, E. A., Kindermann, T. A., and Furrer, C. J. (2009). A motivational perspective on engagement and disaffection:
Conceptualization and assessment of children’s behavioral and emotional participation in academic activities in the classroom. Educational and
Psychological Measurement, 69(3), 493-525.
7 Adapted from Vandell, D. (2000). Peer Social Support, Bullying, and Victimization (Form FLV05GS: Kids in My Class at School)
[measurement instrument]. NICHD Study of Early Child Care and Youth development: Phase III, 2000–2004.
8 Adapted from Parker, J. G. and Asher, S. R. (1993). Friendship and friendship quality in middle childhood: Links with peer group acceptance
and feelings of loneliness and social dissatisfaction. Developmental Psychology, 29(4), 611–621.
9 Adapted from the Pew September Tracking Survey 2009. Citation: Princeton Survey Research Associates International (2009). Pew September
Tracking Survey 2009. Pew Internet & American Life Project.
10 Adapted from the CENSHARE Pet Attachment Survey. Holcomb, R., Williams, R. C., and Richards, P. S. (1985). The elements of
attachment: Relationship maintenance and intimacy. Journal of the Delta Society, 2(1), 28-34.
11 Grade 8 Student Questionnaire, ECLS-K.
12 Adapted from the Short Grit Scale in collaboration with Angela Duckworth for the ECLS-K:2011.
13 Adapted from the Internalizing Problems Scale that was developed for ECLS-K and used in the ECLS-K grade 3 and grade 5 child-reported
Self-Description Questionnaire and the Grade 8 Student Questionnaire.
14 Adapted from the Self-Disclosure & Parental Monitoring/Knowledge Scale (Kerr and Stattin, 2000). Kerr, M., and Stattin, H. (2000). What
parents know, how they know it, and several forms of adolescent adjustment: Further support for a reinterpretation of monitoring. Developmental
Psychology, 36, 366-380.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2014, spring 2015, and spring 2016.
2-102.1.3 Parent Interview
A parent interview was conducted during the spring of fifth grade. Unlike the kindergarten,
first-grade, and second-grade data collections that had both fall and spring interviews, an interview was
not conducted in the fall of subsequent rounds of the study. The average length of the spring fifth-grade
parent interview was approximately 47 minutes. The spring fifth-grade parent interview was slightly
longer than the fourth-grade (34 minutes) interview to incorporate questions needed for the final round of
the study. For example, in the last round of the study it was of interest to ask parents questions about
issues that may have changed since they were first asked (e.g., the primary language spoken in the home
or parents’ educational expectations for their child) or could be different for parents of older children than
for parents of younger children (e.g., barriers to parent participation with the school).
The spring fifth-grade parent interview included many of the same questions that were
included in earlier rounds of the study, for example, questions about parent involvement in the child’s
school; homework; time children spent playing video games; children’s participation in out-of-school
activities; whether there had been a change in the relationship of one of the parent figures to the child
(e.g., adoption); and child health and well-being. In addition, information about children’s country of
origin was collected if it had not been collected in earlier rounds. All questions that were new to the
fourth-grade data collection were retained in the fifth grade (questions about parents’ use of a computer or
other electronic device to find out about children’s homework, school assignments, grades, and how
children at the school were doing as a group, parent reports of the child’s grades, the frequency that the
child avoids school, family monitoring of what the child looks at online and how many hours are spent
online, children’s friendships, how frequently the parent and child argue, and overall life stress in the past
12 months). Also, several questions from earlier rounds of the study that had not been fielded in recent
rounds were included in order to have a final data point in the study (e.g. parent’s educational
expectations for the child, marital/partner satisfaction, use of a language other than English, and outings
with the child). Lastly, several questions about animals and their use to help children with disabilities
were added to the child’s health and well-being section.
Exhibit 2-3 shows the content areas included in the parent interview in the fall and spring of
three grades (kindergarten, first grade, and second grade) and in the spring of third, fourth, and fifth
grades, by data collection round. While many of the same topics were addressed in multiple rounds, there
were some differences in the specific questions asked for each topic. For example, there was only one
question about employment in the spring of third grade and the spring of fourth grade, but there were
multiple questions about employment in earlier interviews. Also, questions about whether parents were on
2-11active duty in the military were asked in the employment section of the spring third-grade, spring fourth-
grade, and spring fifth-grade parent interviews, but were not asked in earlier interviews.
Exhibit 2-3. Parent interview topics, by round of data collection in the ECLS-K:2011: School years
2010–11, 2011–12, 2012–13; spring 2014; spring 2015; and spring 2016
Parent interview topics
Fall
kinder-
garten
Spring
kinder-
garten
Fall
first
grade
Spring
first
grade
Fall
second
grade
Spring
second
grade
Spring
third
grade
Spring
fourth
grade
Spring
fifth
grade
Child care arrangements1 X X X X X X X X X
Child demographic
characteristics2
X X X X X X X X X
Child disabilities and
services3
X X X X X X X X
Child friendships X X
Child health and well-being X X X X X X X X
Child mobility X X X X X X X X
Child school avoidance X X
Child social skills, problem
behaviors, and
approaches to learning4
X X X X X
Country of origin of parent
and child5
X X X X X X
Family structure X X X X X X X
Food sufficiency and food
consumption
X X X X X
Household roster X X X X X X X
Home environment, activities,
resources, and cognitive
stimulation6
X X X X X X X X X
Home language5 X X X X X X
Involvement of nonresident
parent
X X X X X X X
Neighborhood safety X X X
Parent characteristics X X X X X X X
Parenting stress X X X
Parent education5 X X X X X X
Parent employment7 X X X X X X
Parent income and assets X X X X X X
Parent involvement with the
child’s education
X X X X X X X
Parent marital history5 X X
Parent marital status X X X X X X X
Parent respondent’s
psychological well-being
and health
X X X X X
Parent social support X
See notes at end of exhibit.
2-12Exhibit 2-3. Parent interview topics, by round of data collection in the ECLS-K:2011: School years
2010–11, 2011–12, 2012–13; spring 2014; spring 2015; and spring 2016—Continued
Parent interview topics
Fall
kinder-
garten
Spring
kinder-
garten
Fall first
grade
Spring
first
grade
Fall
second
grade
Spring
second
grade
Spring
third
grade
Spring
fourth
grade
Spring
fifth
grade
Parental beliefs and
expectations related to
child’s education
X X X
Parental discipline, warmth,
and emotional
supportiveness
X X X X X X
Peer victimization X X
Time father/other adult male
spends with child
X
Welfare and other public
transfers
X X X X X X
1 In the fall of kindergarten, questions were asked about current child care and child care in the year before kindergarten. In the spring of
kindergarten, questions about child care in the year before kindergarten were asked if information had not been collected in the fall. In the fall of
first and second grades, questions were about child care during the previous summer. In the spring of first, second, third, and fifth grades,
questions asked about current child care. In the spring of fourth grade, the only child care questions asked were those about whether the child
regularly took care of him or herself and, if so, how much time the child spent in self-care.
2 Questions about child demographic characteristics were asked in the fall and spring of kindergarten and then asked in later rounds of the study
if the information was missing from a previous round. Questions about the child’s specific ethnic origin were first asked in the spring third-grade
parent interview; if the information was not provided in the spring of third grade, the questions were asked again in the spring fourth-grade parent
interview.
3 Questions in the fall first- and second-grade interviews were about services for special needs or participation in a special education program
over the previous summer. Questions about disabilities and services in other rounds of the study were not limited to the past summer.
4 In the spring of third grade and the spring of fourth grade, the questions in this section were about working memory. In previous rounds of the
study, the questions were about social skills, behavior, and approaches to learning.
5 Asked if information had not been collected in a previous round. In the spring of fourth and fifth grades, the country of origin of the resident
parent(s) was no longer asked.
6 Questions in the fall first- and second-grade interviews were about home activities, outings with family members, camps, and summer school
during the previous summer. Questions in other rounds of the study were not limited to the summer.
7 In the spring of third and fourth grades, employment was asked about in a single question about whether the parent figure(s) worked part-time,
full-time, were a stay-at-home parent or guardian, or not working. In other rounds of the study, multiple questions about employment and
occupation were asked.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
The parent interview was conducted by telephone for most cases; 5 percent were conducted
as in-person interviews. The respondent to the parent interview was usually a parent or guardian in the
household who identified himself or herself as the person who knew the most about the child’s care,
education, and health. During the spring fifth-grade data collection round, interviewers attempted to
complete the parent interview with the same respondent who had completed the parent interview in the
previous rounds. Another parent or guardian in the household who knew about the child’s care, education,
and health was selected if the previous respondent was not available.
The parent interview was fully translated into Spanish before data collection began and was
administered by bilingual interviewers if parent respondents preferred to speak in Spanish. The parent
interview was not translated into other languages because it was cost prohibitive to do so. However,
2-13interviews were completed with parents who spoke other languages by using an interpreter who translated
the English version during the interview.
2.1.4 General Classroom Teacher Questionnaires
Teacher questionnaires were completed in the spring fifth-grade data collection (spring
2016) by one or more of each child’s classroom teachers as described below. The purposes of these
questionnaires were (1) to gather information about the classroom environments and experiences that may
relate to children’s academic and social development and (2) to obtain information from the teacher’s
perspective about the child’s academic and social development.
The ECLS-K:2011 made a major change in its approach to collecting the teacher
questionnaire data starting in fourth grade. This procedure was described in the Early Childhood
Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011) User’s Manual for the ECLS-K:2011
Kindergarten–Fourth Grade Data File and Electronic Codebook, Public Version (NCES 2018-032)
(Tourangeau et al. 2018b) and is repeated here as a convenience for data file users. In general, as children
move into the upper elementary grades, more than one teacher is involved in a given child’s instruction.
Although in some schools children may have one teacher who teaches them all subjects, it becomes more
common for children in upper elementary grades to have different teachers for at least a few subject areas,
such as reading and language arts, mathematics, science, and/or social studies. There are variations of this
model with multiple teachers providing instruction implemented in schools. For example, students may
have had a different teacher for each subject taught or they may have had one primary teacher for most
subjects and a single other teacher for one subject (e.g., science). In short, it cannot be assumed that each
ECLS-K:2011 child had only one regular classroom teacher who could respond to questions about the
instruction of all subjects and the child’s performance in all subjects.
In order to accommodate this variation in organization for instruction, for the spring 2015
fourth-grade and spring 2016 fifth-grade data collections, the same approach for collecting the teacher
questionnaire data that was used in the fifth-grade round of the ECLS-K was followed. All sampled
children had their reading teacher identified, and that teacher was asked to complete questionnaires.
Information was also collected from children’s mathematics and science teachers. To reduce the response
burden on teachers, half of the sampled children were randomly assigned to have their mathematics
teacher complete questionnaires, while the other half of the sampled children were randomly assigned to
have their science teacher complete questionnaires. Thus, every child had a reading teacher and either a
mathematics or a science teacher identified for him or her. If a child had the same teacher for both reading
2-14and mathematics (for those selected for the mathematics teacher questionnaire) or for both reading and
science (for those selected for the science teacher questionnaire), that same teacher was asked to provide
information on both subjects. The random assignment to have a mathematics or science teacher complete
a questionnaire was conducted in the fourth-grade data collection and used again in the fifth-grade data
collection. Thus, if a child was selected to have the mathematics teacher complete a questionnaire in the
fourth grade, the child was also selected to have the mathematics teacher complete a questionnaire in the
fifth grade.
All identified teachers received a self-administered teacher-level questionnaire that collected
information about the teacher. Teachers were also asked to complete another questionnaire with questions
about the study child and the teachers’ classrooms. This second questionnaire had many items tailored to
the specific subject (reading, mathematics, or science) the teachers taught to study children.
Teacher Questionnaire, Teacher Level
The teacher-level teacher questionnaire asked teachers to provide information on the subjects
he or she taught, use of class time by subject area, school climate, the teacher’s sense of efficacy and job
satisfaction, and background information (e.g., education, certification, teaching experience). In the
exhibits below, content included in the teacher-level questionnaire is marked with “A8” for the spring of
fourth grade and “A9” for the spring of fifth grade. The character “A” is the first character in the names of
variables included on the data file that contain information collected through the teacher-level
questionnaire.
Teacher Questionnaire, Child and Classroom Level
The child- and classroom-level questionnaire consisted of two parts: part 1 containing child-
specific questions and part 2 containing classroom-specific questions. Separate questionnaires were
developed for reading teachers, for mathematics teachers, and for science teachers.
Part 1: Child-specific questions. Each teacher was asked to answer questions about a
specific ECLS-K:2011 study child in their classroom in part 1 of the child- and classroom-level
questionnaire. If a teacher had multiple ECLS-K:2011 study children in his or her classroom, the teacher
received different questionnaires for each child and was asked to complete the questions in Part 1 for each
child. The questionnaires for mathematics and science teachers contained only a few child-level questions
2-15specifically related to mathematics or science, respectively. Because each child’s reading teacher
completed a child- and classroom-level teacher questionnaire, the reading teacher was asked to answer
additional child-level questions that were not included in the mathematics and science teacher
questionnaires. Specifically, the reading teacher questionnaire contained questions related not only to
reading but also to the child’s academic and social skills, classroom behaviors, and peer relationships.
There were also questions in all three reading, mathematics, and science teacher questionnaires asking for
child-specific instructional information (for example, instructional group placement and additional
services the child receives).
Part 2: Classroom-specific questions. The questions in the classroom section of the child-
and classroom-level teacher questionnaire pertained to the reading, mathematics, or science class in which
the sampled student was taught. Specifically, teachers were asked to indicate how much time was spent
on specific skills and activities in that subject area, and to answer questions on instruction and grading
practices, behavioral issues, and homework assignments.
Since one teacher could instruct multiple study children in the same class and would be
given multiple child- and classroom-level questionnaires, data collection procedures were implemented to
minimize teacher burden by not asking teachers to answer questions about the same class for multiple
children. One “key child” was identified for each subject and class. Teachers were asked to complete the
classroom-level questions in Part 2 of the questionnaire only for the “key child.” Part 2 questions were
left unanswered in questionnaires for other students in the same class as the “key child.” If a teacher
taught more than one section/class containing an ECLS-K:2011 student for a given subject, a “key child”
was identified for each of the sections/classes, and the teacher was asked to complete the classroom
questions in part 2 about each of the sections/classes.
The classroom-specific questions focused on the concepts and skills in each subject area.
The ECLS-K:2011 items that asked teachers about reading and mathematics in the kindergarten data
collections came from the ECLS-K. The reading and mathematics concepts and skills asked of teachers in
later rounds of the ECLS-K:2011 were based on the Common Core State Standards.
6 Beginning in fourth
grade, the parallel items in the science teacher questionnaire relied on the Next Generation Science
6 See www.corestandards.org for further information. An effort led by state governors and state commissioners of education to develop the
Common Core State Standards for kindergarten through grade 12 was begun in 2009, through the National Governors Association Center for
Best Practices and the Council of Chief State School Officers.
2-16Standards.7 These two sets of standards are nationally recognized and were developed collaboratively by
state departments of education and subject-matter specialists. The classroom-level questions also gathered
information on instruction and grading practices, classroom behavioral issues, and homework
assignments in the key child’s classroom.
In the exhibits below, content included in the child- and classroom-level questionnaires is
marked with “G8” (reading), “M8” (mathematics), and/or “N8” (science) in the spring of fourth grade and
“G9” (reading), and “M9” (mathematics), and/or “N9” (science) in the spring of fifth grade. The character
G, M, or N is the first character in the names of variables included on the data file that contain
information collected through the child- and classroom-level questionnaires provided to reading teachers,
mathematics teachers, and science teachers, respectively.
Taken together, the content of the various teacher questionnaires in the spring 2014 third-
grade, the spring 2015 fourth-grade, and the spring 2016 fifth-grade teacher questionnaires are much the
same. The topics were reorganized across the child- and classroom-level teacher questionnaires in the
fourth and fifth grades.
Only one classroom-level item was added to the fifth-grade teacher questionnaires that did
not appear in the fourth grade. The fifth-grade reading teacher was asked about the use of a school library
or media center. This item had appeared in the kindergarten, first-grade, and third-grade questionnaires.
Classroom-level items related to formal assessments in reading, mathematics, and science
continued to be asked in fifth grade. However, to make room for questions about parental involvement,
some of the classroom-level items related to Response to Intervention programs were eliminated in the
fifth-grade questionnaires:
 Specialists or special education teachers providing direct instructions to students who
are struggling or at risk of failure in reading/mathematics/science;
 Views on the school’s benchmarks and criteria in reading/mathematics/science
performance;
 Support received for reading/mathematics/science instruction; and
7 See www.nextgenscience.org for further information. The Next Generation Science Standards (NGSS) is a multi-state effort to create new
science education standards for grades K-12 that are grounded in the most current research on science and scientific learning, which was outlined
in the report Framework for K-12 Science Education that was released in 2011 from the National Academies of Science, a non-governmental
organization whose mission is to advise the nation on scientific and engineering issues. In 2013, the NGSS were released for states to consider for
adoption.
2-17 Support received for delivery of effective behavioral supports, collection and
management of assessment data, and use of assessment data to guide instruction.
Several child-level items on parental involvement were added to the fifth-grade reading,
mathematics, and science teacher questionnaires. Although these items did not appear in the fourth-grade
questionnaires, they had been included in the child-level teacher questionnaire in kindergarten, first grade,
and second grade. The parental involvement items added in fifth grade included the following:
 Parent participation in specific activities,
 Teacher’s communication with the child’s parents, and
 Purposes of communications with parents.
One more child-level item was added to the fifth-grade reading teacher questionnaire. The
child’s reading teacher was asked whether the child received special accommodations (e.g., for a
disability or limited English proficient) to participate in the school’s testing or assessment program. This
item had been included in the child-level teacher questionnaire in the earlier grades of the study
(kindergarten, first grade, second grade) but not in third or fourth grade. No child-level topics asked in
previous rounds were eliminated from the fifth-grade questionnaires.
Child-level child behavior topics were added based on discussions with experts who
participated in a Technical Review Panel meeting in November of 2013. New child-level topics added for
fourth and fifth grades included:
 student’s school liking and avoidance,
 teacher ratings of child’s peer group, and
 student’s social skills with peers.
Exhibits 2-4 and 2-5 show the teacher- and child-level topics addressed in the kindergarten
through fifth-grade teacher- and child-level questionnaires, respectively, by data collection round. As
noted in text above, abbreviations in the fourth-grade and fifth-grade columns (which are defined in the
notes to the tables and which match the relevant data file prefix) indicate in which of the fourth-grade and
fifth-grade teacher questionnaires a particular topic was addressed. Although the same topics are included
across rounds, the actual items can vary by data collection round.
2-18Exhibit 2-4. General classroom teacher teacher-level questionnaire topics, by round of data collection in
the ECLS-K:2011: School years: 2010–11, 2011–12, 2012–13; spring 2014; spring 2015;
and spring 2016
Teacher-level questionnaire topics
Fall
kinder-
garten
Spring
kinder-
garten
Spring
first grade
(first-
grade
version)
Spring
first grade
(kinder-
garten
version)
Spring
second
grade
Spring
third
grade
Spring
fourth
grade1
Spring
fifth
grade2
Classroom and student
characteristics
X X X X X X G8/M8/
N8
G9/M9/
N9
Class type (half day or full day) X X
Class organization and resources, X X X X X X
Availability of computers,
Internet
X X X X
Use of technology X X X X G8/M8/
N8
G9/M9/
N9
Instructional activities X X X X X A8/G8/
M8/N8
A9/G9/
M9/N9
Instruction for English language
learners
X X X X X
Content coverage for language
arts
X X X X X4 G8 G9
Content activities for reading and
language arts
X X G8 G9
Content coverage for mathematics X X X X X4 M8 M9
Content activities for mathematics X X M8 M9
Content coverage for science X X X X X4 N8 N9
Content activities for science N8 N9
Activities and resources related to
Response to Intervention
programs
X X X X G8/M8/
N8
G9/M9/
N9
Teacher evaluation and grading
practices
X X X X X A8 A9
See notes at end of exhibit.
2-19Exhibit 2-4. General classroom teacher teacher-level questionnaire topics, by round of data collection in
the ECLS-K:2011: School years: 2010–11, 2011–12, 2012–13; spring 2014; spring 2015;
and spring 2016—Continued
Teacher-level questionnaire topics
Fall
kinder-
garten
Spring
kinder-
garten
Spring
first grade
(first-
grade
version)
Spring
first grade
(kinder-
garten
version)
Spring
second
grade
Spring
third
grade
Spring
fourth
grade1
Spring
fifth
grade2
Parent involvement X X X X X A8 A9
Meeting with other teachers X
Respect from and cooperation
with other teachers
X X X X X
Opportunities for professional
development
X X X X X X G8/M8/
N8
G9/M9/
N9
Teacher’s views on teaching,
school climate, and
environment
X X X X X X A8 A9
Teacher’s experience, education,
and background
X X3 X X X X A8 A9
1, 2 For grades 4 and 5, teacher questionnaires were reorganized by subject area, which resulted in a mix of teacher-level and child-level content
within the three subject area questionnaires. To indicate the location of the identified content within questionnaires, the columns for fourth- and
fifth-grade indicate the name of the questionnaire(s) by using the data file prefix appropriate to the file in which the data can be found. The prefix
for each data file, corresponding to its questionnaire, is as follows:
For fourth grade:
A8: Spring 2015 Fourth-Grade Teacher Questionnaire
G8: Spring 2015 Fourth-Grade Reading and Language Arts Teacher Questionnaire
M8: Spring 2015 Fourth-Grade Mathematics Teacher Questionnaire
N8: Spring 2015 Fourth-Grade Science Teacher Questionnaire
For fifth grade:
A9: Spring 2016 Fifth-Grade Teacher Questionnaire
G9: Spring 2016 Fifth-Grade Reading and Language Arts Teacher Questionnaire
M9: Spring 2016 Fifth-Grade Mathematics Teacher Questionnaire
N9: Spring 2016 Fifth-Grade Science Teacher Questionnaire
3 In the spring of kindergarten, teachers new to the study were asked to complete a supplemental teacher-level questionnaire in order to collect
information on their experience, education, and background that had been collected from other teachers in the fall. Teachers who provided
information in the fall were not asked the same questions again in the spring.
4 In spring third grade, these items were contained in a separate questionnaire to facilitate obtaining responses from multiple teachers, if
applicable.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, spring 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
2-20Exhibit 2-5. General classroom teacher child-level questionnaire topics, by round of data collection in
the ECLS-K:2011: School years 2010–11, 2011–12, 2012–13; spring 2014; spring 2015;
and spring 2016
Child-level
questionnaire topics
Fall
kinder-
garten
Spring
kinder-
garten
Fall
first
grade
Spring
first
grade
Spring
first grade
(kinder-
garten
version)
Fall
second
grade
Spring
second
grade
Spring
third
grade
Spring
fourth
grade1
Spring
fifth
grade1
Student and enrollment
information
X X X X X X X X G8/
M8/N8
G9/
M9/N9
Summer assignments X X
Language and literacy
skills and
knowledge
X X X X X X
Mathematical thinking
skills and
knowledge
X X X X
Science skills and
knowledge
X X
Overall academic
rating
X X
Overall academic
rating, by subject
X X G8/
M8/N8
G9/
M9/N9
Social skills X X X X X X X G8 G9
Approaches to learning X X X X X X X G8 G9
Attention focusing and
inhibitory control
X X X X X G8 G9
School liking and
avoidance
G8 G9
Student-teacher
relationship
X X X X
Peer relationships X G8 G9
Peer victimization
(child as victim
and child as
aggressor)
X X G8 G9
Working memory X
Specialized programs
and services for
the child
X X X X X G8/
M8/N8
G9/
M9/N9
Prediction of child’s
ultimate
educational
attainment
X X X
See notes at end of exhibit.
2-21Exhibit 2-5. General classroom teacher child-level questionnaire topics, by round of data collection in
the ECLS-K:2011: School years 2010–11, 2011–12, 2012–13; spring 2014; spring 2015;
and spring 2016—Continued
Child-level
questionnaire topics
Fall
kinder-
garten
Spring
kinder-
garten
Fall
first
grade
Spring
first
grade
Spring
first grade
(kinder-
garten
version)
Fall
second
grade
Spring
second
grade
Spring
third
grade
Spring
fourth
grade1
Spring
fifth
grade1
Parent involvement X X X X X G8 G9/
M9/N9
Child’s primary
teacher in reading,
mathematics,
science, and
social studies2
X X X X G8/
M8/N8
G9/
M9/N9
1 For grades 4 and 5, teacher questionnaires were reorganized by subject area, which resulted in a mix of teacher-level and child-level content
within the three subject area questionnaires. To indicate the location of the identified content within questionnaires, the columns for fourth- and
fifth-grade indicate the name of the questionnaire(s) by using the data file prefix appropriate to the file in which the data can be found. The prefix
for each data file, corresponding to its questionnaire, is as follows:
For fourth grade:
A8: Spring 2015 Fourth-Grade Teacher Questionnaire
G8: Spring 2015 Fourth-Grade Reading and Language Arts Teacher Questionnaire
M8: Spring 2015 Fourth-Grade Mathematics Teacher Questionnaire
N8: Spring 2015 Fourth-Grade Science Teacher Questionnaire
For fifth grade:
A9: Spring 2016 Fifth-Grade Teacher Questionnaire
G9: Spring 2016 Fifth-Grade Reading and Language Arts Teacher Questionnaire
M9: Spring 2016 Fifth-Grade Mathematics Teacher Questionnaire
N9: Spring 2016 Fifth-Grade Science Teacher Questionnaire
2 The teacher who responded to the child-level teacher questionnaire was asked to indicate for each of these subject areas whether he or she was
the child’s primary teacher for the subject.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
2.1.5 Special Education Teacher Questionnaires
As was done in each year from kindergarten through fourth grade, a set of special education
teacher questionnaires was completed in the spring of the fifth-grade year for each participating child with
an Individualized Education Program (IEP) or equivalent program on record with the school. The
respondent to the questionnaire could have been a staff member identified as the child’s special education
teacher, a related service provider if the child was not taught by a special education teacher, or the child’s
general classroom teacher if that teacher provided all of the child’s education and services required by the
IEP. Two self-administered hard-copy instruments were used, a teacher-level questionnaire and a child-
level questionnaire.
The special education teacher-level questionnaire collected information on the special
education teacher’s background, education, teaching experience, teaching position, and caseload. The
2-22special education child-level questionnaire addressed the following topics: current services received
through an IEP, child’s disabilities (primary disability and all those for which the child received services),
IEP goals and whether the child was meeting those goals, classroom placement, expectations regarding
general education goals, the special education teacher’s communication with other teachers and the
child’s parents, grade placement, and participation in assessments.
The same items appeared in both the fourth-grade and fifth-grade special education teacher
questionnaires. Exhibit 2-6 shows the topics addressed in the kindergarten through fifth-grade special
education teacher-level and child-level questionnaires by data collection round.
Exhibit 2-6. Special education teacher questionnaire topics, by round of data collection in the
ECLS-K:2011: Spring 2011, spring 2012, spring 2013, spring 2014, spring 2015, and
spring 2016
Special education teacher questionnaire topics
Spring
Kinder-
garten
Spring
first
grade
Spring
second
grade
Spring
third
grade
Spring
fourth
grade
Spring
fifth
grade
Teacher-level topics
Teacher characteristics X X X X X X
Teacher education and experience X X X X X X
Teacher position, assignment, and caseload X X X X X X
Child-level topics
Prekindergarten services received through an
Individualized Education Program (IEP)
X
Current services received through an IEP X X X X X X
Child’s disabilities (primary disability and those for
which services have been received)
X X X X X X
Goals of the child’s IEP and extent to which goals
have been met
X X X X X X
Classroom placement X X X X X X
Special education teacher’s communication with
other teachers and the child’s parents
X X X X X X
Expectations regarding general education goals X X X X X X
Grade placement X X X X X
Participation in assessments X X X X X X
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2011, spring 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
2-232.1.6 School Administrator Questionnaires
There was a single version of the school administrator questionnaire for fifth grade. In first
grade through fourth grade, there were two versions of the school administrator questionnaire: (1) a
version for schools that were new to the study or for which a completed school administrator
questionnaire was not received in a prior data collection and (2) a shorter version for schools for which a
school administrator questionnaire had been completed in a prior year. Using a single version in fifth
grade provided an opportunity to obtain the full set of school-level data for all schools for the final data
collection round of the study.
The school administrator questionnaire was a hard-copy paper questionnaire completed by
the school principal/administrator and/or his or her designee during the spring data collection round of the
fifth-grade year. The school administrator questionnaire addressed the following topics: school
characteristics; school-family-community connections; school policies and practices; school programs for
particular populations (language minority children and children with special needs); federal programs;
staffing and teacher characteristics; and school administrator characteristics and background.
The single fifth-grade school administrator questionnaire was based on the fourth-grade
version for schools new to the study, which was the longer version of the two fourth-grade school
administrator questionnaires. Compared with that fourth-grade questionnaire, several items were added
for fifth grade. The added items were not new to the study; they had been included in one or more
previous rounds of data collection. The items added to the fifth-grade school administrator questionnaire,
along with the grade and questionnaire section in which they had last been included, are the following:
 Neighborhood school vs. other catchment system (kindergarten, “school
characteristics”);
 Number of students that the school is designed to accommodate (second grade,
“school characteristics”);
 Programs and services (e.g., before/after school care, health screenings, adult literacy
program, etc.) at the school site (second grade, “school-family-community
connections”);
 Hearing and vision screening services at the school site (second grade, “school-
family-community connections”);
 Grade retention and promotion policies and practices (second grade, “school practices
and policies); and
2-24 Race/ethnic distribution of teaching staff (second grade, “staffing and teacher
characteristics”).
Two items were omitted for fifth grade that had been asked in fourth grade in schools with
an implemented Response to Intervention program:
 Number of years a Response to Intervention program had been implemented.
 Response to Intervention program information provided to parents.
Exhibit 2-7 shows the topics addressed in the kindergarten through fifth-grade school
administrator questionnaires by data collection round, with separate columns for new schools and
returning schools.
2-25School characteristics X X X X X X X X X X
Facilities and resources X X X X X X
School-family-community
connections
X X X X X X X X X X
School policies and
practices
X X X X X X X X X X
Response to Intervention
programs
X X X X X X X X X
School programs for
particular populations
(language minority
children and children
with special needs)
X X X X X X X X X X
2-26
Exhibit 2-7. School administrator questionnaire topics, by round of data collection in the ECLS-K:2011: Spring 2011, spring 2012, spring 2013,
spring 2014, spring 2015, and spring 2016
School administrator
questionnaire topics
Spring
kinder-
garten
Spring
first
grade
(new
schools)
Spring
first
grade
(returning
schools)
Spring
second
grade
(new
schools)
Spring
second
grade
(returning
schools)
Spring
third
grade
(new
schools)
Spring
third
grade
(returning
schools)
Spring
fourth
grade
(new
schools)
Spring
fourth
grade
(returning
schools)
Spring
fifth
grade
Federal programs X X X X X X X X X X
Staffing and teacher
characteristics
X X X X X X X X X X
School administrator
characteristics and
background
X X X X X X X X X X
NOTE: New schools were generally asked more questions about a topic than returning schools. Although questionnaire topics were the same between new and returning schools in a given year, one
exception was that the third-grade school administrator questionnaire for new schools contained questions about school facilities and resources but the third-grade school administrator questionnaire for
returning schools did not have any questions on this topic.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), spring 2011, spring 2012,
spring 2013, spring 2014, spring 2015, and spring 2016.2.1.7 Copyrighted Materials
A number of the measures used in the ECLS-K:2011 assessment and questionnaires were
taken directly or adapted from copyrighted instruments. Exhibit 2-8 lists these copyrighted instruments
and identifies the copyright holder for each.
Exhibit 2-8. Copyright-protected instruments in ECLS-K:2011
Instrument Publisher/copyright holder
Direct child assessment
Peabody Individual Achievement Test – Revised (PIAT-R) Peabody Picture Vocabulary Test – 3rd Edition (PPVT-III) Test of Early Mathematics Ability – 3rd edition (TEMA-3) Test of Early Reading Ability – 3rd edition (TERA-3) Test of Preschool Early Literacy (TOPEL) Woodcock-Johnson Psychoeducational Battery, Third Edition
(WJ-III) – Applied Problems Test
Woodcock Johnson Psychoeducational Battery, Third Edition
(WJ-III) Tests of Cognitive Abilities – Numbers Reversed
Task
Child questionnaire
Self Description Questionnaire I (SDQI) Social Anxiety Scale for Children—Revised Domain-Specific Life Satisfaction Survey from the
NIH Toolbox Emotion Battery
Parent instruments
Social Skills Rating System (SSRS)
Behavior Rating Inventory of Executive Function (BRIEF)
Teacher instruments
Social Skills Rating System (SSRS) Behavior Rating Inventory of Executive Function (BRIEF) Teacher instruments
Children’s Behavior Questionnaire (CBQ) Temperament in Middle Childhood Questionnaire (TMCQ) Student-Teacher Relationship Scale (STRS) Social Skills Rating System (SSRS) Behavior Rating Inventory of Executive Function (BRIEF) Child Behavior Scale Classroom Environment Student Difficulties Scale Pearson Education, Inc.
Pearson Education, Inc.
PRO-ED, Inc.
PRO-ED, Inc.
PRO-ED, Inc.
The Riverside Publishing Company/HMH
Assessments1
The Riverside Publishing Company/HMH
Assessments1
Herbert Marsh
Annette M. La Greca
Northwestern University and the National
Institutes of Health
Pearson Education, Inc.
Psychological Assessment Resources, Inc.
Pearson Education, Inc.
Psychological Assessment Resources, Inc.
Samuel Putnam and Mary Rothbart
Jennifer Simonds and Mary Rothbart
Robert C. Pianta
Pearson Education, Inc.
Psychological Assessment Resources, Inc.
Gary W. Ladd
T. Abry, J. Swanson, and R. A. Fabes
1 Riverside Publishing Company, which was associated with Houghton Mifflin Harcourt, was the copyright holder when ECLS-K:2011 made the
copyright agreement. Subsequently, Riverside Publishing Company became HMH Assessments.
NOTE: There are no copyrighted items included in the questionnaires for special education teachers and school administrators.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K: 2011).
2-272.2 Data Collection Methods
The data collection methods used for the spring fifth-grade round of the ECLS-K:2011 were
the same as those used in previous rounds, with just a few exceptions described below. Please refer to the
Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), User’s Manual for
the ECLS-K:2011 Kindergarten Data File and Electronic Codebook, Public Version (NCES 2015-074)
(Tourangeau et al. 2015a), for an overview of the general study procedures for school recruitment, field
staff training, school contact in the fall, data collection, tracing activities, and data collection quality
control.
2.2.1 Comparison of Data Collection Methods Used in Fifth Grade to Those Used in Earlier
Data Collection Rounds
School recruitment. Fifth-grade school recruitment followed the same procedures used in
fourth-grade school recruitment. Data collection staff team leaders8 recruited only new transfer schools,
meaning those schools to which study children moved between fourth grade and the spring of fifth grade.
Recruitment was not repeated for schools that had participated in the kindergarten, first-, second-, third-,
or fourth-grade years.
Field staff training. Training for the fifth-grade data collection was similar to the training
for the spring fourth-grade collection. Both team leaders and assessors completed a home study prior to
attending in-person training. Both team leaders and assessors were trained on the parent interview, the
child assessment, and the child questionnaire during a 6-day, in-person training. Child assessment and
child questionnaire training included interactive sessions, individual practice, and role plays with partners.
In the spring of fifth grade, all team leaders were trained via the Learning Management System (LMS), an
online learning platform that delivers and tracks assigned trainings in a browser environment. New team
leaders participated in an additional 1-day, in-person training. Training for school recruiters for the fifth-
grade data collection was conducted via WebEx9 as was done in third and fourth grades.
Health technicians, who accompanied the teams into the schools to conduct the hearing
evaluations, were trained in a 5-day, in-person training. For more information on the hearing evaluations
component, see the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011)
8 The team leader is a specially trained ECLS-K:2011 staff member responsible for communicating with schools and making arrangements for
assessment activities and for leading a team of assessors in each school.
9 WebEx is an Internet-based web conferencing tool for sharing presentations in any format with an audience in multiple remote locations.
2-28User’s Manual for the Fifth-Grade Hearing Evaluations Component Data File (NCES 2019-019)
(Tourangeau et al., 2019).
Advance school contact in the fall. Advance school contact procedures for fifth grade
remained the same as those used in the fourth grade. The protocol for the collection of teacher
information implemented in fourth grade was used again in fifth grade. Each child was linked to a reading
teacher and to either a mathematics or science teacher, unlike in rounds prior to fourth grade where each
child was linked to one regular classroom teacher.
Data collection. Data collection procedures used in fifth grade were the same as those used
during the fourth-grade year. As described above, however, revisions were made to the instruments that
had been used in the earlier rounds. As in fourth grade, a child questionnaire was administered via an
audio computer-assisted self-interview (audio-CASI). The executive function component, the Flanker
task, which was added to the fourth-grade child assessment was also included in fifth grade. The Flanker
task measures inhibitory control. Also, the hearing evaluations component was conducted with the same
subset of children originally sampled for evaluations in the fall of second grade.
Tracing activities. Tracing activities for the fifth-grade round remained the same as those
used in earlier rounds.
Quality control. Quality control and validation procedures for the fifth-grade round
remained the same as those used in in earlier rounds.
2-29This page intentionally left blank.3. ECLS-K:2011 DIRECT AND INDIRECT ASSESSMENT DATA
This chapter provides information primarily about the direct and indirect assessment data
from the fifth-grade collection of the Early Childhood Longitudinal Study, Kindergarten Class of 2010–
11 (ECLS-K:2011). The chapter begins with a description of the direct cognitive assessments, providing
information about the scores available in the data file. The chapter then presents information on the
executive function assessments. In fifth grade, study children completed the same three direct measures of
executive function that were administered in fourth grade: a card sort task to assess cognitive flexibility, a
numbers reversed task to assess working memory, and a flanker task to assess inhibitory control. Next the
chapter presents information on the fifth-grade child questionnaire, which repeated some content from the
fourth-grade child questionnaire but also included new content. Finally, the chapter closes with
information on teacher- and parent-reported assessments of children’s cognitive and socioemotional
knowledge and skills.
This chapter includes information about assessment data from the kindergarten through fifth-
grade rounds of data collection in three instances: when those data have been changed since their release
on previous files, when new data from those rounds have been added to the kindergarten through fifth-
grade (K-5) data file, and when necessary to illustrate how fifth-grade data related to a particular measure
or construct differ from data related to the same measure or construct released for the earlier rounds.
Information about assessments that were used in prior rounds but not in fifth grade, for example the
Spanish Early Reading Skills (SERS) assessment, and about scores that were produced only for earlier
rounds, such as raw number-right scores, can be found in the Early Childhood Longitudinal Study,
Kindergarten Class of 2010–11 (ECLS-K:2011), User’s Manual for the ECLS-K:2011 Kindergarten Data
File and Electronic Codebook, Public Version (NCES 2015-074) (Tourangeau et al. 2015a), hereinafter
referred to as the base-year User’s Manual, the Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), User’s Manual for the ECLS-K:2011 Kindergarten–First Grade Data File
and Electronic Codebook, Public Version (NCES 2015-078) (Tourangeau et al. 2015b), the Early
Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), User’s Manual for the
ECLS-K:2011 Kindergarten–Second Grade Data File and Electronic Codebook, Public Version (NCES
2017-285) (Tourangeau et al. 2017), the Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), User’s Manual for the ECLS-K:2011 Kindergarten–Third Grade Data File and
Electronic Codebook, Public Version (NCES 2018-034) (Tourangeau et al. 2018a), and the Early
Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), User’s Manual for the
ECLS-K:2011 Kindergarten–Fourth Grade Data File and Electronic Codebook, Public Version (NCES
2018-032) (Tourangeau et al. 2018b).
3-13.1 Direct Cognitive Assessment: Reading, Mathematics, and Science
The direct cognitive assessments administered in each grade measured children’s knowledge
and skills in reading, mathematics, and science. This section presents information about the direct cognitive
assessment scores available in the data file. More detailed information about the development of the scores,
including a more complete discussion of item response theory (IRT) procedures, can be found in the Early
Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), Third-Grade, Fourth-
Grade, and Fifth-Grade Psychometric Report (NCES 2019-023) (Najarian et al. forthcoming). A
description of the administration of the direct assessments is provided in chapter 2 of this user’s manual.
It must be emphasized that the direct cognitive assessment scores described below are not
directly comparable with those developed for the Early Childhood Longitudinal Study, Kindergarten
Class of 1998–99 (ECLS-K). Although the IRT procedures used in the analysis of data were similar in the
ECLS-K and in the ECLS-K:2011, each study incorporated different items and the resulting scales are
different. A set of comparable scores between the ECLS-K and the ECLS-K:2011 is under development
and is scheduled for release at the end of the ECLS-K:2011 study.
3.1.1 IRT-Based Scores Developed for the ECLS-K:2011
Broad-based scores using the full set of items administered in the kindergarten, first-grade,
second-grade, third-grade, fourth-grade, and fifth-grade assessments in reading, mathematics, and science
were calculated using IRT procedures. IRT is a method for modeling assessment data that makes it
possible to calculate an overall score for each domain measured for each child that can be compared to
scores of other children regardless of which specific items a child is administered. This method was used
to calculate scores for the ECLS-K:2011 because, as discussed in chapter 2, the study employed a two-
stage assessment (in reading and mathematics in kindergarten and in reading, mathematics, and science in
first, second, third, fourth, and fifth grades) in which children were administered a set of items appropriate
for their demonstrated ability level rather than all the items in the assessment. Although this procedure
resulted in children being administered different sets of items, there was a subset of items that all children
received (the items in the routing tests, plus a set of items common across the different second-stage
forms). These common items were used to calculate scores for all children on the same scale.
IRT also was used to calculate scores for all children on the same scale for the science
assessment fielded in the spring of kindergarten even though that assessment was not two-stage. In that
assessment, the assortment of items a child received was not dependent upon routing to a second stage,
3-2but instead on omissions by the child or the discontinuation of the administration of the assessment. In
those cases, IRT was used to estimate the probability that a child would have provided a correct response
when no response was available.
IRT uses the pattern of right and wrong responses to the items actually administered in an
assessment and the difficulty, discriminating ability,1 and “guess-ability” of each item to estimate each
child’s ability on the same continuous scale. IRT has several advantages over raw number-right scoring.
By using the overall pattern of right and wrong responses and the characteristics of each item to estimate
ability, IRT can adjust for the possibility of a low-ability child guessing several difficult items correctly.
If answers on several easy items are wrong, the probability of a correct answer on a difficult item would
be quite low. Omitted items are also less likely to cause distortion of scores, as long as enough items have
been answered to establish a consistent pattern of right and wrong answers. Unlike raw number-right
scoring, which treats omitted items as if they had been answered incorrectly, IRT procedures use the
pattern of responses to estimate the probability of a child providing a correct response for each assessment
question. Finally, IRT scoring makes possible longitudinal measurement of gain in achievement, even
when the assessments that are administered to a child are not identical at each point, for example, when a
child was administered different levels of the second-stage form in the fall and spring data collections
within one year or different sets of items across grades.
3.1.1.1 Theta and the Standard Error of Measurement (SEM) of Theta
A theta score is provided in the ECLS-K:2011 data file for each child who participated in the
direct cognitive assessment for each cognitive domain assessed and for each data collection in which the
assessment was administered. The theta score2 is an estimate of a child’s ability in a particular domain
(e.g., reading, mathematics, or science) based on his or her performance on the items he or she was
actually administered. The theta scores are reported on a metric ranging from -4 to 4, with lower scores
indicating lower ability and higher scores indicating higher ability. Theta scores tend to be normally
distributed because they represent a child’s latent ability and are not dependent on the difficulty of the
items included within a specific test.
The standard error of theta provides a measure of uncertainty of the theta score estimate for
each child. Adding and subtracting twice the standard error from the theta score estimates provides an
1 The discriminating ability describes how well changes in ability level predict changes in the probability of answering the item correctly at a
particular ability level.
2 Theta is iteratively estimated and re-estimated and the theta score is derived from the means of the posterior distribution of the theta estimate.
3-3approximate 95 percent confidence interval or range of values that is likely to include the true theta score.
Unlike classical item theory, in which the precision of the scores is consistent across all examinees, IRT
allows the standard error to vary. Larger standard errors of measurement can be the result of estimations
of thetas in the extremes of the distribution (very low or very high ability) or for children who responded
to a limited number of items (i.e., children who responded to all items administered generally have lower
standard errors of measurement than those children responding to fewer items because more information
about their actual performance is available, thereby making estimates of their ability more precise).
Unlike prior longitudinal data files, every reading, mathematics, and science direct assessment
score on the K-5 longitudinal file has been re-estimated for each child, at each round, and in each domain,
due to a modification in the calibration methodology used for the final analyses once all of the data
collection was completed. In prior rounds, the method used to compute the theta scores resulted in the
calculation of theta for a given round that did not change based on later administrations of the assessments.
Therefore, for any given child, the theta scores provided in subsequent data files were the same as theta
scores released in earlier data files.3 This, however, is no longer the case on the K-5 longitudinal file.
As the fifth-grade round was the final round of collection, the methodology for the scaling of
the reading, mathematics, and science direct child assessments was reexamined to confirm that the scores
developed over the nine rounds of data collection accurately and precisely measured growth over time.
Three approaches for computing scores were explored.
 Unconstrained scaling solution. The unconstrained solution, which is the
methodology employed for the kindergarten through fourth-grade rounds, produces a
set of separate grade-specific scales and then aligns these scales on a common
metric. Data from each grade first are calibrated separately, and then the separate IRT
calibrations are aligned to the base year metric via a multiple chain linking approach
(Stocking and Lord, 1983). For example, data from fall and spring kindergarten were
calibrated together and data from fall and spring first grade were calibrated together.
Next, data from the first-grade calibration were aligned on the kindergarten scale.
Such an approach imposes few restrictions on IRT item parameters for items that
appear in multiple grades. It therefore preferences model-data fit at the cost of
requiring large numbers of item parameters to be estimated.
 Constrained scaling solution. A second approach, the constrained scaling solution,
employs a single IRT calibration based on a pooled data set that included assessment
3 One exception is the reading thetas provided in the base-year data file. After the kindergarten-year data collection, the methodology used to
calibrate and compute reading scores changed; therefore, the reading thetas reported in the base-year file are not the same as the kindergarten
reading thetas provided in the files with later-round data. Any analysis involving kindergarten reading theta scores and reading theta scores from
later rounds, for example an analysis looking at growth in reading knowledge and skills between the spring of kindergarten and the spring of first
grade, should use the kindergarten reading theta scores from data files released after the base year. The reading theta scores released in the
kindergarten-year data file are appropriate for analyses involving only the kindergarten-round data; analyses conducted with only data released in
the base-year file are not incorrect, since those analyses do not compare kindergarten scores to scores in later rounds that were computed
differently.
3-4data from every round of data collection and directly produces a set of results on a
single vertically aligned scale. This is the approach that had been employed in earlier
ECLS longitudinal studies (the ECLS-K and ECLS-B) and is most consistent with an
interpretation of a single common vertical scale. It makes the strongest assumptions
about the equality of IRT item parameters across multiple collection points. It,
therefore, preferences model parsimony and scale interpretability at the cost of greater
model-data misfit. The threat of greater model-data misfit comes from the assumption
that a common item characteristic curve may not adequately fit the item across
multiple rounds, especially for non-adjacent grades.
 Partially constrained scaling solution. A third approach, the partially constrained
scaling solution, represents a compromise between the grade-specific vertically
aligned scales (as produced for the ECLS-K:2011 kindergarten through fourth-grade
data) and a single vertical scale (as produced for the ECLS-K kindergarten through
eighth-grade data). For the partially constrained scaling solution, two separate IRT
calibrations were conducted – one for the combined data set from the first six rounds
(fall and spring kindergarten, fall and spring first grade, and fall and spring second
grade) and one for the combined data set from rounds 7 through 9 (spring third, spring
fourth, and spring fifth grades). Within each of these two calibrations, separate ability
distributions were estimated for each data collection round but a single common item
characteristic curve was estimated for each item. The results of these two scalings
were then vertically aligned using the Stocking-Lord procedure (Stocking and Lord,
1983). The partially constrained solution was intended to balance the tradeoff between
model-data fit and parsimony/interpretability of the resulting scales.
For the three scaling solutions for each assessment domain, both the scale score metric and
the IRT theta metric were evaluated by examining patterns of average growth, within-round correlations,
and patterns of change for within-grade standard deviations. Patterns of average growth were examined to
see if a single IRT metric was supported by the data in all three approaches. After placing all results
separately for each domain on a common metric, patterns of average growth are nearly identical under the
three approaches. Within-round correlations were considered to see if the approaches yielded similar
results. The within-round correlations between scores across the three scaling approaches were near
one. Patterns of change for within-grade standard deviations were examined in order to determine if the
approaches showed comparable patterns of variability. If differences were observed, that would suggest
compression in the vertical IRT scale under one or more approaches. In the theta metric, there were some
differences in the pattern of change for within-grade standard deviations for mathematics and
reading. These standards deviations were reduced at a steeper rate in the constrained and partially
constrained solutions than was the case for the unconstrained solution, meaning some compression in the
overall scale.4 The same was not observed for the theta metric science results. For the science results, the
4 IRT-based vertical scales may indicate that within-grade variability in test performance decreases over time, also known as “scale compression.”
The decreasing variability may be explained by the IRT ability estimation for low- and high-scoring students across grade-level assessments. If
scale compression exists, the ability estimates are “compressed” toward the mean, resulting in higher estimates of ability for the lower-ability
students and lower estimates of ability for the higher-ability students. This compression may result in estimates of growth for students in the tails
of the ability distribution that are higher or lower than expected. The constrained and partially-constrained solutions, by definition, pool data
3-5constrained and partially constrained standard deviations remained similar to the unconstrained approach,
meaning the science constrained and unconstrained approaches showed less scale compression than was
the case in reading and math. For all three subject areas, in the scale score metric little evidence of any
difference across scaling approaches in the magnitude or pattern of change in standard deviation was
evident. The shapes of the theta distributions in all but reading5 were quite similar and, for all three
subject areas, were very similar in the scale score metric. Despite the similarity of distribution shapes
across scaling for most data collection rounds and subjects, there are individual instances of scale
shrinkage and compression as a function of scaling approach.
Based on these findings, the partially constrained solution results were selected for the final
scores. This solution is less restrictive than the single vertical scale but provides a more parsimonious
summarization of the regularities in the data than using grade-specific scales. Compared to the
unconstrained solution, the partially constrained approach is better supported by the data structures (i.e.,
targeted blocks administered over multiple grades and, therefore, more and better data are available for
item parameter estimation), and is more consistent with the intended interpretation of a vertical
scale. See the ECLS-K:2011 Third-Grade, Fourth-Grade, and Fifth-Grade Psychometric Report
(Najarian et al. forthcoming) for details.
Tables 3-1 and 3-2 list the names of the variables pertaining to the reading, mathematics, and
science IRT theta scores and standard errors of measurement available in the data file, along with the
variable descriptions, value ranges, weighted means, and standard deviations.6 As can be seen in the
tables, theta scores are available for all data collection rounds for reading and mathematics. For science,
theta scores are available for all rounds except the fall of kindergarten; the science assessment was not
included in that first round of data collection. The variable names and descriptions end with K5,
indicating these are scores released on the kindergarten–fifth grade (K–5) longitudinal data file.
Because the recomputed kindergarten, first-grade, second-grade, third-grade, and fourth-
grade theta scores are available in the kindergarten through fifth-grade data file, it is recommended that
researchers conduct any new analyses with the recomputed kindergarten theta scores using this file. This
across grade levels and thus the overall theta abilities may be compressed within grade level but also by possible compression to the overall
mean. In the ECLS-K:2011 estimates, comparison of the scales across solutions did not show a significant compression in scale.
5 Small average differences (0.1 to 0.2 of a standard deviation) in the reading theta metric were observed across all rounds from fall kindergarten
through spring fourth grade. In spring of fifth grade, the average difference in the reading theta metric is closer to 0.3 of a standard deviation. In
math and science, the average differences in the theta metric were near 0.1 of a standard deviation or less in across all rounds.
6 The name and description for each variable in the tables begin with an “X,” indicating that it is a derived/calculated variable, and a data
collection round number (1 for the fall kindergarten round, 2 for the spring kindergarten round, 3 for the fall first-grade round, 4 for the spring
first-grade round, 5 for the fall second-grade round, 6 for the spring second-grade round, 7 for the spring third-grade round, and 8 for the spring
fourth-grade round, and 9 for the spring fifth-grade round). These variable naming conventions are used for all the variables mentioned in this
chapter. More information about variable naming conventions can be found in chapter 7.
3-6recommendation does not imply that analyses using the previous data file are incorrect or any less valid.
Any new analyses using the kindergarten through fifth-grade data file would include more precise
estimates of child abilities and item parameters since all of the available assessment data was used in
developing those estimates. For more information on the methods used to calculate theta scores, see the
ECLS-K:2011 Third-Grade, Fourth-Grade, and Fifth-Grade Psychometric Report (Najarian et al.
forthcoming).
Table 3-1. Direct cognitive assessment: Item Response Theory (IRT) theta scores, fall and spring
kindergarten, fall and spring first-grade, fall and spring second-grade, spring third-grade,
spring fourth-grade, and spring fifth-grade assessments: School years 2010–11, 2011–12,
2012–13; spring 2014; spring 2015; and spring 2016
Variable Description n
Range of
possible
values
Weighted
mean
Standard
deviation
X1RTHETK5 X1 READING THETA-K5 15,669 -4.0–+4.0 -1.24 0.792
X2RTHETK5 X2 READING THETA-K5 17,186 -4.0–+4.0 -0.29 0.660
X3RTHETK5 X3 READING THETA-K5 5,194 -4.0–+4.0 0.05 0.584
X4RTHETK5 X4 READING THETA-K5 15,115 -4.0–+4.0 0.55 0.493
X5RTHETK5 X5 READING THETA-K5 4,725 -4.0–+4.0 0.72 0.402
X6RTHETK5 X6 READING THETA-K5 13,837 -4.0–+4.0 0.94 0.360
X7RTHETK5 X7 READING THETA-K5 12,866 -4.0–+4.0 1.12 0.300
X8RTHETK5 X8 READING THETA-K5 12,074 -4.0–+4.0 1.29 0.295
X9RTHETK5 X9 READING THETA-K5 11,427 -4.0–+4.0 1.45 0.346
X1MTHETK5 X1 MATH THETA-K5 15,595 -4.0–+4.0 -1.15 0.702
X2MTHETK5 X2 MATH THETA-K5 17,143 -4.0–+4.0 -0.40 0.626
X3MTHETK5 X3 MATH THETA-K5 5,222 -4.0–+4.0 -0.03 0.594
X4MTHETK5 X4 MATH THETA-K5 15,103 -4.0–+4.0 0.51 0.554
X5MTHETK5 X5 MATH THETA-K5 4,729 -4.0–+4.0 0.68 0.523
X6MTHETK5 X6 MATH THETA-K5 13,830 -4.0–+4.0 1.04 0.528
X7MTHETK5 X7 MATH THETA-K5 12,866 -4.0–+4.0 1.42 0.462
X8MTHETK5 X8 MATH THETA-K5 12,080 -4.0–+4.0 1.64 0.464
X9MTHETK5 X9 MATH THETA-K5 11,426 -4.0–+4.0 1.83 0.464
X2STHETK5 X2 SCIENCE THETA-K5 16,936 -4.0–+4.0 -0.60 0.737
X3STHETK5 X3 SCIENCE THETA-K5 5,180 -4.0–+4.0 -0.32 0.809
X4STHETK5 X4 SCIENCE THETA-K5 15,072 -4.0–+4.0 0.13 0.786
X5STHETK5 X5 SCIENCE THETA-K5 4,724 -4.0–+4.0 0.40 0.750
X6STHETK5 X6 SCIENCE THETA-K5 13,819 -4.0–+4.0 0.75 0.730
X7STHETK5 X7 SCIENCE THETA-K5 12,856 -4.0–+4.0 1.18 0.650
X8STHETK5 X8 SCIENCE THETA-K5 12,069 -4.0–+4.0 1.53 0.620
X9STHETK5 X9 SCIENCE THETA-K5 11,419 -4.0–+4.0 1.87 0.659
NOTE: Fall kindergarten estimates (X1) and spring kindergarten estimates (X2) estimates are weighted by W1C0. Fall first-grade estimates (X3)
are weighted by W3CF3P_30, and spring first-grade estimates (X4) are weighted by W4CS4P_20. Fall second-grade estimates (X5) are weighted
by W6CF6P_2A0, and spring second-grade estimates (X6) are weighted by W6CS6P_20. Spring third-grade estimates (X7) are weighted by
W7C7P_20. Spring fourth-grade estimates (X8) are weighted by W8C8P_20. Spring fifth-grade estimates (X9) are weighted by W9C9P_20. The
unweighted sample n indicates the number of cases with valid data regardless of the presence of a valid analytic weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
3-7Table 3-2. Direct cognitive assessment: Item Response Theory (IRT) standard errors of measurement
(SEM), fall and spring kindergarten, fall and spring first-grade, fall and spring second-grade,
spring third-grade, spring fourth-grade, and spring fifth-grade assessments: School years
2010–11, 2011–12, 2012–13; spring 2014; spring 2015; and spring 2016
Variable Description n
Range of
possible
values
Weighted
mean
Standard
deviation
X1RSETHK5 X1 READING STD ERR OF THETA-K5 15,669 0.0–3.0 0.30 0.083
X2RSETHK5 X2 READING STD ERR OF THETA-K5 17,186 0.0–3.0 0.20 0.080
X3RSETHK5 X3 READING STD ERR OF THETA-K5 5,194 0.0–3.0 0.16 0.061
X4RSETHK5 X4 READING STD ERR OF THETA-K5 15,115 0.0–3.0 0.12 0.040
X5RSETHK5 X5 READING STD ERR OF THETA-K5 4,725 0.0–3.0 0.11 0.023
X6RSETHK5 X6 READING STD ERR OF THETA-K5 13,837 0.0–3.0 0.11 0.016
X7RSETHK5 X7 READING STD ERR OF THETA-K5 12,866 0.0–3.0 0.11 0.010
X8RSETHK5 X8 READING STD ERR OF THETA-K5 12,074 0.0–3.0 0.11 0.016
X9RSETHK5 X9 READING STD ERR OF THETA-K5 11,427 0.0–3.0 0.13 0.030
X1MSETHK5 X1 MATH STD ERR OF THETA-K5 15,595 0.0–3.0 0.28 0.055
X2MSETHK5 X2 MATH STD ERR OF THETA-K5 17,143 0.0–3.0 0.24 0.036
X3MSETHK5 X3 MATH STD ERR OF THETA-K5 5,222 0.0–3.0 0.23 0.037
X4MSETHK5 X4 MATH STD ERR OF THETA-K5 15,103 0.0–3.0 0.21 0.030
X5MSETHK5 X5 MATH STD ERR OF THETA-K5 4,729 0.0–3.0 0.20 0.032
X6MSETHK5 X6 MATH STD ERR OF THETA-K5 13,830 0.0–3.0 0.19 0.023
X7MSETHK5 X7 MATH STD ERR OF THETA-K5 12,866 0.0–3.0 0.20 0.022
X8MSETHK5 X8 MATH STD ERR OF THETA-K5 12,080 0.0–3.0 0.21 0.034
X9MSETHK5 X9 MATH STD ERR OF THETA-K5 11,426 0.0–3.0 0.19 0.038
X2SSETHK5 X2 SCIENCE STD ERR OF THETA-K5 16,936 0.0–3.0 0.71 0.081
X3SSETHK5 X3 SCIENCE STD ERR OF THETA-K5 5,180 0.0–3.0 0.51 0.073
X4SSETHK5 X4 SCIENCE STD ERR OF THETA-K5 15,072 0.0–3.0 0.48 0.059
X5SSETHK5 X5 SCIENCE STD ERR OF THETA-K5 4,724 0.0–3.0 0.45 0.065
X6SSETHK5 X6 SCIENCE STD ERR OF THETA-K5 13,819 0.0–3.0 0.43 0.051
X7SSETHK5 X7 SCIENCE STD ERR OF THETA-K5 12,856 0.0–3.0 0.39 0.084
X8SSETHK5 X8 SCIENCE STD ERR OF THETA-K5 12,069 0.0–3.0 0.40 0.067
X9SSETHK5 X9 SCIENCE STD ERR OF THETA-K5 11,419 0.0–3.0 0.37 0.083
NOTE: Fall kindergarten estimates (X1) and spring kindergarten estimates (X2) are weighted by W1C0. Fall first-grade estimates (X3) are
weighted by W3CF3P_30, and spring first-grade estimates (X4) are weighted by W4CS4P_20. Fall second-grade estimates (X5) are weighted by
W6CF6P_2A0, and spring second-grade estimates (X6) are weighted by W6CS6P_20. Spring third-grade estimates (X7) are weighted by
W7C7P_20. Spring fourth-grade estimates (X8) are weighted by W8C8P_20. Spring fifth-grade estimates (X9) are weighted by W9C9P_20. The
unweighted sample n indicates the number of cases with valid data regardless of the presence of a valid analytic weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
3.1.1.2 Scale Scores
The IRT-based overall scale score for each content domain is an estimate of the number of
items a child would have answered correctly in each data collection round if he or she had been
administered all of the questions for that domain that were ever administered during the study (that is, all
of the 205 unique questions in the router and the three second-stage reading forms administered in
3-8kindergarten, first grade, second grade, third grade, fourth grade, and fifth grade; all of the 206 unique
questions in the router and the three second-stage mathematics forms administered in kindergarten, first
grade, second grade, third grade, fourth grade, and fifth grade; and all of the 130 unique items
administered in the single-stage kindergarten science form and the router and three second-stage science
forms in first grade, second grade, third grade, fourth grade, fifth grade).
To calculate the IRT-based overall scale score for each domain, a child’s theta is used to
predict a probability for each assessment item that the child would have gotten that item correct. Then, the
probabilities for all the items fielded as part of the domain in every round are summed to create the
overall scale score. Because the computed scale scores are sums of probabilities, the scores are not
integers.
Gain scores in each domain may be obtained by subtracting the IRT scale scores at an earlier
round from the IRT scale scores at a later round. For example, subtracting the fall kindergarten
mathematics score from the spring kindergarten mathematics score would result in a score indicating gain
across the kindergarten year. Similarly, a gain score from kindergarten entry to the end of fifth grade
would be obtained by subtracting the fall kindergarten mathematics score from the spring fifth-grade
mathematics score. Users should note that the scale scores are only comparable across rounds within a
single data file. In other words, the scale scores for a given domain in the K–5 data file are all comparable
to one other, but they are not comparable to the scale scores for that domain reported in the previously
released files. The scale scores are recomputed for each file because the scale scores represent the
estimated number correct for all items across all assessments administered; the total number of items in
the pool expands each year as more difficult items are added to the assessments.
Scores for different subject areas are not comparable to each other because they are based on
different numbers of questions and content that is not necessarily equivalent in difficulty. For example, if
a child’s IRT scale score in reading is higher than in mathematics, it would not be appropriate to interpret
that to mean the child performs better in reading than in mathematics.
Table 3-3 provides the names of the variables pertaining to the IRT scale scores available in
the data file, along with the variable descriptions, value ranges, weighted means, and standard deviations.
3-9Table 3-3. Direct cognitive assessment: Item Response Theory (IRT) scale scores, fall and spring
kindergarten, fall and spring first-grade, fall and spring second-grade, spring third-grade,
spring fourth-grade, and spring fifth-grade assessments: School years 2010–11, 2011–12,
2012–13; spring 2014; spring 2015; and spring 2016
Variable Description n
Range of
possible
values
Weighted
mean
Standard
deviation
X1RSCALK5 X1 READING IRT SCALE SCORE-K5 15,669 0.0–205.0 53.85 11.224
X2RSCALK5 X2 READING IRT SCALE SCORE-K5 17,186 0.0–205.0 68.57 14.315
X3RSCALK5 X3 READING IRT SCALE SCORE-K5 5,194 0.0–205.0 77.03 16.715
X4RSCALK5 X4 READING IRT SCALE SCORE-K5 15,115 0.0–205.0 94.47 17.812
X5RSCALK5 X5 READING IRT SCALE SCORE-K5 4,725 0.0–205.0 101.22 17.413
X6RSCALK5 X6 READING IRT SCALE SCORE-K5 13,837 0.0–205.0 111.93 16.922
X7RSCALK5 X7 READING IRT SCALE SCORE-K5 12,866 0.0–205.0 120.66 15.331
X8RSCALK5 X8 READING IRT SCALE SCORE-K5 12,074 0.0–205.0 129.31 14.513
X9RSCALK5 X9 READING IRT SCALE SCORE-K5 11,427 0.0–205.0 136.26 15.337
X1MSCALK5 X1 MATH IRT SCALE SCORE-K5 15,595 0.0–206.0 35.21 11.479
X2MSCALK5 X2 MATH IRT SCALE SCORE-K5 17,143 0.0–206.0 49.42 13.342
X3MSCALK5 X3 MATH IRT SCALE SCORE-K5 5,222 0.0–206.0 58.01 14.110
X4MSCALK5 X4 MATH IRT SCALE SCORE-K5 15,103 0.0–206.0 72.25 15.500
X5MSCALK5 X5 MATH IRT SCALE SCORE-K5 4,729 0.0–206.0 77.41 15.950
X6MSCALK5 X6 MATH IRT SCALE SCORE-K5 13,830 0.0–206.0 89.71 17.920
X7MSCALK5 X7 MATH IRT SCALE SCORE-K5 12,866 0.0–206.0 103.70 17.802
X8MSCALK5 X8 MATH IRT SCALE SCORE-K5 12,080 0.0–206.0 112.30 17.631
X9MSCALK5 X9 MATH IRT SCALE SCORE-K5 11,426 0.0–206.0 119.45 17.339
X2SSCALK5 X2 SCIENCE IRT SCALE SCORE-K5 16,936 0.0–130.0 33.57 7.353
X3SSCALK5 X3 SCIENCE IRT SCALE SCORE-K5 5,180 0.0–130.0 36.95 9.044
X4SSCALK5 X4 SCIENCE IRT SCALE SCORE-K5 15,072 0.0–130.0 42.71 10.213
X5SSCALK5 X5 SCIENCE IRT SCALE SCORE-K5 4,724 0.0–130.0 46.63 10.722
X6SSCALK5 X6 SCIENCE IRT SCALE SCORE-K5 13,819 0.0–130.0 52.25 11.606
X7SSCALK5 X7 SCIENCE IRT SCALE SCORE-K5 12,856 0.0–130.0 59.83 11.914
X8SSCALK5 X8 SCIENCE IRT SCALE SCORE-K5 12,069 0.0–130.0 66.73 11.902
X9SSCALK5 X9 SCIENCE IRT SCALE SCORE-K5 11,419 0.0–130.0 73.38 12.743
NOTE: Fall kindergarten estimates (X1) and spring kindergarten estimates (X2) are weighted by W1C0. Fall first-grade estimates (X3) are
weighted by W3CF3P_30, and spring first-grade estimates (X4) are weighted by W4CS4P_20. Fall second-grade estimates (X5) are weighted by
W6CF6P_2A0, and spring second-grade estimates (X6) are weighted by W6CS6P_20. Spring third-grade estimates (X7) are weighted by
W7C7P_20. Spring fourth-grade estimates (X8) are weighted by W8C8P_20. Spring fifth-grade estimates (X9) are weighted by W9C9P_20. The
unweighted sample n indicates the number of cases with valid data regardless of the presence of a valid analytic weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, spring 2015, spring 2016.
3-103.1.2 Variables Indicating Exclusion from the Direct Assessment Due to Disability
The variables X1EXDIS, X2EXDIS, X3EXDIS, X4EXDIS, X5EXDIS, X6EXDIS,
X7EXDIS, X8EXDIS, and X9EXDIS can be used to identify children who were excluded from the
assessment because they needed an accommodation the study did not provide or because they had an
Individualized Education Program (IEP) that indicated they could not take part in standardized
assessments. These variables are coded 1, Excluded from assessment due to disability, for children who
were excluded from the assessment for these reasons. All other children are coded 0 for variables
X1EXDIS, X2EXDIS, X4EXDIS, X6EXDIS, X7EXDIS, X8EXDIS, and X9EXDIS. For the variables
pertaining to the fall first-grade and fall second-grade data collections (X3EXDIS and X5EXDIS),
children who were part of the subsample in those rounds and not excluded from the assessments are
coded 0 and children who were not part of the subsample (and, therefore, not eligible for the assessments
in these rounds) are coded as system missing.7
3.1.3 Choosing the Appropriate Score for Analysis
When choosing scores to use in analysis, researchers should consider the nature of their
research questions, the type of statistical analysis to be conducted, the population of interest, and the
audience. The sections below discuss the general suitability of the different types of scores for different
analyses.
 The IRT-based theta scores are overall measures of ability. They are appropriate for
both cross-sectional and longitudinal analyses. They are useful in examining
differences in overall achievement among subgroups of children in a given data
collection round or across rounds, as well as in analysis of correlations between
achievement and child, family, and school characteristics. The fall kindergarten,
spring kindergarten, fall first-grade, spring first-grade, fall second-grade, spring
second-grade, spring third-grade, spring fourth-grade, and spring fifth-grade theta
scores included in the K-5 data file are on the same metric. Therefore, an analyst
looking at growth across the kindergarten year could, for example, subtract the fall
kindergarten score from the spring kindergarten score to compute a gain score. When
looking at growth from kindergarten entry to the end of fifth grade, an analyst could
subtract the fall kindergarten score from the spring fifth-grade score to compute a gain
score.
The theta scores may be more desirable than the scale scores for use in a multivariate
analysis because their distribution generally tends to be more normal than the
distribution of the scale scores. It is recommended that analysts review the
7 The “system missing” code appears as a blank when viewing codebook frequencies and in the ASCII data file. System missing codes
(blanks) indicate that data for an entire instrument or assessment are missing due to unit nonresponse.
3-11distributions for normality. In assessments where the number of items or number of
observations is low, the normality of the distribution may be affected. In the ECLS-
K:2011, the kindergarten science and kindergarten and first-grade SERS distributions
deviated from normal, due to the limited number of items and observations,
respectively. Additionally, in the extreme tails of the theta distributions in each
domain, a combination of some extremely low-performing and some extremely high-
performing children who took the assessment and the instrument itself may result in
clustered estimates. By design, in order to limit the length of the assessment and the
number of too easy or too difficult items any one child would be administered, the
assessment does not have many items administered at the difficulty ranges in the tails.
Including more items appropriate for children at the ability extremes would have
required a reduction in the number of items at the range of ability of nearly all the
sampled children (> 99 percent). Thus, some clustering of thetas may be observed in
the extreme tails of the theta distributions.
For a broader audience of readers unfamiliar with IRT modeling techniques, the
metric of the theta scores (from -4 to 4) may be less readily interpretable than the
metric of the scale scores. Researchers should consider their analysis and the audience
for their research when selecting between the theta and the scale score.
 The IRT-based scale scores also are overall measures of achievement. They are
appropriate for both cross-sectional and longitudinal analyses. They are useful in
examining differences in overall achievement among subgroups of children in a given
data collection round or in different rounds, as well as in analysis looking at
correlations between achievement and child, family, and school characteristics. The
fall kindergarten, spring kindergarten, fall first-grade, spring first-grade, fall second-
grade, spring second-grade, spring third-grade, spring fourth-grade, and spring fifth-
grade scale scores included in the K-5 data file are on the same metric. Therefore, an
analyst looking at growth across the kindergarten year could subtract the fall
kindergarten score from the spring kindergarten score to compute a gain score. Or
when looking at growth from kindergarten entry to the end of fifth grade, an analyst
could subtract the fall kindergarten score from the spring fifth-grade score to compute
a gain score. Results expressed in terms of scale score points, scale score gains, or an
average scale score may be more easily interpretable by a wider audience than results
based on the theta scores.
3.1.4 Analytic Considerations for Measuring Gains in the ECLS-K:2011
An important issue to be considered when analyzing achievement scores and gains is
assessment timing: children’s age at assessment, the date of assessment, and the time interval between
assessments. Most sampled children were born throughout the second half of 2004 and first half of 2005,
but their birth dates were not related to testing dates. As a result, children were tested at different
developmental and chronological ages. Assessment dates ranged from August to December for the fall
data collections, and from March to June for the spring data collections. Children assessed later in a data
collection period in a particular grade level, for example in December during a fall collection, may be
3-12expected to have an advantage over children assessed earlier in the data collection period, for example in
the first days or weeks of school, because they had more exposure to educational content before being
assessed. Substantial differences in the intervals between assessments may also affect analysis of gain
scores. Children assessed in September for the fall data collection and June for the spring data collection
have more time to learn knowledge skills than do children assessed first in November and then again in
March. These differences in interval may or may not have a significant impact on analysis results. In
designing an analysis plan, it is important to consider whether and how differences in age, assessment
date, and interval may affect the results; to look at relationships between these factors and other variables
of interest; and to adjust for differences, if necessary.
When using the IRT scale scores as longitudinal measures of overall growth, analysts should
keep in mind that gains made at different points on the scale have qualitatively different interpretations.
Children who made gains toward the lower end of the scale, for example, in skills such as identifying
letters and associating letters with sounds, are learning different skills than children who made gains at
the higher end of the scale, for example, those who have gone from reading sentences to reading passages,
although their gains in number of scale score points may be the same. Comparison of gains in scale score
points is most meaningful for groups that started with similar initial status. One way to account for
children’s initial status is to include a prior round assessment score as a control variable in an analytic
model. For example, the fall kindergarten scale score could be included in a model using the spring
kindergarten scale score as the outcome.
3.1.5 Reliability of the ECLS-K:2011 Scores
Reliability statistics assess consistency of measurement, or the extent to which test items in a
set are related to each other and to the score scale as a whole. For tests of equal length, reliability estimates
can be expected to be higher for sets of items that are closely related to the underlying construct than for
tests with more diversity of content. Conversely, for tests with similar levels of diversity in content,
reliabilities tend to be higher for longer tests compared to shorter tests. Reliabilities range from 0 to 1.
Table 3-4 presents the reliability statistics computed for the IRT-based scores for each
subject area for the fall and spring of kindergarten, the fall and spring of first grade, the fall and spring of
second grade, the spring of third grade, the spring of fourth grade, and the spring of fifth grade. The
reliability of the overall ability estimate, theta, is based on the variance of repeated estimates of theta for
each individual child compared with total sample variance. The reliabilities calculated for theta also apply
to the scores derived from the theta estimate, namely, the IRT scale scores.
3-13Table 3-4. Reliability of Item Response Theory (IRT)-based scores (theta and scale scores), by round of
data collection and domain, for fall and spring kindergarten, fall and spring first grade, fall
and spring second grade, spring third grade, spring fourth grade, and spring fifth grade:
School years 2010–11, 2011–12, 2012–13; spring 2014; spring 2015; and spring 2016
Domain
Number
of
items
Fall
kinder-
garten
Spring
kinder-
garten
Fall
first
grade
Spring
first
grade
Fall
second
grade
Spring
second
grade
Spring
third
grade
Spring
fourth
grade
Spring
fifth
grade
Reading 205 0.92 0.94 0.95 0.95 0.91 0.90 0.86 0.87 0.86
Mathematics 206 0.92 0.93 0.93 0.93 0.93 0.94 0.92 0.91 0.92
Science 130 † 0.73 0.83 0.84 0.86 0.85 0.83 0.82 0.86
† Not applicable: field test findings indicated that science knowledge and skills could not be validly and reliably assessed in the fall of
kindergarten and thus were assessed beginning in spring kindergarten.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
3.1.6 Validity of the ECLS-K:2011 Scores
Evidence for the validity of the direct cognitive assessments was derived from several
sources. A review of national and state performance standards, comparison with state and commercial
assessments, and the judgments of curriculum experts all informed the development of the test
specifications.
The content category specifications for the ECLS-K:2011 reading assessments in
kindergarten through second grade are based on the 2009 Reading Frameworks for the National
Assessment of Educational Progress (NAEP) (National Assessment Governing Board 2008), with the
addition of basic reading skills and vocabulary categories suited for the earlier grades. Although the
NAEP framework was selected for its rigorous design and its use in many years of national
administrations by National Center for Education Statistics (NCES), because the NAEP assessments are
administered starting in fourth grade, it was necessary to consult other sources to extend the NAEP
content percentage specifications down to earlier grades. Experts in reading assessment development
consulted the ECLS-K kindergarten, first-grade, third-grade, and fifth-grade reading assessment
frameworks; current curriculum standards from Texas, California, New Jersey, Florida, and Virginia; and
the Common Core State Standards.8 The ECLS-K:2011 reading specifications for third grade, fourth
grade, and fifth grade are built upon those developed for the earlier grades and supplemented by the
fourth- and eighth-grade NAEP Reading Frameworks for 2011 (National Assessment Governing Board
2010a), as well as the third-, fourth-, and fifth-grade standards from the same five states noted.
8 See http://www.corestandards.org for further information. An effort led by state governors and state commissioners of education to develop the
Common Core State Standards for kindergarten through grade 12 was begun in 2009, through the National Governors Association Center for
Best Practices and the Council of Chief State School Officers.
3-14The ECLS-K:2011 mathematics test specifications for kindergarten through second grade
are based on the frameworks developed for the ECLS-K assessments, which were based on the NAEP
mathematics frameworks and extended down to earlier grades. The content of the mathematics framework
is consistent with recommendations presented in the Mathematics Framework for the 2005 NAEP
(National Assessment Governing Board 2004a), the National Council of Teachers of Mathematics
Principles and Standards for School Mathematics (2000), and with state standards of California, New
Jersey, Tennessee, Texas, and Virginia. These are also consistent with general findings from the National
Mathematics Advisory Panel (2008). For third grade, fourth grade, and fifth grade, the content covered in
the ECLS-K:2011 mathematics assessment was determined by comparing the state or national standards
from Texas, Virginia, NAEP, and the National Council of Teachers of Mathematics (NCTM). Common
Core State Standards were not used in the comparison since these standards are similar to the national
standards set by NCTM and NAEP. As in reading, the framework in the later grades builds on the
framework developed for the earlier grades, using the same sources.
The science knowledge and skills assessed in the ECLS-K:2011 were chosen based on the
areas identified as being important to assess in the 1996–2005 and 2011 NAEP science frameworks
(National Assessment Governing Board 2004b, 2010b). However, because the NAEP science frameworks
begin in fourth grade, the science standards of six states (Arizona, California, Florida, New Mexico,
Texas, and Virginia) were analyzed to find common topics that are taught at the lower grade levels. In
these states and for each grade level, three or four standards were drawn from each of four common
content categories (scientific inquiry, life science, physical science, and Earth and space science) and
these four areas were selected as the content categories for the ECLS-K:2011 science assessment
framework.
Pools of potential assessment items were developed for each content domain based on the
framework or standards pertinent to the domain. An expert panel of school educators, including
curriculum specialists in the subject areas, then examined the pool of items for content and framework
strand design, accuracy, nonambiguity of response options, and appropriate formatting. The items were
included in a field test and better performing items were selected for the final assessment battery.
3.2 Direct Cognitive Assessment: Executive Function
Executive functions are interdependent processes that work together to regulate and
orchestrate cognition, emotion, and behavior and that help a child to learn in the classroom. Three
3-15measures of executive function were administered in the fifth-grade direct child assessment battery, to
assess cognitive flexibility, working memory, and inhibitory control. The NIH Toolbox Flanker Inhibitory
Control and Attention Task (Flanker) (Zelazo et al 2013), which measures inhibitory control in the
context of selective visual attention, was administered for the first time in fourth grade, and then the same
version of the task was administered again in the fifth grade. The Flanker complemented the two
additional measures of executive function included in fifth grade, which were also included in the
kindergarten, first-grade, second-grade, third-grade and fourth-grade assessments: the Dimensional
Change Card Sort (DCCS) (Zelazo 2006; Zelazo et al. 2013), assessing children’s cognitive flexibility,
and the Numbers Reversed subtest of the Woodcock-Johnson III (WJ III) Tests of Cognitive Abilities
(Woodcock, McGrew, and Mather 2001), assessing working memory. The same versions of the DCCS
and the Numbers Reversed tasks were administered in fall and spring of the kindergarten year and fall and
spring of first grade. In second grade, the DCCS was changed to computerized administration to remain
age-appropriate through fifth grade. The same computerized version was used again in third grade, fourth
grade, and fifth grade. The Numbers Reversed task remained the same across all rounds of collection,
kindergarten through fifth grade.
3.2.1 Dimensional Change Card Sort
The Dimensional Change Card Sort (DCCS) (Zelazo 2006; Zelazo et al. 2013) is used to
collect information on children’s cognitive flexibility.
In the kindergarten and first-grade data collections, the DCCS was administered as a
physical, table-top card sort with the items administered by a trained assessor. Beginning with the second-
grade data collections, a computerized version of the DCCS developed for the National Institutes of
Health Toolbox for the Assessment of Neurological and Behavioral Function (NIH Toolbox) was
administered. The shift to a computerized version of the task was made so that the DCCS would remain
age-appropriate through the end of data collection for ECLS-K:2011. For more information on the
physical, table-top card sort task administered in kindergarten and first grade and differences between the
physical version and computerized version, see chapter 3 of the User’s Manual for the Kindergarten–
Second Grade Data File and Electronic Codebook, Public Version (NCES 2017-285) (Tourangeau et al.
2017). This section describes the computerized version of the DCCS that was administered in the spring
of fifth grade, which is the same version administered in the second-grade, third-grade, and fourth-grade
rounds.
3-16The computerized task was developed as part of the National Institutes of Health Toolbox
for the Assessment of Neurological and Behavioral Function (see www.nihtoolbox.org) and is appropriate
for ages 3–85 (Zelazo et al. 2013). The task had been under development during the planning phases for
the earliest rounds of the ECLS-K:2011 and became available in time to be incorporated into the second-
grade data collections. The NIH Toolbox Dimensional Change Card Sort Test (NIH Toolbox DCCS) is a
task that is used across the 3 through 85 age range, but it has two different start points based on the age of
the child in order to limit administration time. The NIH Toolbox DCCS consists of 40 trials, including 5
pre-switch trials (where children are asked to sort by one dimension, e.g., color), 5 post-switch trials
(where children are asked to sort by a different dimension, e.g., shape), and 30 mixed-block trials (in
which the sorting dimension, either color or shape, varies by trial). Testing conducted in the development
of the NIH Toolbox DCCS indicated that 8-year-olds typically scored at ceiling on the pre-switch and
post-switch trials. Consequently, children under age 8 begin with the pre-switch trials, and children age 8
and above begin with the mixed-block trials and are given credit in the scoring for completing the pre-
switch and post-switch trials accurately.
For the ECLS-K:2011 administrations of the computerized DCCS, all ECLS-K:2011
children were administered the version of the NIH Toolbox DCCS for ages 8 years and older, regardless
of their age at the time of assessment. In second grade, approximately 90 percent of the ECLS-K:2011
children in the fall subsample for second grade and approximately 40 percent of children in the spring of
second grade who had a score on the DCCS were not yet 8 years old when the DCCS was administered.
In third grade, nearly all children who participated in the DCCS (99.95 percent) were at least 8 years old
when the DCCS was administered. In fourth and fifth grades, all children who participated in the DCCS
were at least 8 years old when the DCCS was administered. The decision to administer the same version
of the DCCS from second grade forward, regardless of whether the child was age 8, was made so that all
study children would receive the same version of the DCCS task in second grade and in later rounds of
data collection. Use of the same measure allows for a longitudinal analysis of performance on the DCCS
from second grade into later rounds of data collection.
As noted earlier, the construct assessed in the physical version of the DCCS that was
administered in kindergarten and first grades and the computerized version of the DCCS is the same—
cognitive flexibility. However, the way the construct is assessed and the scoring differ across the versions.
One key difference between the two versions is that the computerized version captures data on the amount
of time in milliseconds that it takes the child to complete any given item; it is not possible to accurately
measure reaction time at the necessary level of precision in the physical version. Therefore, the
computerized version supports the use of both accuracy of sorting and reaction time to assess overall
performance while the physical card sort assesses performance by accuracy alone.
3-17In each of the 30 mixed-block trials administered via computer to children in the ECLS-
K:2011 beginning in the second-grade rounds, the children were presented with a stimulus picture of a
ball or truck that was either yellow or blue. A prerecorded female voice announced the sorting rule to be
used for that trial (“color” or “shape”) as the appropriate word “color” or “shape” was briefly displayed in
the center of screen. Next, the stimulus picture was displayed in the center of screen, where the word had
just appeared. Children then selected one of two pictures at the bottom of the screen (a blue ball on the
left or a yellow truck on the right) that was either the same shape or the same color as the stimulus
picture, depending on whether the shape or color sorting rule was in effect for the trial. Children indicated
their choice of picture by pressing the arrow key on the laptop keyboard that was associated with the
picture; the left arrow key was used to select the picture on the left side of the screen and the right arrow
key was used to select the picture on the right side of the screen. Children were instructed to use just one
pointer finger to press the arrow keys. They were asked to return their pointer finger to the button in
between the left and right arrow keys (marked with a fuzzy sticker, and so identified as the “fuzzy
button”) in between trials to standardize the start location for every child’s finger, with the goal of
maximizing accuracy in the measurement of response time. Both reaction time to sort the card and
accuracy of its placement according to the sorting rule in effect for the trial were recorded by the
computer program.
The sorting rules (i.e., to either sort by shape or color) were intermixed across the trials, and
one rule was more common than the other. The shape rule was used for 23 trials while the color rule was
used in 7 trials. For example, the child may be asked to sort by shape for 4 trials in a row, then to sort by
color on trial 5, and then to sort by shape on trials 6 and 7. One sorting rule was presented more
frequently in order to build a response tendency (i.e., a response that is “preferred” because it happens
more frequently, resulting in a predisposition to respond in that manner). A predisposition to sort by the
dominant rule (i.e., shape) can result in either more errors or a slower reaction or response time on
nondominant trials because it is necessary to inhibit the dominant response (i.e., sorting by shape) in order
to shift to the less frequent sorting rule (i.e., color). The “cost” associated with the shift from a more
frequent rule (the “dominant” rule) to a less frequent rule (the “nondominant” rule) tends to differ by the
age of the participant (Davidson et al. 2006). The “cost” to younger children is that they tend to make
more errors on the nondominant rule trials; that is, they do not demonstrate the cognitive flexibility to
make the switch between rules even when prompted. Younger children do not tend to slow themselves
down in favor of higher accuracy and, therefore, accuracy is a better metric of performance for young
children (Zelazo et al. 2013). In contrast, older children and adults tend to demonstrate a speed/accuracy
tradeoff; they slow down the pace at which they respond in order to maintain accuracy. Thus, the “cost”
to older children and adults is seen in reaction time on the nondominant rule trials. The formula used to
3-18produce scores from the data collected by the computerized DCCS factors in reaction time on the
infrequent or nondominant trials when a child demonstrates sufficiently accurate performance across all
the test trials, defined as being accurate on more than 80 percent of the trials (Zelazo et al. 2013). Thus,
the computerized DCCS provides a measure of performance through this developmental shift to learning
to trade speed for accuracy. More information on scoring is provided below.
The 30 test trials were administered only to children who successfully completed the practice
portion of the DCCS. The practice consisted of a minimum of 8 trials and a maximum of 24 trials,
depending upon how quickly the child demonstrated that he or she understood the task. For the first set of
practice trials, the assessor instructed the child how to sort by shape using text automatically presented on
the DCCS screen that was read by the assessor along with additional standardized instructions presented
by the assessor. Following the instructions, the computer administered four practice trials asking the child
to sort by shape. If the child sorted at least three of the four items correctly by shape, he or she progressed
to the color practice. If the child sorted more than one item in the set of four incorrectly, he or she was
presented with a second set of four practice items. If the child failed to sort three of four items correctly
by shape in the second set of practice items, he or she was presented a third set; failure of this third set
ended the DCCS program before any actual scored trials were presented.
Once a child passed the shape practice trials, the assessor instructed on how to sort by color,
and the computer presented 4 to 12 practice trials asking to sort by color. Like the shape practice trials, up
to three sets of four items could be presented before the DCCS advanced to the scored trials. If the child
was not able to pass the color practice, the DCCS program ended after the third set of color practice items,
again before any actual scored trials were presented.
In contrast with the scored trials, the practice trials maintained one sorting rule for all items
presented in succession until practice for the rule was complete. An additional difference between the
practice and scored trials was that the stimulus pictures in the practice trials were white or brown rabbits
and boats.
Item-level data for the 30 test trials are included in the data file. They are provided in three
blocks of 30 items for each participant that indicate: (1) correct versus incorrect responses (C*DCCS1-
C*DCCS30); (2) the type of trial, reported as dominant (most frequently presented but not included in
reaction time scores; shape is the dominant sorting rule) or nondominant (less frequently presented and
used to calculate reaction time scores; color is the nondominant sorting rule) (C*GAME1-C*GAME30);
and (3) reaction times reported in milliseconds (C*TARGRT1-C*TARGRT30). Variable names for the
item-level data begin with “C9” for spring fifth grade.
3-19As in second, third, and fourth grades, the overall computed score reported for the fifth-
grade DCCS is derived using a formula provided by the task developer and follows the scoring algorithm
used for this task in the NIH Toolbox (see the NIH Toolbox Scoring and Interpretation Guide, [Slotkin,
Nowinski et al. 2012], for additional information on scoring). Scores range from 0 to 10, with weight
given to accuracy (0 to 5 units) and reaction time (0 to 5 units) in the computation of the scores. Accuracy
is considered first. If the child’s accuracy rate is less than or equal to 80 percent, the child’s overall
computed score is based entirely on accuracy. If the child’s accuracy rate is more than 80 percent, the
child’s overall computed score is based on a combination of accuracy and reaction time.
The accuracy score factored into the computation of the overall score can range from 0 to 5.
There are a total of 40 accuracy points that are scaled down to a maximum score of 5: for each correct
response, the child earns a score of .125 (5 points divided by 40 trials). Because all children used the start
point of the DCCS for children 8 years and older, each child was administered the 30 mixed-block trials,
and each child who successfully passed the practice items was automatically given 10 accuracy points for
the 5 pre-switch and the 5 post-switch trials of the DCCS that were not administered. Therefore, the
accuracy component of the overall computed DCCS score is calculated as follows:
DCCS accuracy score = 0.125 * number of correct responses9
If the child’s accuracy rate is higher than 80 percent, a reaction time score is added to the child’s accuracy
score.10 Like the accuracy score, the reaction time score ranges from 0 to 5 points.
The reaction time component of the overall computed score for the computerized DCCS is
computed using the child’s median reaction time to correct nondominant trials (i.e., the trials with the less
frequently used sorting rule, color), following the same scoring algorithm outlined in the scoring manual
for the NIH Toolbox (Slotkin, Nowinski et al. 2012). First, for those children with greater than 80 percent
accuracy on the 40 trials, the median reaction time is calculated based on reaction times for correct
9 The number of correct responses = 10 + the number of correct trials out of the 30 mixed block trials. Once the child has passed the practice
trials and advanced into the scored portion of the assessment, 10 accuracy points are automatically awarded due to the chosen start point for the
task. For this reason, it is not possible for ECLS-K:2011 children to get an accuracy score of 0. Therefore, the minimum possible value for the
DCCS accuracy score is 1.25 and the maximum possible DCCS accuracy score is 5.
10 The criterion of greater than 80 percent accuracy is calculated based on all 40 trials (30 administered trials plus the 10 trials not administered).
That is, 80 percent of 40 trials is 32 items. However, this can also be thought of in terms of how many items out of the 30 administered trials are
required. If the criterion is 80 percent of the 40 trials, this translates to 23 of the 30 administered trials. For example, if a child responds accurately
on 23 of the 30 mixed block trials, the child’s accuracy rate equals 82.5 percent (10 points automatically awarded for the pre-switch and post-
switch trials plus the 23 correct mixed block trials divided by 40; 33/40 = .825). In this example, the child’s accuracy score would be [(10 + 23) *
.125] = 4.125. Because the accuracy rate is greater than 80 percent, the child’s reaction time score would be added to this accuracy score to
obtain the overall computed score for the DCCS. Alternatively, if the child responded accurately on 22 of the 30 mixed-block trials, the child’s
accuracy rate would equal 80 percent and, therefore, the child’s accuracy is not greater than 80 percent and the child’s overall score would be
based solely on accuracy (overall computed score = [(10 + 22) * .125] = 4).
3-20nondominant trials with reaction times greater than or equal to 100 milliseconds (msec) and within plus or
minus three standard deviations from the child’s mean reaction time on the correct nondominant trials.
The minimum median reaction time allowed is 500 msec; the maximum median reaction time is 3,000
msec. If the child’s median reaction time falls outside this range, the child’s median reaction is set to the
minimum or maximum allowable range: reaction times between 100 msec and 500 msec were set to 500
msec and reaction times between 3,000 msec and 10,000 msec (the maximum trial duration) are set to
3,000 msec. A log (base 10) transformation is applied to the median reaction times to create a more
normal distribution. The log values are then algebraically rescaled to a 0 to 5 range and then reversed
such that faster (better) reaction times have higher values and slower reaction times have lower values.
The formula for rescaling the median reaction times is the following:
where RT is the median reaction time on nondominant trials within set outer limits.11
To summarize, the overall computed score on the computerized DCCS is equal to the child’s
accuracy score if the child’s accuracy rate is less than or equal to 80 percent. If the child’s accuracy rate is
greater than 80 percent, the child’s overall computed score is equal to the child’s accuracy score plus the
child’s reaction time score, which is derived from the child’s reaction time on correct nondominant trials
as described above. Additional details on the calculation of the computed score are available in the NIH
Toolbox Scoring and Interpretation Guide (Slotkin, Nowinski, et al. 2012) and the NIH Toolbox
Technical Manual (Slotkin, Kallen, et al. 2012).
The fall and spring second-grade, spring third-grade, spring fourth-grade, and spring fifth-
grade computed scores (X5DCCSSCR, X6DCCSSCR, X7DCCSSCR, X8DCCSSCR, and X9DCCSSCR)
range from 0 to 10, with weight given to accuracy (0 to 5 units) and reaction time (0 to 5 units) in the
computation of the score. The overall computed score for the computerized DCCS can be used to examine
change across rounds that use the computerized DCCS (i.e., performance in the fall of second grade can
be directly compared to performance in the spring of second grade, the spring of third grade, the spring of
fourth grade, and the spring of fifth grade).
11 The median reaction time (RT) used to calculate the reaction time score falls within the range of 500 msec through 3,000 msec. Calculation of
the median score requires a minimum of at least one correct nondominant trial reaction time that is greater than 100 msec. When the child
reached the accuracy threshold for including the reaction time component in the scoring but did not have any within-range reaction times on
correct nondominant trials, the child’s overall computed score on the DCCS was set equal to the child’s accuracy score, and reaction time was not
factored into the child’s score.
3-21It is important for researchers using the DCCS data to be aware of the characteristics of the
overall DCCS scores and determine how best to use these scores in their analyses. As noted above, the
NIH-developed scoring model computes scores differently depending on sorting accuracy. The use of this
scoring model with the data collected from children in the ECLS-K:2011 resulted in a non-normal
distribution. For example, approximately 4 percent of children in the third-grade data collection who have
a computed overall score failed to achieve greater than 80 percent accuracy. In fourth grade, this
percentage was 2 percent. In fifth grade, 1 percent of children who have a computed overall score did not
achieve greater than 80 percent accuracy. The score for these children is calculated based solely on
accuracy. The remaining children (96 percent in third grade, 98 percent in fourth grade, and 99 percent in
fifth grade) who have a computed overall score have scores calculated based on both accuracy and
reaction time.
The non-normal distribution may be problematic for statistical analyses. For this reason,
users may want to run analyses that do not use the overall score as is with the full sample. For example,
users could conduct their analyses separately for the two groups of children so that each analysis only
includes children with scores calculated in the same way, or they may decide to limit their analyses to
only one group. Another option is for users to analyze all children using the score indicating accuracy
alone, recognizing that this score is highly skewed, as most children were able to sort the cards with at
least 80 percent accuracy. Users may also want to consider investigating alternative scoring models using
the item-level accuracy and reaction time data available on the data file. The decision about how best to
use the DCCS overall score in analysis is left to the user, given the research questions being addressed.
Analysts may choose to examine other ways researchers have analyzed data with similar distributions, or
other executive function or card sort data, in deciding how best to utilize the ECLS-K:2011 DCCS data.
The variable names, descriptions, value ranges, weighted means, and standard deviations for
the second-, third-, fourth-, and fifth-grade DCCS scores are provided in table 3-5. For information on the
kindergarten and first-grade scores, see the User’s Manual for the ECLS-K:2011 Kindergarten–Second
Grade Data File and Electronic Codebook, Public Version (NCES 2017-285) (Tourangeau et al. 2017) .
The following scores based on the fifth-grade computerized administration are presented on the data file:
overall score for spring fifth grade (X9DCCSSCR; range: 0-10); accuracy score for spring fifth grade
(X9CSACC; range: 0-5) that is scaled as described above to compute the overall DCCS score; reaction time
score for spring fifth grade (X9CSNDRT; range: 0-5) that is scaled to compute the overall DCCS score;
count of correct, dominant trials (X9CSDAC; range: 0-23); and count of correct nondominant trials
(X9CSNDAC; range: 0-7). Researchers should note that the count of correct dominant trials and the count
of correct nondominant trials represent accuracy by trial type for the 30 administered trials and are different
from the total accuracy score (X9CSACC, DCCS Accuracy Component [0-5] Score) that is derived to
3-22compute the overall DCCS computed score. Researchers should also note that the reaction time score was
only computed for cases for which the accuracy score was greater than 80 percent. If the accuracy score was
not greater than 80 percent, then the reaction time score was set to -9 (not ascertained).
Table 3-5. Dimensional Change Card Sort variable names, descriptions, value ranges, weighted means,
and standard deviations for fall and spring second grade, spring third grade, spring fourth
grade, and spring fifth grade: School year 2012–13, spring 2014, spring 2015, and spring
2016
Variable name Description n
Value
ranges1
Weighted
mean
Standard
deviation
X5DCCSSCR X5 Computed (Overall) Score 4,708 0–10 6.37 1.402
X6DCCSSCR X6 Computed (Overall) Score 13,774 0–10 6.69 1.345
X7DCCSSCR X7 Computed (Overall) Score 12,744 0–10 7.19 1.098
X8DCCSSCR X8 Computed (Overall) Score 12,021 0–10 7.63 0.965
X9DCCSSCR X9 Computed (Overall) Score 11,386 0–10 7.97 0.943
X5CSACC X5 DCCS Accuracy Component (0-5) Score 4,708 0–5 4.53 0.589
X6CSACC X6 DCCS Accuracy Component (0-5) Score 13,774 0–5 4.59 0.504
X7CSACC X7 DCCS Accuracy Component (0-5) Score 12,744 0–5 4.72 0.356
X8CSACC X8 DCCS Accuracy Component (0-5) Score 12,021 0–5 4.80 0.274
X9CSACC X9 DCCS Accuracy Component (0-5) Score 11,386 0–5 4.82 0.246
X5CSNDRT X5 DCCS Nondom RT Component (0-5) Score 4,067 0–5 2.09 0.758
X6CSNDRT X6 DCCS Nondom RT Component (0-5) Score 12,405 0–5 2.33 0.765
X7CSNDRT X7 DCCS Nondom RT Component (0-5) Score 12,222 0–5 2.58 0.777
X8CSNDRT X8 DCCS Nondom RT Component (0-5) Score 11,790 0–5 2.88 0.768
X9CSNDRT X9 DCCS Nondom RT Component (0-5) Score 11,247 0–5 3.19 0.790
X5CSDAC X5 DCCS Dominant Trial Accuracy Count 4,708 0–23 20.19 4.468
X6CSDAC X6 DCCS Dominant Trial Accuracy Count 13,774 0–23 20.62 3.758
X7CSDAC X7 DCCS Dominant Trial Accuracy Count 12,744 0–23 21.53 2.535
X8CSDAC X8 DCCS Dominant Trial Accuracy Count 12,021 0–23 22.05 1.852
X9CSDAC X9 DCCS Dominant Trial Accuracy Count 11,386 0–23 22.18 1.638
X5CSNDAC X5 DCCS Nondominant Trial Accuracy Count 4,708 0–7 6.08 1.128
X6CSNDAC X6 DCCS Nondominant Trial Accuracy Count 13,774 0–7 6.11 1.100
X7CSNDAC X7 DCCS Nondominant Trial Accuracy Count 12,744 0–7 6.21 1.011
X8CSNDAC X8 DCCS Nondominant Trial Accuracy Count 12,021 0–7 6.33 0.926
X9CSNDAC X9 DCCS Nondominant Trial Accuracy Count 11,386 0–7 6.40 0.865
1 Because 10 accuracy points are automatically awarded due to the chosen start point for the task, it is not possible for ECLS-K:2011 children to
obtain an accuracy score of 0. Therefore, the lowest accuracy component (0-5) score in the data file is 1.25, and the lowest computed (overall)
score in the data file is also 1.25.
NOTE: Fall second-grade estimates (X5) are weighted by W6CF6P_2A0, and spring second-grade estimates (X6) are weighted by W6CS6P_20.
Spring third-grade estimates (X7) are weighted by W7C7P_20. Spring fourth-grade estimates (X8) are weighted by W8C8P_20. Spring fifth-
grade estimates (X9) are weighted by W9C9P_20. The unweighted sample n indicates the number of cases with valid data regardless of the
presence of a valid analytic weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
3-233.2.1.1 Dimensional Change Card Sort Data Flags
Nine flags indicate the presence or absence of Dimensional Change Card Sort data.
X1DCCSFLG and X2DCCSFLG indicate the presence of data for the fall and spring of kindergarten,
respectively. X3DCCSFLG and X4DCCSFLG indicate the presence of first-grade data for the fall and
spring, respectively; X5DCCSFLG and X6DCCSFLG indicate that data are present for the overall
computed DCCS score (X5DCCSSCR/X6DCCSSCR) for the fall and spring of second grade,
respectively; X7DCCSFLG indicates that data are present for the overall computed DCCS score
(X7DCCSSCR) for the spring of third grade; X8DCCSFLG indicates that data are present for the overall
DCCS score (X8DCCSSCR) for the spring of fourth grade; and X9DCCSFLG indicates that data are
present for the overall DCCS score (X9DCCSSCR) for the spring of fifth grade.
The use of computers for the administration of the DCCS in second, third, fourth, and fifth
grades allowed the completion flags (X5DCCSFLG, X6DCCSFLG, X7DCCSFLG, X8DCCSFLG,
X9DCCSFLG) to be developed with additional detail that was not available for kindergarten and first grade.
The values indicate whether the task was administered, whether the overall computed DCCS score is
present, and, if a score is not present, the reason why it is not present. Reasons why a score is not present
when the DCCS was administered include failing the Shape practice trials, failing the Color practice trials,
and having an administrative breakoff (meaning the assessor ended the task) either before or after passing
the practice trials. Administrative breakoffs could have occurred for a variety of reasons such as an external
event (for example, a fire drill or the child needing to return to class) that interrupted an assessment session.
Note that the Shape Game preceded the Color Game during the practice trials. There are differences
between the second-grade, third-grade, fourth-grade, and fifth-grade DCCS flags, as explained below.
The DCCS flags for the fall and spring of second grade, the spring of fourth grade, and the
spring of fifth grade have 6 possible values. A description of the values of these completion flags is
presented in exhibit 3-1.
Exhibit 3-1. Data flag description for the computerized Dimensional Change Card Sort for fall and
spring second grade, spring fourth grade, and spring fifth grade: School year 2012–13,
spring 2015, and spring 2016
X5DCCSFLG/X6DCCSFLG/X8DCCSFLG/X9DCCSFLG Value
Not Administered 0
DCCS computed (overall) score present 1
Failed Shape Game practice 2
Failed Color Game practice 3
Breakoff before passing practice trials 4
Breakoff after passing practice trials 5
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study,
Kindergarten Class of 2010–11 (ECLS-K:2011), fall 2012, spring 2013, spring 2015, and spring 2016.
3-24The DCCS flag variable for the spring of third grade, X7DCCSFLG, ranges from 0 to 7. A
description of the values of the completion flag is presented in exhibit 3-2. Two additional codes not used
in second, fourth, and fifth grades were added to the third-grade flag to identify a small number of cases
that were affected by a programming error that occurred in the third-grade administration of the DCCS.
This error resulted in giving children credit for a correct response when the child did not provide a
response to a trial. This scoring error occurred in both the practice and test trials. Scoring errors that
occurred during the test trials were corrected in the data. These errors did not affect the child’s experience
during the test, but only affected how the trial was recorded.
Exhibit 3-2. Data flag description for the computerized the computerized Dimensional Change Card
Sort (DCCS) for spring third grade: Spring 2014
X7DCCSFLG Value
Not Administered 0
DCCS computed (overall) score present 1
Failed Shape Game practice 2
Failed Color Game practice 3
Breakoff before passing practice trials 4
Breakoff after passing practice trials 5
Programming error but still passed practice, DCCS data present 6
Programming error, insufficient practice, DCCS data set to -4 7
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study,
Kindergarten Class of 2010–11 (ECLS-K:2011), spring 2014.
Errors that occurred during the third-grade practice trials, however, did affect the child’s
experience during the test and, in some cases, resulted in insufficient opportunity for the child to
demonstrate an understanding of the rules of the game. When a child did not respond to a trial in the
practice, the program treated the nonresponse as a correct response and provided incorrect audio feedback
to the child. The audio feedback that the child heard was “That’s right,” even though the child did not
provide a response. If the child did not respond to a trial, the trial was supposed to be scored as incorrect,
and the audio feedback was supposed to indicate that the child responded with an incorrect answer and
reteach the rule. The erroneous feedback during the practice could have confused the child about the rules
of the game. It is important for the child to demonstrate a clear understanding of the rules of the game in
the practice trials before progressing to the test trials to ensure that performance is not a reflection of
failing to understand the instructions. Under some circumstances, having nonresponse scored as correct
affected what practice trials were administered.
Cases affected by the third-grade programming error were examined to determine whether
they met the criteria for moving into the test trials based on the items for which they did provide a
3-25response (that is, whether they demonstrated sufficient understanding of the task despite receiving
erroneous feedback). These cases, children who had at least one instance of nonresponse in the practice,
are flagged as a 6 or 7 in the DCCS flag variable depending on whether they met the criteria. Cases that
have X7DCCSFLG=6 passed the practice trials with the responses they provided during the
administration of the DCCS. For example, a child may have had 3 correct responses and 1 nonresponse
within the block of four practice trials and, thus, the criterion of responding correctly to at least 3 of 4
correct in order to proceed was still reached. As another example, the child could have had two
nonresponse trials and two incorrect trials and failed the first practice set. In this case, the child would
have been administered another practice block of four trials and could have passed on that set of practice
trials. Cases that have the value of 6 on the DCCS flag are cases that successfully met the criteria for
passing both the shape and color practice and advanced to the test trial, despite receiving at least one
instance of erroneous feedback. There are 189 cases that have X7DCCSFLG=6, and data for these cases
are provided on the data file. Additional information on this error is provided in the appendix.
Cases that have X7DCCSFLG=7 did not demonstrate sufficient understanding of the task
with the responses they provided and were not given sufficient practice per the administration protocols to
have their scores included in the data file. These cases were not given the opportunity to meet the
criterion for passing the practice because nonresponse was incorrectly recorded as a correct response. For
example, children who had 2 correct trials, 1 incorrect trial and 1 nonresponse trial (incorrectly scored as
“correct”) were incorrectly given credit for passing the practice, even though they only had 2 correct trials
and did not meet the criterion of at least 3 of 4 correct to pass. In this example, if the program had
performed correctly, the child would have been given additional training and additional opportunities to
pass the practice. Because of the programming error, this did not happen and the child progressed to the
test trials without truly meeting the criterion for successfully passing the practice. Because it was not
possible to determine whether the children could have passed the practice if given the correct
opportunities, the data were suppressed. There are 92 cases that have X7DCCSFLG=7. These cases have
DCCS data set to -4 (suppressed due to insufficient practice).
3.2.2 Numbers Reversed
The Numbers Reversed measure assesses the child’s working memory. It is a backward digit
span task that requires the child to repeat an orally presented sequence of numbers in the reverse order in
which the numbers are presented. For example, if presented with the sequence “3…5,” the child would be
expected to say “5…3.” Children are given up to 5 two-number sequences. If the child gets three
consecutive two-number sequences incorrect, then the Numbers Reversed task ends. If the child does not get
three consecutive two-number sequences incorrect, the child is then given up to 5 three-number sequences.
3-26The sequence becomes increasingly longer, up to a maximum of eight numbers, until the child gets three
consecutive number sequences of the same length incorrect (or completes all number sequences).
Item-level data for the Numbers Reversed subtask for the fall and spring of kindergarten,
first grade, and second grade, and spring of third grade, fourth grade, and fifth grade are provided in the
ECLS-K:2011 K-5 data file. The maximum number of items any child could have been administered in
all data collection rounds was 30 items (5 two-digit number items; 5 three-digit number items; 4 four-
digit number items; 4 five-digit number items; 4 six-digit number items; 4 seven-digit number items; and
4 eight-digit number items). Each item is scored “correct” (i.e., the child correctly repeated the number
sequence in reversed order), “incorrect” (i.e., the child did not correctly repeat the number sequence in
reversed order), or “not administered” (i.e., the child was not administered the item because he or she did
not answer enough items correctly to advance to this item). The “not administered” code is different than
a system missing code in that only those children who were administered the Numbers Reversed subtask
could have a “not administered” code. If a child was not administered the Numbers Reversed subtask at
all, his or her case would have a missing code for the Numbers Reversed scores. Variable names for the
item-level data from the fall kindergarten assessments begin with “C1,” and variable names for the item-
level data from the spring kindergarten assessments begin with “C2.” Similarly, variable names for item-
level data from the fall and spring first-grade assessments begin with “C3” and “C4,” while those for fall
and spring second grade and spring third grade begin with “C5,” “C6,” and “C7,” respectively. Variable
names for the item-level data from the spring fourth-grade assessment begin with “C8,” and variable
names for the item-level data from the spring fifth-grade assessment begin with “C9.” Variable
descriptions for these items indicate the length of the digit sequence (e.g., C1 Numbers Reversed Two-
digit sequence #1). In addition to the item-level data, five scores developed using guidelines from the
publisher’s scoring materials are included in the data file for Numbers Reversed: the W-ability12 score, the
age standard score, the grade standard score, the age percentile score, and the grade percentile score.
Before analyzing the Numbers Reversed data, it is important that researchers understand the
characteristics of these scores and how these characteristics may affect the analysis and interpretation of
the Numbers Reversed data in the context of the ECLS-K:2011. Depending on the research question and
analysis being conducted, one of the scores may be more preferable than another. For example, the W
score may be best for a longitudinal analysis, whereas the age or grade percentile rank and/or age or grade
standardized score may be better suited for an analysis focusing on one point in time. The descriptions
below provide more information about which score may be better suited for a given analysis.13
12 The W-ability score is a W score that represents the individual’s level of ability on the task presented.
13 More information on these publisher scores can be found in the Woodcock-Johnson III Tests of Achievement Examiner’s Manual: Standard
and Extended Batteries (Mather and Woodcock 2001).
3-27The W score, a type of standardized score, is a special transformation of the Rasch ability
scale and provides a common scale of equal intervals that represents both a child’s ability and the task
difficulty. The W scale is particularly useful for the measurement of growth and can be considered a
growth scale. Typically, the W scale has a mean of 500 and standard deviation of 100. Furthermore, the
publisher of the WJ III has set the mean to the average of performance for a child of 10 years, 0 months.
This means that it would be expected that most children younger than 10 years, 0 months would obtain W
scores lower than the mean of 500, and most older children would be expected to have scores above the
mean of 500. Also, as a child develops with age, it would be expected that the child’s W score would
increase to reflect growth. For example, when a child’s W-ability score increases from 420 to 440, this
indicates growth, and this would be the same amount of growth in the measured ability as any other
student who gained 20 W points elsewhere on the measurement scale.
As mentioned above, the W score is an equal-interval scale, suited for analyses such as
correlations and regressions. Higher W scores indicate that a child provided more correct responses and
generally indicate that a child was able to correctly respond to at least some longer number sequences.
The W score accounts for only the total number of administered sequences answered correctly and does
not reflect the pattern of responses, meaning the W score does not indicate how many of each length
number sequence the child answered correctly. As noted above, the data file includes item-level data that
can be used to examine patterns of response.
The W score for each child in the ECLS-K:2011 was determined using norming data
provided by the publisher. More specifically, a sample child was assigned the W score from the publisher
norming data that was associated with the child’s raw number-right score, the child’s age (in months), and
the language of administration.
In kindergarten and first grade, the Numbers Reversed subtask was administered in both
English and Spanish. It was administered in Spanish to children routed through the assessment battery in
Spanish because they did not pass an English language screener.14 Norming data were provided separately
for English and Spanish administrations of the task. Publisher materials indicate that the W scores earned
on English administrations of the Numbers Reversed task are comparable to W scores earned on Spanish
administrations of the task; nevertheless, differences related to precision of measurement in the norming
samples result in different W scores for the same raw-number right score depending on the language of
administration. For example, the lowest earnable W score on the English administration of the Numbers
Reversed task is 403 (equivalent to a raw score of 0), and the lowest earnable W score on the Spanish
administration is 393 (equivalent to raw score of 0). While this difference in the W scores between
English and Spanish administration is largest at the lower end of the W distribution, the difference occurs
14 More information about how children’s home language affected children’s routing through the assessment battery in each round of data
collection is provided in chapter 5 of the ECLS-K:2011 Kindergarten Psychometric Report (Najarian et al. 2018a).
3-28along the entirety of the W distribution. For example, a raw score of 11 corresponds to a W score of 496 in
the English administration norming data and a W score of 494 in the Spanish administration norming data.
The data file includes one W score variable per round of data collection that contains data for all children
administered the Numbers Reversed task, regardless of the language of administration. Researchers who
want to account for language of administration in their analyses can use the data flag provided on the data
file for each round (X*FLSCRN) to identify which children were administered Numbers Reversed in
English and which children were administered Numbers Reversed in Spanish. All children were
administered the assessments in English starting with the second-grade data collection. Therefore, the
second-, third-, fourth-, and fifth grade Numbers Reversed scores for all children are based on an English
administration of the assessment, and data flags to indicate language administration in grades second
through fifth are not provided on the data file.
Although the W score is reflective of the average performance of 10-year-olds, and the ECLS-
K:2011 children are younger in the earlier rounds of the study, it is included in the data file to enable the
measurement of changes in children’s working memory longitudinally across all rounds of the study. Also,
it facilitates comparisons of the ECLS-K:2011 data with data from other studies that include the Numbers
Reversed task. Users should keep in mind that most ECLS-K:2011 sample children were primarily 5 or 6
years old during the kindergarten data collections, 6 or 7 years old during the first-grade data collections, 7
or 8 years old during the second-grade data collections, 8 or 9 years old during the third-grade data
collection, 9 and 10 years old during the fourth-grade data collection, and 10 and 11 years old during the
fifth-grade data collection15 while the W scores compare their performance to that of 10-year-olds. As a
result, W scores from the ECLS-K:2011 appear to show that the ECLS-K:2011 children demonstrated below
average performance on this task from kindergarten through fourth grades and above average performance
in fifth grade. However, because the mean of the W scale was set by the publisher based on the average
performance for a child 10 years, 0 months, this pattern is as expected. As expected, the discrepancy
declined as the participating children grew older and closer to age 10. Because the average age at
assessment was approximately age 11 years in the spring of fifth grade, it is not surprising that the average
W score is above 500, the mean set for the average performance of a child 10 years, 0 months.
A score of 403 (393 for the Spanish administration) is potentially a meaningful baseline value
for the ability level of children who are unable to answer any items correctly. Over time, as children develop
more ability that is measurable by the WJ III Numbers Reversed task, the study is able to compare
children’s baseline Numbers Reversed W score (fall kindergarten and/or spring kindergarten Numbers
Reversed W score) with children’s scores across future administrations of the task. However, researchers
should understand that a raw score of 0 (which translates to a W score of 403 for the English administration
15 For the fourth-grade assessment, approximately 56 percent of the children were 10 years old or older, and approximately 44 percent of the
children were 9 years old or younger. For the fifth-grade assessment, nearly all the children were 10 years old or older (99.9 percent).
3-29and 393 for the Spanish administration) is an imprecise measure of children’s ability in the area of working
memory, because it is unknown how close a child was to getting at least one answer correct.
In the fall of kindergarten, approximately 40 percent of students did not demonstrate
sufficient skills as measured by this assessment to score above the lowest scalable score (403 for English
assessment and 393 for Spanish assessment). In the spring of kindergarten, approximately 20 percent of
students did not score above the lowest scalable score (403 for English, 393 for Spanish). In the fall of
first grade, less than 13 percent scored at the lowest scalable score, and only 6 percent scored at the
lowest scalable score in the spring of first grade. In the fall of second grade, less than 4 percent scored the
lowest scalable score, and slightly more than 2 percent received the lowest score in the spring. In the
spring of third grade, 1 percent scored at the lowest scalable score. In the spring of fourth grade, 0.6
percent scored at the lowest scalable score. In the spring of fifth grade, 0.5 percent scored the lowest
scalable score.
A factor that may contribute to the large number of children scoring 403 (and 393 for
Spanish) in kindergarten is that some ECLS-K:2011 assessors did not properly administer the practice
items, which may have resulted in some children never fully understanding what they were being asked to
do during the Numbers Reversed task. During field observations of the assessors, it was noted that when
children did not correctly answer the first practice item, there were inconsistencies in the administration
of additional practice items. It is not possible to determine the extent to which improper administration of
the practice items affected the results. However, readers should keep in mind that this may have affected
performance for some (but not all) children. In conducting analyses, researchers need to decide how to
handle the 403 (393 for Spanish) scores; the decision for how to do so is left up to the analyst based on
his or her analytic goals. For the first-grade and later data collections, assessor training for the Numbers
Reversed task was changed to improve the consistency and clarity of administration of the practice items.
The instructions trainers provided to the assessors emphasized the need to present practice items
consistently and to present multiple practice items when necessary. More information about the Numbers
Reversed scoring and data can be found in the ECLS-K:2011 Kindergarten Psychometric Report
(Najarian et al. 2018a).
The four additional Numbers Reversed scores are the age standard score, the grade
standard score, the age percentile score, and the grade percentile score. These scores indicate children’s
status relative to their peers through age-normed and grade-normed transformations of the data. That is,
these scores are relative to same-aged or same-grade subjects in the WJ III norming sample. The standard
scores are created by the publisher and have a mean of 100 and a standard deviation of 15. The score is a
linear transformation of a Z score (mean of 0 and a standard deviation of 1), which is derived from a
person’s achieved W score. The percentile rank scores describe performance on a scale from 0 to 100
3-30relative to the performance of subjects in the WJ III norming sample that is at the same age or grade as
the ECLS-K:2011 subjects.
As with the kindergarten and first-grade W scores, the kindergarten and first-grade standard
scores and percentile scores in the data file contain data from both the English and Spanish administrations
of the Numbers Reversed task. Standard scores and percentile scores are a function of the child’s age or
grade at assessment. The publisher’s scoring protocols result in standard and percentile scores that extend to
slightly lower ages for children who were administered the task in Spanish compared to children who were
administered the task in English, again due to differences in the precision of measurement within the
norming samples. Children 62 months and younger who were administered the Numbers Reversed task in
English and who earned a raw score of 0 or 1 have a W score but do not have a standard score or percentile
score (W scores are a function of the number correct and not a function of age). However, all children who
were administered this task in Spanish, including those aged 62 months and younger have a W score,
standard scores, and percentile scores, regardless of their raw score. Again, researchers who want to account
for language of administration in their analyses during kindergarten or first grade can use the variables
X1FLSCRN, X2FLSCRN, X3FLSCRN, and X4FLSCRN to identify language.
For both the age-normed scores and the grade-normed scores, standard scores and percentile
ranks lend themselves to different interpretations. Standard scores and percentile ranks are not essentially
the same. Standard scores are deviation-based scores, based upon a mean and standard deviation that
remains constant across the entire range. They are interval data, where values are separated by a constant
interval that maintains the same meaning across the full range. Percentile ranks are neither interval data
nor constant and cannot be used interchangeably with standardized scores. As such, standard scores are
most appropriately used for comparisons across children and between groups; W scores (also a deviation-
based score metric) are most appropriately used to look at growth over time, where age-normed standard
scores may remain relatively constant with an age-expected rate of growth. Percentiles are less ideal for
longitudinal analyses; although they can be used to examine relative rank order consistency across time
periods, the W scores would be better to assess change and/or stability across time.
The weighted means for the ECLS-K:2011 population are lower than the established means
from the WJ III norming sample in some rounds and higher than the established means from the WJ III
norming sample in other rounds.
16 For example, the average W scores for the ECLS-K:2011 population
are less than 500 in kindergarten through fourth grades but higher than 500 in fifth grade. The average
16 Normative data for the WJ III were gathered from 8,818 subjects in more than 100 geographically diverse U.S. communities (McGrew and
Woodcock 2001). The kindergarten through 12th grade sample was composed of 4,783 subjects. The norming sample was selected to be
representative of the U.S. population from age 24 months to age 90 years and older. Subjects were randomly selected within a stratified sampling
design that controlled for the following 10 specific community and subject variables: census region (Northeast, Midwest, South, West);
community size (city and urban, larger community, smaller community, rural area); sex; race (White, Black, American Indian, Asian and Pacific
Islander); Hispanic or non-Hispanic; type of school (elementary, secondary, public, private, home); type of college/university (2-year, 4-year,
public, private); education of adults; occupational status of adults; occupation of adults in the labor force.
3-31age standard scores are less than 100 in all rounds. The average grade standard scores are less than 100 in
kindergarten through second grades but higher than 100 in third through fifth grades. The average age
and grade percentile scores are less than 50 in some rounds and above 100 in other rounds. The lower
mean for the W scores in the ECLS-K:2011 may be attributed to the derivation of the score being a
comparison to the average 10-year-old (generally 10-year-olds are in fourth or fifth grade)17 or to
differences between the ECLS-K:2011 population and the WJ III norming sample. The differences
between weighted means for the average age and grade standard scores and percentile scores for the
ECLS-K:2011 population compared to the established means from the WJ III norming sample may also
be attributable to differences between the ECLS-K:2011 population and the WJ III norming sample.
The variable names, descriptions, value ranges, weighted means, and standard deviations for
the Numbers Reversed scores from the fall of kindergarten to the spring of fifth grade are shown in
table 3-6.
Table 3-6. Numbers Reversed variable names, descriptions, value ranges, weighted means, and standard
deviations for fall and spring kindergarten, fall and spring first grade, fall and spring second
grade, spring third grade, spring fourth grade, and spring fifth grade: School years 2010–11,
2011–12, 2012–13; spring 2014; spring 2015; and spring 2016
Variable name Description n
Value
ranges
Weighted
mean
Standard
deviation
X1NRWABL X1 Numbers Reversed W-Ability Score 15,598 393–603 432.56 30.028
X1NRSSCR X1 Numbers Reversed Age Standard Score 14,445 45–200 93.10 16.510
X1NRSSGR X1 Numbers Reversed Grade Standard Score 15,598 33–200 96.40 14.569
X1NRPERC X1 Numbers Reversed Age Percentile 14,445 0–100 37.89 31.786
X1NRPEGR X1 Numbers Reversed Grade Percentile 15,598 0–100 41.98 30.886
X2NRWABL X2 Numbers Reversed W-Ability Score 17,147 393–603 449.49 30.412
X2NRSSCR X2 Numbers Reversed Age Standard Score 17,124 39–200 94.92 17.017
X2NRSSGR X2 Numbers Reversed Grade Standard Score 17,147 33–200 94.76 16.049
X2NRPERC X2 Numbers Reversed Age Percentile 17,124 0–100 42.44 30.970
X2NRPEGR X2 Numbers Reversed Grade Percentile 17,147 0–100 41.89 29.980
X3NRWABL X3 Numbers Reversed W-Ability Score 5,222 393–603 458.42 27.990
X3NRSSCR X3 Numbers Reversed Age Standard Score 5,221 36–200 94.21 16.969
X3NRSSGR X3 Numbers Reversed Grade Standard Score 5,222 24–200 95.19 17.815
X3NRPERC X3 Numbers Reversed Age Percentile 5,221 0–100 41.23 28.832
X3NRPEGR X3 Numbers Reversed Grade Percentile 5,222 0–100 43.61 29.857
X4NRWABL X4 Numbers Reversed W-Ability Score 15,107 393–603 469.56 25.395
X4NRSSCR X4 Numbers Reversed Age Standard Score 15,102 24–200 95.90 16.872
X4NRSSGR X4 Numbers Reversed Grade Standard Score 15,107 19–200 95.42 18.159
X4NRPERC X4 Numbers Reversed Age Percentile 15,102 0–100 44.35 28.470
X4NRPEGR X4 Numbers Reversed Grade Percentile 15,107 0–100 44.07 29.276
See notes at end of table.
17 For the fourth-grade assessment, approximately 56 percent of the children were 10 years old or older, and approximately 44 percent of the
children were 9 years old or younger. For the fifth-grade assessment, nearly all children were 10 years old or older (99.9 percent).
3-32Table 3-6. Numbers Reversed variable names, descriptions, value ranges, weighted means, and standard
deviations for fall and spring kindergarten, fall and spring first grade, fall and spring second
grade, spring third grade, spring fourth grade, and spring fifth grade: School years 2010–11,
2011–12, 2012–13; spring 2014; spring 2015; and spring 2016—Continued
Variable name Description n
Value
ranges
Weighted
mean
Standard
deviation
X5NRWABL X5 Numbers Reversed W-Ability Score 4,727 403–603 473.93 23.736
X5NRSSCR X5 Numbers Reversed Age Standard Score 4,727 29–200 94.93 16.574
X5NRSSGR X5 Numbers Reversed Grade Standard Score 4,727 19–200 95.85 17.561
X5NRPERC X5 Numbers Reversed Age Percentile 4,727 0–100 42.13 27.609
X5NRPEGR X5 Numbers Reversed Grade Percentile 4,727 0–100 44.17 28.742
X6NRWABL X6 Numbers Reversed W-Ability Score 13,832 403–603 480.70 22.841
X6NRSSCR X6 Numbers Reversed Age Standard Score 13,828 25–200 95.80 16.749
X6NRSSGR X6 Numbers Reversed Grade Standard Score 13,832 18–200 95.52 17.715
X6NRPERC X6 Numbers Reversed Age Percentile 13,828 0–100 43.67 27.765
X6NRPEGR X6 Numbers Reversed Grade Percentile 13,832 0–100 43.59 28.680
X7NRWABL X7 Numbers Reversed W-Ability Score 12,877 403–603 489.78 21.624
X7NRSSCR X7 Numbers Reversed Age Standard Score 12,874 20–200 96.34 16.185
X7NRSSGR X7 Numbers Reversed Grade Standard Score 12,877 18–195 102.74 17.037
X7NRPERC X7 Numbers Reversed Age Percentile 12,874 0–100 44.10 27.742
X7NRPEGR X7 Numbers Reversed Grade Percentile 12,877 0–100 55.90 28.907
X8NRWABL X8 Numbers Reversed W-Ability Score 12,085 403–603 497.17 21.333
X8NRSSCR X8 Numbers Reversed Age Standard Score 12,082 15–192 96.65 15.975
X8NRSSGR X8 Numbers Reversed Grade Standard Score 12,085 19–200 101.86 16.819
X8NRPERC X8 Numbers Reversed Age Percentile 12,082 0–100 44.28 27.780
X8NRPEGR X8 Numbers Reversed Grade Percentile 12,085 0–100 54.01 28.724
X9NRWABL X9 Numbers Reversed W-Ability Score 11,430 403–603 503.12 22.005
X9NRSSCR X9 Numbers Reversed Age Standard Score 11,429 130–182 96.67 16.494
X9NRSSGR X9 Numbers Reversed Grade Standard Score 11,430 19–200 100.92 17.017
X9NRPERC X9 Numbers Reversed Age Percentile 11,429 0–100 44.34 28.576
X9NRPEGR X9 Numbers Reversed Grade Percentile 11,430 0–100 52.28 29.149
NOTE: Fall kindergarten estimates (X1) and spring kindergarten estimates (X2) are weighted by W1C0. Fall first-grade estimates (X3) are
weighted by W3CF3P_30, and spring first-grade estimates (X4) are weighted by W4CS4P_20. Fall second-grade estimates (X5) are weighted by
W6CF6P_2A0, and spring second-grade estimates (X6) are weighted by W6CS6P_20. Spring third-grade estimates (X7) are weighted by
W7C7P_20. Spring fourth-grade estimates (X8) are weighted by W8C8P_20. Spring fifth-grade estimates (X9) are weighted by W9C9P_20. The
unweighted sample n indicates the number of cases with valid data regardless of the presence of a valid analytic weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
3.2.2.1 Numbers Reversed Data Flags
Nine flags indicate the presence or absence of Numbers Reversed data. X1NRFLG and
X2NRFLG indicate the presence of data for the fall and spring of kindergarten, respectively. X3NRFLG
and X4NRFLG indicate the presence of first-grade data for the fall and spring, respectively, and
X5NRFLG and X6NRFLG indicate the presence of fall and spring second-grade data, respectively.
3-33X7NRFLG, X8NRFLG, and X9NRFLG indicate the presence of data for spring third-grade, spring
fourth-grade, and spring fifth-grade, respectively.
There is one other flag, X*NRGEST, related to Numbers Reversed that is provided for each
round of data collection. The Numbers Reversed grade-normed scores (X*NRSSGR, X*NRPEGR) are
normed according to how far into the school year the assessment was conducted. Decimals are used to
indicate the number of months into the school year the child had been in the grade at the time of the
assessment (e.g., 0.1 = 1 month; 0.2 = 2 months, etc.; 0.9 = 9 months, including time in the summer prior
to the start of the next grade level). When school year start and end dates were not available, it was
necessary to estimate the decimal representing the proportion of the school year completed when the
assessment occurred. X*NRGEST indicates whether the number of months completed in the grade was
estimated for that round of data collection. In fifth grade, time in grade was estimated for 3 percent of
children.
3.2.3 The NIH Toolbox Flanker Inhibitory Control and Attention Task (Flanker)
The NIH Toolbox Flanker Inhibitory Control and Attention Task (Flanker) is a
computerized task that was developed as part of the NIH Toolbox for the Assessment of Neurological and
Behavioral Function (NIH Toolbox) and is appropriate for ages 3–85 (Zelazo et al. 2013). The Flanker
was adapted from the Attention Network Test (ANT; e.g., Rueda et al., 2004), which was based on the
Eriksen flanker task (Eriksen and Eriksen, 1974). The Flanker (Zelazo et al 2013) was added to the
ECLS-K:2011 assessment battery in fourth grade and it was administered again in fifth grade. It is a
measure of executive function; specifically, it is a measure of inhibitory control in the context of selective
visual attention.
The ECLS-K:2011 used the version of the NIH Toolbox Flanker task that is for children 8
years and older.18 Starting with the fourth-grade administration of the ECLS-K:2011, all children were at
least 8 years old. The Flanker task measures inhibitory control in the context of selective visual attention
(Slotkin, Nowinski, et al. 2012). In this task children must inhibit an automatic response tendency that
may interfere with achieving a goal and use selective attention to consciously direct sensory or thought
18 The NIH Toolbox Flanker task has two different start points based on the age of the child. Children aged 3-7 begin the task with trials that use
fish as the stimulus and progress to harder trials that use arrows as stimuli if performance on the fish trials is 90 percent or more correct. By
design, children who are 8 years and older begin with the arrow trials and are given credit for successful completion of the fish trials because it
was determined that the majority of children 8 years and older could successfully complete the easier fish trials. The task includes two different
start points in order to reduce participant burden and create a task with a shorter administration time. Because all children in the ECLS-K:2011
study were at least 8 years of age in the fifth-grade data collection, all of them began with the arrow trials and were given credit in the scoring for
successfully completing the fish trials.
3-34processes to a stimulus in the visual field in the service of goal-directed behavior. In the Flanker task,
children are asked to focus attention on a central stimulus while ignoring or inhibiting attention to stimuli
presented on either side of the central stimulus. The stimulus used for children 8 years and older is a
series of five arrows, pointing either left of right. The arrows that “flank” the central arrow, which are
referred to as “flankers,” either point in the same direction as the central arrow (congruent) or in the
opposite direction as the central arrow (incongruent). The flanker arrows act as distractors, taking
attention away from the central arrow that is supposed to be the focus of the child’s attention. Children
are presented with 20 arrow trials and are asked to press a button on the computer to indicate the direction
the central stimulus (arrow) is pointing. Like the DCCS, the score based on the Flanker is derived from a
formula that takes into consideration both accuracy and reaction time (Zelazo et al. 2013; Slotkin,
Nowinski, et al. 2012). Performance on the incongruent trials is used to derive a score that is a measure of
inhibitory control in the context of selective visual attention.
At the start of the 20 test trials, children were instructed to “Keep your eyes on the star.
Answer as fast as you can without making mistakes. If you make a mistake, just keep going.” Each of the
test trials began with a picture of a star presented on the screen in the location where the central (target)
stimulus was about to appear. The star served to direct the child’s gaze and orient the child’s attention to a
standard location, the location where the child needed to be looking. Next, the word “MIDDLE” appeared
on the screen in the same location while a prerecorded female voice said “middle,” to remind the child to
look at the middle arrow and to indicate the direction of that arrow. Next, a series of five arrows appeared
on the screen in a line, and the child’s task was to press the left arrow key if the arrow in the middle of the
five arrows (i.e., the central arrow) was pointing to the left or press the right arrow key if the central
arrow was pointing to the right.
The 20 test trials were the same for all children. The direction of the central arrow was
counterbalanced across the 20 trials, and there were more congruent trials than incongruent trials. There
were 13 congruent trials (central arrow pointed in the same direction as the arrows flanking it) and 7
incongruent trials (central arrow pointed in the opposite direction as the arrows flanking it). For example,
the central arrow for trial 1 was left-facing, and the flankers were congruent; the central arrow for trial 2
was right-facing, and the flankers were congruent; and the center arrow for trial 3 was right-facing, and
the flankers were incongruent (i.e., left-facing). Like the DCCS, the congruent and incongruent trials in
the Flanker were intermixed across the trials, and the number of congruent trials preceding an
incongruent trial did not follow a pattern. Congruent trials were more frequent in order to build a response
tendency (i.e., a response that is “preferred” because it happens more frequently, resulting in a
predisposition to respond in that manner). A predisposition to respond based on the orientation of the
distractors flanking the central stimulus further increases the difficulty of the incongruent trials; the child
3-35must ignore or inhibit attention to the distractors, and this is easier to do when the flankers are congruent.
Congruent trials are easier because there is no conflict between the central stimulus and its flankers since
all the arrows are pointing in the same direction. Incongruent trials are more difficult because the flankers
pointing in the opposition direction from the central stimulus create a distraction with conflicting
information. The child needs to respond based solely on the direction of the central stimulus rather than
the conflicting and distracting information. To do this, the child must selectively attend to the central
arrow, inhibit attention to the conflicting and distracting information provided by the flankers, and inhibit
an automatic tendency to respond based on the direction of the flankers.
There is a “cost” in performance that is associated with the conflicting and distracting
information presented in the incongruent trials. As discussed in the section on the DCCS, the “cost” to the
child’s performance on this task that is associated with this conflict can be seen in either more errors or a
slower reaction or response time on incongruent trials. The type of “cost” that is demonstrated (more
errors vs. slower reaction time) tends to differ by the age of the participant (Davidson et al. 2006).
Younger children tend to demonstrate this cost by having more errors in performance, whereas older
children tend to demonstrate this cost by having slower reaction times. Younger children tend to make
more errors on incongruent trials because they tend to respond quickly without making an adjustment for
the need to ignore the conflict presented by the distractors. Younger children do not slow themselves
down in favor of higher accuracy, and, therefore, accuracy is a better metric of performance for young
children (Zelazo et al. 2013). In contrast, older children and adults tend to demonstrate a speed/accuracy
tradeoff; they slow down the pace at which they respond in order to maintain accuracy. Thus, older
children and adults demonstrate their “cost” to ignore the conflict of the incongruent flankers in terms of
their reaction time on incongruent trials.
Using a scoring method that takes both speed and accuracy into consideration is a strategy
for overcoming the challenge of comparing scores of children with developmental differences in the
ability to make a speed accuracy tradeoff. The scoring algorithm used to produce scores from the data
collected by the Flanker is analogous to the formula used for the computerized DCCS. The scoring
algorithm factors in reaction time on the incongruent trials but only when the child demonstrates
sufficiently accurate performance across all the test trials, defined as being accurate on more than 80
percent of the trials (Zelazo et al. 2013). Thus, the Flanker provides a measure of performance through
this developmental shift to learning to trade speed for accuracy. More information on scoring is provided
below.
The 20 test trials were administered only to children who successfully completed the practice
portion of the Flanker. The assessor instructed the child on how to do the task by reading the standardized
3-36task instructions that appeared on the screen alongside example stimuli and by familiarizing the child with
the response buttons to use on the computer keyboard (left and right arrow key). The child could be
presented with up to three sets of four practice trials. Each set of practice trials included two congruent
trials (one with all arrows pointing to the left and one with all arrows pointing to the right) and two
incongruent trials (one with a left-facing central arrow and one with a right-facing central arrow). In order
to pass the practice and progress to the test or scored trials, the child had to have three or more correct
practice trials within a single set of four practice trials. If the child did not pass the first set of practice
trials, a second set was presented. If the child did not pass the second set of practice trials, a third set of
practice trials was administered. If the child was not able to pass any of the three sets of practice trials, the
Flanker ended before any actual scored trials were presented and the child moved into the science
assessment.
Before the practice trials started, children were presented with a screen providing the same
standardized instructions that are described above for the test trials, which the assessor read. As noted
above, the instructions stated, “Keep your eyes on the star. Answer as fast as you can without making
mistakes. If you make a mistake, just keep going.” The practice trials were like the subsequent test trials
in that a star appeared first on the screen to act as focal point and a recorded female voice said “middle”
to remind the child to look at and indicate the direction of the middle arrow. However, unlike in the test
trials, during the practice trials the recorded voice was used to provide feedback to the child. If the child
answered a practice trial correctly, the recorded voice said “That’s right!” If the child did not respond
correctly to a practice trial, the recorded voice provided feedback to the child to explain the correct
answer and why it was correct.
Item-level data for the 20 scored test trials are included in the data file. Data are provided for
four aspects of each test trial: (1) correct versus incorrect responses (C9FLKACC1-C9FLKACC20); (2)
the type of trial, reported as congruent (more frequently presented but not included in reaction time
scores; central arrow faces in the same direction as the flanking arrows) or incongruent (less frequently
presented and used to calculate reaction time scores; central arrow faces in the direction opposite from the
flanking arrows) (C9FLKCIC1-C9FLKCIC20); (3) reaction time reported in milliseconds (C9FLKRT1-
C9FLKRT20); and (4) the direction that the central arrow faces (C9FLKARW1-C9FLKARW20).19
Therefore, there are four variables associated with each of the 20 test trials. Children who did not pass any
of the three sets of practice trials do not have item-level data because the item-level data correspond to the
actual scored trials. Variable names for the item-level data begin with “C9” for spring fifth grade.
19 A variable to describe the direction that the central arrow faces is not necessary for analyzing task performance. It is included on the data file to
allow researchers to reconstruct the exact trials that were presented in case there is interest in doing so.
3-37The overall computed score reported for the fifth-grade Flanker is derived using a formula
provided by the task developer and follows the scoring algorithm used for this task in the NIH Toolbox
(see NIH Toolbox Scoring and Interpretation Guide (Slotkin, Nowinski et al. 2012) for additional
information on scoring). This is the same formula used to score the computerized DCCS score, adjusted
for task parameters (number of administered trials). Like the DCCS, the overall Flanker score ranges
from 0 to 10, with weight given to accuracy (0 to 5 units) and reaction time (0 to 5 units) in the
computation of scores. Accuracy is considered first. If the child’s accuracy rate is less than or equal to 80
percent, the child’s overall computer score is based entirely on accuracy. If the child’s accuracy rate is
more than 80 percent, the child’s overall computed score is based on a combination of accuracy and
reaction time. Children who did not pass any of the three sets of practice trials do not have an overall
Flanker score.
The accuracy score factored into the computation of the overall score can range from 0 to 5.
Because all children used the Flanker start point for children 8 years and older, each child who
successfully passed the practice was administered 20 test trials and was automatically given 20 accuracy
points for 20 trials that are only administered to children younger than 8 years old. Therefore, there are a
total of 40 accuracy points that are scaled down to a maximum score of 5: for each correct response, the
child earns a score of .125 (5 points divided by 40). The accuracy component of the overall computed
Flanker score is calculated as follows:
Flanker accuracy score = 0.125 * number of correct responses20
If the child’s accuracy rate is higher than 80 percent, a reaction time score is added to the child’s accuracy
score.21 Like the accuracy score, the reaction time score ranges from 0 to 5 points.
The reaction time component of the overall computed score for the Flanker is computed
using the child’s median reaction time to correct incongruent trials (i.e., the trials with the flanking arrows
20 The number of correct responses = 20 + the number of correct arrow trials out of the 20 administered trials. Thus, once the child has passed the
practice trials and advanced into the scored portion of the assessment, 20 accuracy points are automatically awarded due to the chosen start point
for the task. For this reason, it is not possible for ECLS-K:2011 children to get an accuracy score of 0. Therefore, the minimum possible value for
the Flanker accuracy score is 2.5, and the maximum possible Flanker accuracy score is 5.
21 The criterion of greater than 80 percent accuracy is calculated based on all 40 trials (20 administered arrow trials plus the 20 nonadministered
trials that are only administered to children younger than 8 years old and are assumed to be correct and automatically awarded in this
administration). That is, 80 percent of 40 trials is 32 items. However, this can also be thought of in terms of how many items out of the 20
administered arrow trials are required. If the criterion is 80 percent of the 40 trials, this translates to 12 of the 20 administered trials (12
administered trials + 20 nonadministered trials = 32; 32 is 80 percent of the total of 40 trials). For example, if a child responds accurately on 13 of
the 20 administered arrow trials, the child’s accuracy rate equals 82.5 percent (20 points automatically awarded for the nonadministered 20 trials
plus the 13 correct arrow trials divided by 40; 33/40 = .825). In this example, the child’s accuracy score would be [(20 + 13) * .125] = 4.125.
Because the accuracy rate is greater than 80 percent, the child’s reaction time score would be added to this accuracy score to obtain the overall
computed score for the Flanker. Alternatively, if the child responded accurately on 12 of the 20 administered arrow trials, the child’s accuracy
rate would equal 80 percent and, therefore, the child’s accuracy is not greater than 80 percent and the child’s overall score would be based solely
on accuracy (overall computed score = [(20 + 12) * .125] = 4).
3-38facing in a direction opposite the central arrow), following the same scoring algorithm outlined in the
scoring manual for the NIH Toolbox (Slotkin, Nowinski, et al. 2012). First, for those children with greater
than 80 percent accuracy on the 40 trials, the median reaction time is calculated based on reaction times
for correct incongruent trials with reaction times greater than or equal to 100 milliseconds (msec) and
within plus or minus three standard deviations from the child’s mean reaction time on the correct
incongruent trials. The minimum median reaction time allowed is 500 msec; the maximum median
reaction time is 3,000 msec. If the child’s median reaction time falls outside this range, the child’s median
reaction is set to the minimum or maximum allowable range: reaction times between 100 msec and 500
msec were set to 500 msec and reaction times between 3,000 msec and 10,000 msec (the maximum trial
duration) are set to 3,000 msec. A log (base 10) transformation is applied to the median reaction times to
create a more normal distribution. The log values are then algebraically rescaled to a range of 0 to 5 and
then reversed such that faster (better) reaction times have higher values and slower reaction times have
lower values. The formula for rescaling the median reaction times is the following:
where RT is the median reaction time on incongruent trials within set outer limits.22
To summarize, the overall computed score on the computerized Flanker is equal to the
child’s accuracy score if the child’s accuracy rate is less than or equal to 80 percent. If the child’s
accuracy rate is greater than 80 percent, the child’s overall computed score is equal to the child’s accuracy
score plus the child’s reaction time score, which is derived from the child’s reaction time on correct
incongruent trials as described above. Additional details on the calculation of the computed score are
available in the NIH Toolbox Scoring and Interpretation Guide (Slotkin, Nowinski, et al. 2012) and the
NIH Toolbox Technical Manual (Slotkin, Kallen, et al. 2012).
It is important for researchers using the Flanker data to be aware of the characteristics of the
overall Flanker scores and determine how best to use these scores in their analyses. As noted above, the
NIH-developed scoring model computes scores differently depending on accuracy. The use of this scoring
model with the data collected from children in the ECLS-K:2011 resulted in a non-normal distribution.
For example, 32 children who have a computed overall Flanker score in the fifth-grade data collection
failed to achieve greater than 80 percent accuracy (0.3 percent). The score for these children is calculated
22 The median reaction time (RT) used to calculate the reaction time score falls within the range of 500 msec through 3,000 msec. Calculation of
the median score requires a minimum of at least one correct incongruent trial reaction time that is greater than 100 msec. When the child reached
the accuracy threshold for including the reaction time component in the scoring but did not have any within-range reaction times on correct
incongruent trials, the child’s overall computed score on the Flanker was set equal to the child’s accuracy score, and reaction time was not
factored into the child’s score.
3-39based solely on accuracy. There are 11 children in the fifth-grade data collection (0.1 percent) who met
the accuracy threshold but did not have any correct incongruent trials; therefore, their score was set equal
to their accuracy score because it was not possible to have a reaction time score for correct, incongruent
trials. Thus, there were a total of 43 children (32 + 11) whose overall Flanker score is based on accuracy
alone (0.4 percent). The remaining children (99.6 percent in fifth grade) who have a computed overall
score have scores calculated based on both accuracy and reaction time.
The non-normal distribution may be problematic for statistical analyses. For this reason,
users may want to run analyses that do not use the overall Flanker score as is with the full sample. For
example, users could conduct their analyses separately for the two groups of children so that each analysis
only includes children with scores calculated in the same way, or they may decide to limit their analyses
to only one group. Users who want to analyze all children using the score indicating accuracy alone
should recognize that this score is highly skewed, as most children were able to indicate the direction the
central arrow was pointing with at least 80 percent accuracy. Users may also want to consider
investigating alternative scoring models using the item-level accuracy and reaction time data available on
the data file. The decision about how best to use the Flanker overall score in analysis is left to the user,
given the research questions being addressed. Analysts may choose to examine other ways researchers
have analyzed data with similar distributions, or other executive function or flanker data, in deciding how
best to utilize the ECLS-K:2011 Flanker data. Table 3-7 presents the Flanker variable names,
descriptions, value ranges, weighted means, and standard deviations for the spring of fourth grade and the
spring of fifth grade.
Table 3-7. Flanker variable names, descriptions, value ranges, weighted means, and standard deviations
for spring fourth grade and spring fifth grade: Spring 2015 and spring 2016
Variable name Description n
Value
ranges1
Weighted
mean
Standard
deviation
X8FLANKER X8 Flanker Computed (Overall) Score 12,009 0–10 7.98 0.984
X8FLKACC X8 Flanker Accuracy Component (0-5) Scr 12,009 0–5 4.96 0.129
X8FLKICRT X8 Flanker Incon RT Component (0-5) Scr 11,934 0–5 3.03 0.923
X8FLKCAC X8 Flanker Congruent Accuracy Count 12,009 0–13 12.93 0.484
X8FLKICAC X8 Flanker Incongruent Accuracy Count 12,009 0–7 6.78 0.770
X9FLANKER X9 Flanker Computed (Overall) Score 11,399 0–10 8.41 0.872
X9FLKACC X9 Flanker Accuracy Component (0-5) Scr 11,399 0–5 4.97 0.107
X9FLKICRT X9 Flanker Incon RT Component (0-5) Scr 11,355 0–5 3.45 0.830
X9FLKCAC X9 Flanker Congruent Accuracy Count 11,399 0–13 12.94 0.389
X9FLKICAC X9 Flanker Incongruent Accuracy Count 11,399 0–7 6.81 0.643
1 Because 20 accuracy points are automatically awarded due to the chosen start point for the task, it is not possible for ECLS-K:2011 children to
obtain an accuracy score of 0. Therefore, the lowest accuracy component (0-5) score in the data file is 2.5, and the lowest computed (overall)
score in the data file is also 2.5.
NOTE: Spring fourth-grade estimates (X8) are weighted by W8C8P_20. Spring fifth-grade estimates (X9) are weighted by W9C9P_20. The
unweighted sample n indicates the number of cases with valid data regardless of the presence of a valid analytic weight.
3-40SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2015 and spring 2016.
3.2.3.1 Flanker Data Flag
There are two flags to indicate the presence or absence of Flanker data. X8FLNKFLG
indicates the presence of data for the overall computed Flanker score (X8FLANKER) for the spring of
fourth grade, and X9FLNKFLG indicates the presence of data for the overall computed Flanker score
(X9FLANKER) for the spring of fifth grade. The flag values indicate whether the task was administered,
whether the overall computed Flanker score is present and, if a score is not present, the reason why it is
not present. Reasons why a score is not present when the Flanker was administered include failing the
practice trials or having an administrative breakoff (meaning the assessor ended the task) either before or
after passing the practice trials. Administrative breakoffs could have occurred for a variety of reasons
such as an external event (for example, a fire drill or the child needing to return to class) that interrupted
an assessment session.
The Flanker flags for the spring of fourth grade and the spring of fifth grade have five
possible values. A description of the values of this completion flag is presented in exhibit 3-3. The flag is
equal to system missing when the child was not a participant in the round of data collection.
Exhibit 3-3. Data flag description for the Flanker for the spring of fourth grade and spring of fifth
grade: Spring 2015 and spring 2016
X8FLNKFLG/X9FLNKFLG Value
Not Administered 0
Flanker computed (overall) score present 1
Failed Arrows practice 2
Breakoff before passing practice trials 3
Breakoff after passing practice trials 4
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study
Kindergarten Class of 2010–11 (ECLS-K:2011), spring 2015 and spring 2016.
3.3 Child Questionnaire
In the spring of third grade, the spring of fourth grade, and the spring of fifth grade, a child
questionnaire was administered to children at the beginning of the child assessment session. As discussed
in section 2.1.1, the ECLS-K:2011 child questionnaire was administered on a computer using audio
computer-assisted self-interview (audio-CASI) technology and headphones. In third grade, the child
3-41questionnaire had 37 questions and took approximately 11 minutes to complete. In fourth grade, the child
questionnaire had 35 questions and took approximately 8 minutes to complete. In fifth grade, the child
questionnaire had 48 items and took approximately 10 minutes to complete.
The fifth-grade child questionnaire included both new items and items that were selected
from the third-grade and fourth-grade child questionnaires. In the third-grade, fourth-grade, and fifth-
grade child questionnaires, children were asked about social anxiety, specifically fear of negative
evaluation by peers, and about peer victimization. The fifth-grade child questionnaire also included a
subset of items asked in third-grade about how satified the children were with aspects of the life including
questions about their parents, neighborhood, and belongings. Much of the content from the fourth-grade
questionnaire was asked again in fifth grade. As in the fourth-grade questionniare, the fifth-grade
questionnaire included questions that asked children about their behavioral engagement in school, peer
social support, feelings of loneliness at school, and media usage and family rules about media useage. The
fifth-grade child questionnaire added new questions that asked children about their feelings about schoool
belonging, grit (i.e., perseverance over the very long term in pursuit of a goal), worry about school, and
parental monitoring. The questions about school belonging were originally asked in the grade 8 student
questionnaire from ECLS-K, and questions about worry about school were selected from a larger set of
items on internalizing problem behaviors that were developed and used in grades 3, 5, and 8 in ECLS-K.
Exhibit 3-4 shows the content areas included in the third-grade, fourth-grade, and fifth-grade
child questionnaires and the corresponding item-level variables along with their sources. Variable names
for the item-level data begin with “C7” for spring third grade, “C8” for spring fourth grade, and “C9” for
spring fifth grade. Many of the items in the child questionnaire were adapted from existing scales and
were used with the permission of the author. Data for the individual items are included in the K-5 data
file, but composite variables for each construct are not provided; it is left to analysts to decide how best to
use these data in their analyses. The Early Childhood Longitudinal Study, Kindergarten Class of 2010–11
(ECLS-K:2011), Third-Grade, Fourth-Grade, and Fifth-Grade Psychometric Report (Najarian et al.
forthcoming) contains additional information on the items from the third-grade, fourth-grade, and fifth-
grade child questionnaires.
3-42Exhibit 3-4. Child questionnaire topics and item-level variables for spring third grade, spring fourth
grade, and spring fifth grade: Spring 2014, spring 2015, and spring 2016
Number
Child questionnaire topics Grade
of items Item-level variable names
Perceived Interest/Competence in Reading1 3 5 C7LKREAD, C7INTREAD, C7CTWREAD,
C7GDREAD, C7ENJREAD
Perceived Interest/Competence in Math1 3 5 C7LIKMTH, C7INTMTH, C7CTWMTH, C7GDMTH,
C7ENJMTH
Perceived Interest/Competence in Science1 3 5 C7LKSCI, C7INTSCI, C7CTWSCI, C7GDSCI,
C7ENJSCI
Perceived Interest/Competence in Peer
Relationships1
3 6 C7HASFRNDS, C7MKFRNDS, C7GETALNG,
C7EASYLIK,C7WTMEFRND, C7MORFRND
Peer Victimization2 3, 4, 5 4 C*TEASED, C*LIESABT, C*PUSHCH,
C*EXCLDCH
Social Anxiety/Fear of Negative Evaluation3 3, 4, 5 3 C*WRYTHK, C*WRYDTLK, C*AFRDNTLK
Prosocial Behavior4 3 3 C7CHEERUP, C7HLPOTH, C7NICEOTH
Life Satisfaction5 3, 5 6, 36 C7HAPHOB, C*HAPTHGS, C*HAPATTN,
C7HAPFRND, C7HAPSKIL, C*HAPNBHD
Behavioral Engagement7 4, 5 5 C*TRYHRD, C*WRKHRD, C*PARDIS, C*PAYATT,
C*LSTNCL
Peer Social Support8 4, 5 6 C*KIDBTR, C*KIDPLY, C*KIDHAP, C*KIDHLP,
C*FRIEND, C*HELPMN
Loneliness9 4, 5 3 C*LONELY, C*LFTOUT, C*ALONE
Media Usage10 4, 5 3, 511 C8OFTTXT/C9OFTTXT,
C8RULWHO/C9RULWHO,
C8RULWHN/C9RULWHN.
C9ONLINE, C9SOCLNET
4 18 C8CURPET, C8EVRPET, C8AGEPET, C8NUMPET,
C8PETDOG, C8PETCAT, C8PETRAB, C8PETBRD,
C8PETFSH, C8PETSNK, C8PETHRS, C8PETOTH,
C8HVFVPET, C8FAVPET, C8PLYPET,
C8PETHMW, C8PETSAD, C8PETFAM
School Belonging13 5 5 C9FITIN, C9CLOSCL, C9CLOSTC, C9ENJOY,
C9SAFE
Grit14 5 6 C9FINISH, C9TRYMST, C9WKGOAL,
C9WKHDQT, C9WKSETDO, C9TRYIMPRV
Worry/Stress About School15 5 5 C9WRYTST, C9HARDFIN, C9ASHAME,
C9WRYWEL, C9WRYFIN
Parental Monitoring16,17 5 3 C9KNWFREE, C9KNWHW, C9KNWGRD
Pets12 1 Adapted from the Self Description Questionnaire I (SDQI) © Herbert Marsh. SELF Research Centre (Bankstown Campus) University of
Western Sydney, Australia. Used with permission.
2 Peer victimization items were adapted from a 21-item scale by Espelage, D.L., and Holt, M. (2001). Bullying and victimization during early
adolescence: Peer influences and psychosocial correlates. Journal of Emotional Abuse, 2: 123–142.
3 Adapted from the Social Anxiety Scale for Children—Revised ©1993 Annette M. La Greca, University of Miami. Used with permission. La
Greca, A.M. and Stone, W.L. (1993). Social anxiety scale for children—revised: Factor structure and concurrent validity. Journal of Clinical
Child Psychology, 22(1): 17–27.
4 Adapted from the Children’s Social Behavior Scale—Self Report (CSBS-S). Crick, N.R., and Grotpeter, J.K. (1995). Relational aggression,
gender, and social psychological adjustment. Child Development, 66: 710–722.
5 Adapted from the NIH Toolbox for Assessment of Neurological and Behavioral Function (version 1.0): Domain-Specific Life Satisfaction
Survey from the NIH Toolbox Emotion Battery (www.NIHToolbox.org) © 2012 Northwestern University and the National Institutes of Health.
Used with permission.
6 There were six items from the Domain-Specific Life Satisfaction Scale administered in third grade, but only three of these six items were
repeated in fifth grade.
7 Adapted from Skinner, E.A., Kindermann, T.A., and Furrer, C.J. (2009). A motivational perspective on engagement and disaffection:
Conceptualization and assessment of children’s behavioral and emotional participation in academic activities in the classroom. Educational and
Psychological Measurement, 69(3): 493-525.
3-438 Adapted from Vandell, D.L. (2000). Peer Social Support, Bullying, and Victimization (Form FLV05GS: Kids in My Class at School)
[measurement instrument]. NICHD Study of Early Child Care and Youth Development: Phase III, 2000-2004.
9 Adapted from Parker, J.G., and Asher, S.R. (1993). Friendship and friendship quality in middle childhood: Links with peer group acceptance
and feelings of loneliness and social dissatisfaction. Developmental Psychology, 29(4): 611-621.
10 Adapted from the PEW September Tracking Survey 2009. Princeton Survey Research Associates International (2009). PEW September
Tracking Survey 2009. Pew Internet and American Life Project.
11 There were three items on media usage in fourth grade that asked children about frequency of online activity and family rules. These items
along with two additional items about particular types of online activities were asked in fifth grade
12 Adapted from the CENSHARE Pet Attachment Survey. Holcomb, R., Williams, R.C., and Richards, P.S. (1985). The elements of attachment:
Relationship maintenance and intimacy. Journal of the Delta Society, 2(1): 28-34.
13 Grade 8 Student Questionnaire, ECLS-K.
14 Adapted from the Character Growth Card in collaboration with Angela Duckworth for the ECLS-K:2011.
15 Adapted from the Internalizing Problems Scale that was developed for ECLS-K and used in the ECLS-K grade 3 and grade 5 child-reported
Self-Description Questionnaire and the Grade 8 Student Questionnaire.
16 Adapted from the Self-Disclosure & Parental Monitoring/Knowledge Scale (Kerr and Stattin, 2000). Kerr, M., and Stattin, H. (2000). What
parents know, how they know it, and several forms of adolescent adjustment: Further support for a reinterpretation of monitoring. Developmental
Psychology, 36: 366-380.
17 In the spring of fourth grade, parents were also asked about parental monitoring of media usage. Parents were asked if they monitor how many
hours their child spends online (P8MONTIM) and if they monitor what their child looks at online or what websites and accounts their child can
join online (P8MONCON). These questions complement questions asked of the child on the child questionnaire.
NOTE: An asterisk “*” is a placeholder for the round number in variable names. Third grade is round 7, fourth grade is round 8, and fifth grade is
round 9. For example, the variable C*TEASED is listed in the table; this indicates that the variables C7TEASED, C8TEASED, and C9TEASED
are available in the dataset.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2014, spring 2015, and spring 2016.
3.4 Teacher- and Parent-Reported Measures of Child Behavior and Peer Relationships
In the fifth-grade data collection, teachers and parents reported their perceptions of the
child’s behavior and the child’s friendships or relationships with peers. This section provides information
on teacher-reported social skills, approaches to learning behaviors, attentional focusing, inhibitory
control, peer relationships, and school liking and avoidance behaviors. This section also provides
information on parents’ perceptions of their child’s friendships and their child’s school avoidance
behaviors. This section focuses on child behaviors and relationships reported by teachers and parents in
the fifth-grade data collection. Prior-round manuals contain information on additional measures of child
behavior and relationships that were collected in earlier rounds (e.g., teachers completed the Student-
Teacher-Relationship Scale in kindergarten through third grades, and information on this scale can be
found in the User’s Manual for the ECLS-K:2011 Kindergarten–Third Grade Data File and Electronic
Codebook, Public Version [NCES 2018-034] [Tourangeau et al. 2018a]; parents and teachers reported on
child behaviors related to working memory in prior rounds, and information on these items can be found
in the User’s Manual for the ECLS-K:2011 Kindergarten–Fourth Grade Data File and Electronic
Codebook, Public Version [NCES 2018-032] [Tourangeau et al. 2018b]).
In kindergarten through third grade, the child’s classroom teacher completed a child-level
teacher questionnaire that included questions about the child’s behavior. A single classroom teacher was
asked to report for each child in these earlier grades because it is more typical for a child to have only one
teacher or to be taught by one teacher for a majority or significant portion of the day. The ECLS-K:2011
3-44made a major change in its approach to collecting the teacher questionnaire data starting in fourth grade
because it becomes increasingly more likely that students have different teachers for different subjects as
students progress through elementary school. In fourth and fifth grades, instead of having a single child-
level teacher questionnaire, there were three separate subject-specific child-level teacher questionnaires:
one for the child’s reading and language arts teacher, one for the child’s mathematics teacher, and one for
the child’s science teacher. (See chapter 2 for additional information on the structure of the teacher
questionnaires.) The reading, mathematics, and science subject-specific child-level teacher questionnaires
each contained classroom-level questions related to the content of the class but also a few child-level
questions specifically related to either the child’s reading, mathematics, or science experience and one
question related to classroom-level social and self-regulatory child behaviors in the specific class. The
reading teacher was asked to answer additional child-level questions that were not included in the
mathematics and science teacher questionnaires, many of which were asked of the classroom teacher in
prior rounds of data collection (kindergarten through third grade), including reports of the teacher’s
perceptions of the child’s behaviors. In fourth and fifth grades, the teacher identified as the child’s reading
and language arts teacher reported his or her perceptions of the child’s behavior, including social skills,
approaches to learning, attentional focusing, inhibitory control, school liking, and social interactions and
relationships in the classroom.
3.4.1 Teacher-Reported Social Skills
In the fall and spring data collections in kindergarten through second grade, and the spring
data collections in third, fourth, and fifth grade, teachers reported how often their ECLS-K:2011 students
exhibited certain social skills and behaviors using a four-option frequency scale ranging from “never” to
“very often.” Teachers also had the option of indicating that they had not had an opportunity to observe
the described behavior for the child being asked about. The items measuring children’s social skills and
behaviors are based on items from the Social Skills Rating System (Gresham and Elliott 1990)23 and were
included in the self-administered child-level teacher questionnaire in kindergarten, first grade, second
grade, and third grades and in the child-level reading and language arts teacher questionnaire in fourth
and fifth grades. The social skills battery includes some items taken verbatim from the Social Skills
Rating System, some items that are modifications of original Social Skills Rating Systems items, and some
items that measure the same kinds of skills and behaviors captured in the Social Skills Rating System but
use wording developed specifically for the ECLS studies. Sections 2.1.3 and 2.1.4 in chapter 2 have
additional information on the teacher questionnaires.
23 The Social Skills Rating System is a copyrighted instrument (1990 NCS Pearson) and has been adapted with permission. These are items
developed by Gresham and Elliott (1990).
3-45Four social skill scales were developed based on teachers’ responses to these questionnaire
items. The score on each scale is the mean rating on the items included in the scale. The four teacher
scales are as follows: Self-Control (4 items), Interpersonal Skills (5 items), Externalizing Problem
Behaviors (6 items),24 and Internalizing Problem Behaviors (4 items). A score was computed when the
respondent provided a rating on at least a minimum number of the items that composed the scale. The
minimum numbers of items that were required to compute a score were as follows: Self-Control (3 out of
4 items), Interpersonal Skills (4 out of 5 items), Externalizing Problem Behaviors (4 out of 6 items), and
Internalizing Problem Behaviors (3 out of 4 items). Higher scores indicate that the child exhibited the
behavior represented by the scale more often (e.g., higher Self-Control scores indicate that the child
exhibited behaviors indicative of self-control more often; higher Interpersonal Skills scores indicate that
the child interacted with others in a positive way more often). Variable names for the teacher scale scores,
descriptions, value ranges, weighted means, and standard deviations for these scales are shown in table
3-8.25
Data for the individual items contributing to each scale for each round of data collection are
presented in the K–5 data file for the first time. These items were not included in any prior data file due to
copyright restrictions. Permission was granted from the publisher to include them in this last file
produced for the study.
Table 3-9 presents the internal consistency reliability (Cronbach’s alpha) estimates of the
Self-Control, Interpersonal Skills, Externalizing Problem Behaviors, and Internalizing Problem Behaviors
scales derived from information reported by the teacher.
24 For children who were in first grade during the first-grade data collections (rounds 3 and 4) and for all children in subsequent rounds of data
collection (rounds 5, 6, 7, 8, and 9), the externalizing problem behaviors composite is based on 6 items. This is different from how the composite
was created for the kindergarten rounds (rounds 1 and 2). One additional item was included at the end of the “Social Skills” section of the
questionnaire in first, second, third, fourth grades. The item asked about the child’s tendency to talk at times when the child was not supposed to
be talking. The item was added because it had been included in the first-grade round of the ECLS-K and was factored into the calculation of that
study’s first-grade composite score.
25 Two versions of the teacher-level and child-level teacher questionnaires were used in the spring of first grade: one version for students who
were in first grade or higher during the data collection period and one for students who had been retained in kindergarten for the 2011–12 school
year. Details of the differences in these questionnaires are presented in chapter 2 of the ECLS-K:2011 User’s Manual for the ECLS-K:2011
Kindergarten–First Grade Data File and Electronic Codebook, Public Version (NCES 2015-078) (Tourangeau et al. 2015b).
3-46Table 3-8. Teacher-reported social skills scales variable names, descriptions, value ranges, weighted
means, and standard deviations for fall and spring kindergarten, fall and spring first grade,
fall and spring second grade, spring third grade, spring fourth grade, and spring fifth grade:
School years 2010–11, 2011–12, 2012–13; spring 2014; spring 2015; and spring 2016
Variable name Description n
Value
ranges
Weighted
mean
Standard
deviation
X1TCHCON X1 Teacher Report Self-Control 13,550 1–4 3.07 0.629
X1TCHPER X1 Teacher Report Interpersonal Skills 13,708 1–4 2.98 0.639
X1TCHEXT X1 Teacher Report Externalizing Problem Behaviors 14,385 1–4 1.61 0.631
X1TCHINT X1 Teacher Report Internalizing Problem Behaviors 14,239 1–4 1.47 0.494
X2TCHCON X2 Teacher Report Self-Control 15,796 1–4 3.17 0.637
X2TCHPER X2 Teacher Report Interpersonal Skills 15,799 1–4 3.13 0.650
X2TCHEXT X2 Teacher Report Externalizing Problem Behaviors 15,903 1–4 1.64 0.639
X2TCHINT X2 Teacher Report Internalizing Problem Behaviors 15,865 1–4 1.51 0.498
X3TCHCON X3 Teacher Report Self-Control 4,658 1–4 3.21 0.591
X3TCHPER X3 Teacher Report Interpersonal Skills 4,724 1–4 3.14 0.613
X3TCHEXT X3 Teacher Report Externalizing Problem Behaviors 4,964 1–4 1.67 0.590
X3TCHINT X3 Teacher Report Internalizing Problem Behaviors 4,848 1–4 1.48 0.483
X4TCHCON X4 Teacher Report Self-Control 13,202 1–4 3.21 0.621
X4TCHPER X4 Teacher Report Interpersonal Skills 13,288 1–4 3.14 0.657
X4TCHEXT X4 Teacher Report Externalizing Problem Behaviors 13,398 1–4 1.73 0.619
X4TCHINT X4 Teacher Report Internalizing Problem Behaviors 13,306 1–4 1.55 0.508
X4KTCHCON X4K Teacher Report Self-Control 418 1–4 3.09 0.616
X4KTCHPER X4K Teacher Report Interpersonal Skills 418 1–4 3.04 0.671
X4KTCHEXT X4K Teacher Report Externalizing Problem Behaviors 419 1–4 1.78 0.614
X4KTCHINT X4K Teacher Report Internalizing Problem Behaviors 418 1–4 1.62 0.498
X5TCHCON X5 Teacher Report Self-Control 4,174 1–4 3.23 0.614
X5TCHPER X5 Teacher Report Interpersonal Skills 4,178 1–4 3.13 0.621
X5TCHEXT X5 Teacher Report Externalizing Problem Behaviors 4,426 1–4 1.65 0.610
X5TCHINT X5 Teacher Report Internalizing Problem Behaviors 4,342 1–4 1.50 0.522
X6TCHCON X6 Teacher Report Self-Control 12,472 1–4 3.22 0.629
X6TCHPER X6 Teacher Report Interpersonal Skills 12,518 1–4 3.12 0.664
X6TCHEXT X6 Teacher Report Externalizing Problem Behaviors 12,657 1–4 1.72 0.625
X6TCHINT X6 Teacher Report Internalizing Problem Behaviors 12,577 1–4 1.59 0.528
X7TCHCON X7 Teacher Report Self-Control 11,736 1–4 3.27 0.619
X7TCHPER X7 Teacher Report Interpersonal Skills 11,768 1–4 3.14 0.657
X7TCHEXT X7 Teacher Report Externalizing Problem Behaviors 11,898 1–4 1.69 0.615
X7TCHINT X7 Teacher Report Internalizing Problem Behaviors 11,830 1–4 1.61 0.535
See notes at end of table.
3-47Table 3-8. Teacher-reported social skills scales variable names, descriptions, value ranges, weighted
means, and standard deviations for fall and spring kindergarten, fall and spring first grade,
fall and spring second grade, spring third grade, spring fourth grade, and spring fifth grade:
School years 2010–11, 2011–12, 2012–13; spring 2014; spring 2015; and spring 2016—
Continued
Variable name Description n
Value
ranges
Weighted
mean
Standard
deviation
X8TCHCON X8 Teacher Report Self-Control 10,848 1–4 3.28 0.604
X8TCHPER X8 Teacher Report Interpersonal Skills 10,867 1–4 3.12 0.648
X8TCHEXT X8 Teacher Report Externalizing Problem Behaviors 11,000 1–4 1.65 0.594
X8TCHINT X8 Teacher Report Internalizing Problem Behaviors 10,923 1–4 1.58 0.534
X9TCHCON X9 Teacher Report Self-Control 10,235 1–4 3.29 0.609
X9TCHPER X9 Teacher Report Interpersonal Skills 10,224 1–4 3.13 0.650
X9TCHEXT X9 Teacher Report Externalizing Problem Behaviors 10,359 1–4 1.63 0.590
X9TCHINT X9 Teacher Report Internalizing Problem Behaviors 10,294 1–4 1.57 0.518
NOTE Fall kindergarten estimates (X1) and spring kindergarten estimates (X2) are weighted by W1C0. Fall first-grade estimates (X3) are
weighted by W3CF3P3T0, and spring first-grade estimates (X4) are weighted by W4CS4P_2T0. Fall second-grade estimates (X5) are weighted
by W6CF6P_2A0, and spring second-grade estimates (X6) are weighted by W6CS6P_2T0. Spring third-grade estimates (X7) are weighted by
W7C27P_7T70. Spring fourth-grade estimates (X8) are weighted by W8C28P_8T80. Spring fifth-grade estimates (X9) are weighted by
W9C29P_9T90. Items contributing to the teacher-reported social skill scales were adapted with permission from the Social Skills Rating System
(©1990 NCS Pearson). Variables that begin with “X4K” are for data collected in the spring first grade data collection for children who were
retained in kindergarten. The unweighted sample n indicates the number of cases with valid data regardless of the presence of a valid analytic
weight. The respondent in kindergarten through third grades (rounds 1-7) was the child’s classroom teacher. The respondent in fourth grade
(round 8) and fifth grade (round 9) was the child’s reading and language arts teacher.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
3-48Table 3-9. Teacher-reported social skill scales reliability estimates for fall and spring kindergarten, fall
and spring first grade, and fall and spring second grade, spring third grade, spring fourth
grade, and spring fifth grade: School years 2010–11, 2011–12, 2012–13; spring 2014; spring
2015; and spring 2016
Variable name Description
Number of
items
Reliability
coefficient
X1TCHCON X1 Teacher Report Self-Control 4 .81
X1TCHPER X1 Teacher Report Interpersonal Skills 5 .86
X1TCHEXT X1 Teacher Report Externalizing Problem Behaviors 5 .88
X1TCHINT X1 Teacher Report Internalizing Problem Behaviors 4 .79
X2TCHCON X2 Teacher Report Self-Control 4 .82
X2TCHPER X2 Teacher Report Interpersonal Skills 5 .87
X2TCHEXT X2 Teacher Report Externalizing Problem Behaviors 5 .89
X2TCHINT X2 Teacher Report Internalizing Problem Behaviors 4 .78
X3TCHCON X3 Teacher Report Self-Control 4 .79
X3TCHPER X3 Teacher Report Interpersonal Skills 5 .85
X3TCHEXT X3 Teacher Report Externalizing Problem Behaviors 5 .88
X3TCHINT X3 Teacher Report Internalizing Problem Behaviors 4 .77
X4TCHCON X4 Teacher Report Self-Control 4 .81
X4TCHPER X4 Teacher Report Interpersonal Skills 5 .86
X4TCHEXT X4 Teacher Report Externalizing Problem Behaviors 5 .86
X4TCHINT X4 Teacher Report Internalizing Problem Behaviors 4 .76
X4KTCHCON X4K Teacher Report Self-Control 4 .79
X4KTCHPER X4K Teacher Report Interpersonal Skills 5 .88
X4KTCHEXT X4K Teacher Report Externalizing Problem Behaviors 5 .87
X4KTCHINT X4K Teacher Report Internalizing Problem Behaviors 4 .73
X5TCHCON X5 Teacher Report Self-Control 4 .80
X5TCHPER X5 Teacher Report Interpersonal Skills 5 .85
X5TCHEXT X5 Teacher Report Externalizing Problem Behaviors 6 .88
X5TCHINT X5 Teacher Report Internalizing Problem Behaviors 4 .78
X6TCHCON X6 Teacher Report Self-Control 4 .81
X6TCHPER X6 Teacher Report Interpersonal Skills 5 .86
X6TCHEXT X6 Teacher Report Externalizing Problem Behaviors 6 .87
X6TCHINT X6 Teacher Report Internalizing Problem Behaviors 4 .78
X7TCHCON X7 Teacher Report Self-Control 4 .80
X7TCHPER X7 Teacher Report Interpersonal Skills 5 .86
X7TCHEXT X7 Teacher Report Externalizing Problem Behaviors 6 .87
X7TCHINT X7 Teacher Report Internalizing Problem Behaviors 4 .78
X8TCHCON X8 Teacher Report Self-Control 4 .80
X8TCHPER X8 Teacher Report Interpersonal Skills 5 .86
X8TCHEXT X8 Teacher Report Externalizing Problem Behaviors 6 .87
X8TCHINT X8 Teacher Report Internalizing Problem Behaviors 4 .79
X9TCHCON X9 Teacher Report Self-Control 4 .80
X9TCHPER X9 Teacher Report Interpersonal Skills 5 .86
X9TCHEXT X9 Teacher Report Externalizing Problem Behaviors 6 .88
X9TCHINT X9 Teacher Report Internalizing Problem Behaviors 4 .79
NOTE: Items contributing to the teacher-reported social skill scales were adapted with permission from the Social Skills Rating System (SSRS)
(©1990 NCS Pearson). The respondent in kindergarten through third grades (rounds 1-7) was the child’s classroom teacher. The respondent in
fourth grade (round 8) and fifth grade (round 9) was the child’s reading and language arts teacher.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, spring 2015,and spring 2016.
3-493.4.2 Teacher-Reported Approaches to Learning Items and Scale
The child-level teacher questionnaire fielded in every round of data collection from the fall
of kindergarten to the spring of third grade and the child-level reading and language arts teacher subject-
specific child-level teacher questionnaire in fourth and fifth grades included seven items, referred to as
“Approaches to Learning” items, that asked the teachers to report how often their ECLS-K:2011 students
exhibited a selected set of learning behaviors (keeps belongings organized; shows eagerness to learn new
things; works independently; easily adapts to changes in routine; persists in completing tasks; pays
attention well; and follows classroom rules).26 These items were presented in the same item set as the
social skills items adapted from the Social Skills Rating System (described above in section 3.4.1), and
teachers used the same frequency scale to report how often each child demonstrated the behaviors
described. The Approaches to Learning scale score is the mean rating on the seven items included in the
scale. A score was computed when the respondent provided a rating on at least 4 of the 7 items that
composed the scale. Higher scale scores indicate that the child exhibited positive learning behaviors more
often. The item-level data for the teacher-reported Approaches to Learning items are included in the data
file along with the other child-level teacher questionnaire data. Variable names for the item-level data
from the fall and spring kindergarten child-level teacher questionnaire begin with “T1” and “T2,”
respectively. Variable names for the item-level data from the fall first-grade child-level teacher
questionnaire begin with “T3.” Those for the item-level data from the spring first-grade child-level
teacher questionnaire for children in first grade begin with “T4,” while those for children held back in
kindergarten begin with “T4K.” Variable names for the fall of second grade begin with “T5,” and those
for the spring of second grade begin with “T6.” Variable names for the spring of third grade begin with
“T7,” and those for spring of fourth grade begin with “G8.” Variable names for the spring of fifth grade
begin with “G9.” The variable names, descriptions, value ranges, weighted means, and standard
deviations for the teacher-reported Approaches to Learning scale scores are shown in table 3-10. The
Approaches to Learning scale has a reliability estimate of .91 for each round of data collection from
kindergarten through fourth grade and .92 for fifth grade, as measured by Cronbach’s alpha.
26 The Approaches to Learning teacher items were developed specifically for the ECLS-K; they were not taken from an existing source. These
items were fielded as part of what was called the Teacher Social Rating Scale in the ECLS-K. The first six items (i.e., keeps belongings
organized; shows eagerness to learn new things; works independently; easily adapts to changes in routine; persists in completing tasks; pays
attention well) were included in the Teacher Social Rating Scale used in the kindergarten rounds of the ECLS-K. The seventh item (i.e., follows
classroom rules) was added in the first-grade round of the ECLS-K.
3-50Table 3-10. Teacher-reported Approaches to Learning scale variable names, descriptions, value ranges,
weighted means, and standard deviations for fall and spring kindergarten, fall and spring
first grade, fall and spring second grade, spring third grade, spring fourth grade, and spring
fifth grade: School years 2010–11, 2011–12, 2012–13; spring 2014; spring 2015; and spring
2016
Variable name Description n
Value
ranges
Weighted
mean
Standard
deviation
X1TCHAPP X1 Teacher Report Approaches to Learning 14,770 1–4 2.93 0.680
X2TCHAPP X2 Teacher Report Approaches to Learning 15,978 1–4 3.09 0.689
X3TCHAPP X3 Teacher Report Approaches to Learning 5,022 1–4 3.04 0.677
X4TCHAPP X4 Teacher Report Approaches to Learning 13,449 1–4 3.07 0.700
X4KTCHAPP X4K Teacher Report Approaches to Learning 417 1–4 2.94 0.704
X5TCHAPP X5 Teacher Report Approaches to Learning 4,507 1–4 3.05 0.688
X6TCHAPP X6 Teacher Report Approaches to Learning 12,689 1–4 3.07 0.707
X7TCHAPP X7 Teacher Report Approaches to Learning 11,913 1–4 3.08 0.711
X8TCHAPP X8 Teacher Report Approaches to Learning 11,028 1–4 3.09 0.696
X9TCHAPP X9 Teacher Report Approaches to Learning 10,403 1–4 3.11 0.696
NOTE: Fall kindergarten estimates (X1) and spring kindergarten estimates (X2) are weighted by W1C0. Fall first-grade estimates (X3) are
weighted by W3CF3P3T0, and spring first-grade estimates (X4) are weighted by W4CS4P_2T0. Fall second-grade estimates (X5) are weighted
by W6CF6P_2A0, and spring second-grade estimates (X6) are weighted by W6CS6P_2T0. Spring third-grade estimates (X7) are weighted by
W7C27P_7T70. Spring fourth-grade estimates (X8) are weighted by W8C28P_8T80. Spring fifth-grade estimates (X9) are weighted by
W9C29P_9T90. Variables that begin with “X4K” are for data collected in the spring first grade data collection for children who were retained in
kindergarten. The unweighted sample n indicates the number of cases with valid data regardless of the presence of a valid analytic weight. The
respondent in kindergarten through third grades (rounds 1-7) was the child’s classroom teacher. The respondent in fourth grade (round 8) and
fifth grade (round 9) was the child’s reading and language arts teacher.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
3.4.3 Teacher-Reported Attentional Focusing and Inhibitory Control: Children’s Behavior
Questionnaire (CBQ) and Temperament in Middle Childhood Questionnaire (TMCQ)
The fall kindergarten, spring kindergarten, and spring first-grade child-level teacher
questionnaires (both the version for students in first grade and the version for students in kindergarten)
included 12 items from the Short Form of the Children’s Behavior Questionnaire (CBQ) (Putnam and
Rothbart 2006)27 asking teachers to indicate how often their ECLS-K:2011 children exhibited certain
social skills and behaviors related to inhibitory control and attentional focusing, two indicators related to
executive functioning. Rothbart describes inhibitory control as the “capacity to plan and to suppress
inappropriate approach responses under instructions or in novel or uncertain situations” (Rothbart et al.
2001, p. 1406). Teachers were presented with statements about how the children might have reacted to a
number of situations in the past 6 months and were asked to indicate how “true” or “untrue” those
statements were about that child on a 7-point scale ranging from “extremely untrue” to “extremely true,”
27 The Children’s Behavior Questionnaire is a copyrighted instrument: Putnam, S.P., and Rothbart, M.K. (2006). Development of Short and
Very Short Forms of the Children’s Behavior Questionnaire. Journal of Personality Assessment, 87(1): 103-113. Used with permission.
3-51with a middle option of “neither true nor untrue.” If a statement or situation did not apply to that child, the
teacher could indicate “not applicable.”
The CBQ is appropriate for assessment of children ages 3 through 7 years, so it could not be
used past the first-grade rounds of data collection. To remain age appropriate, the CBQ was replaced with
the Temperament in Middle Childhood Questionnaire (TMCQ) (Simonds and Rothbart 2004)28 in the
spring of second grade. The TMCQ was designed as an upward age-extension of the CBQ and is
appropriate for children ages 7 through 10 years. While many of the items from the TMCQ are different
from the items on the CBQ, the items are believed to assess the same or similar constructs in an age-
appropriate way. Teachers received the same instructions for the CBQ and TMCQ items, although the
TMCQ items were rated on a 5-point scale instead of the 7-point scale used for the CBQ items. For the
TMCQ items, teachers used a 5-point scale ranging from “almost always untrue” to “almost always true,”
with a middle option of “sometimes true, sometimes untrue.” Like the CBQ, there was a “not applicable”
option that the teacher could select if the statement or situation did not apply to the child.
Item-level data for the items that make up the Attentional Focusing and Inhibitory Control
scales are provided on the kindergarten-fifth grade data file. Variable names for the item-level data from
the fall and spring kindergarten child-level teacher questionnaire begin with “T1” and “T2,” respectively.
Variable names for the item-level data from the spring first-grade child-level teacher questionnaire for
children in first grade begin with “T4,” while variable names for children held back in kindergarten
during spring 2012 begin with “T4K.” Variable names for the spring second grade begin with “T6,” and
those for spring third grade begin with “T7.” Variable names from the reading subject-specific child-level
questionnaire begin with “G8” in fourth grade and “G9” in fifth grade.
The data file includes two scale scores for each round of data collection in which each
measure was included: (1) Attentional Focus and (2) Inhibitory Control. In kindergarten and first grade
these scores are derived from the CBQ, and in second, third, fourth, and fifth grade these scores are
derived primarily from the TMCQ, as explained further below. The scale scores were developed using
guidelines from the developers of both the CBQ and TMCQ.
In kindergarten and first grade, the ECLS-K:2011 fielded all 6 items from the Attentional
Focusing subscale and all 6 items from the Inhibitory Control subscale of the CBQ Short Form. As such,
the kindergarten and first-grade Attentional Focus and Inhibitory Control scores are each based on all 6
items in the relevant Short Form subscale. Because the CBQ was initially designed as a parent-report
28 The Temperament in Middle Childhood Questionnaire is a copyrighted instrument: Adapted from the Temperament in Middle Childhood
Questionnaire. © 2004 Jennifer Simonds and Mary K. Rothbart, University of Oregon. Used with permission.
3-52measure, the item wording for 3 of the items from the CBQ Inhibitory Control subscale was modified
slightly for use in the ECLS-K:2011 to make the items more appropriate for a school setting.
In second, third, fourth and fifth grade, the ECLS-K:2011 fielded 6 of the 7 items from the
original TMCQ Attentional Focusing subscale. For the inhibitory control dimension, the ECLS-K:2011
fielded 6 of the 8 items from the TMCQ Inhibitory Control subscale and one item from the CBQ
Inhibitory Control subscale. Therefore, the second-, third-, fourth-, and fifth-grade Attentional Focusing
scale scores reflect the 6 items fielded by the ECLS-K:2011, not the full set of items in the original
TMCQ scale. The second-, third-, fourth-, and fifth-grade Inhibitory Control scale scores reflect the 7
items fielded by the ECLS-K:2011 (6 from the TMCQ and one from the CBQ), again not the full set of
items in the original TMCQ scale. Because the TMCQ was designed as a parent-report measure, the item
wording on one item from the TMCQ Attentional Focusing subscale was modified slightly to make it
more appropriate for a school setting and, similarly, one item on the TMCQ Inhibitory Control subscale
was modified.
For the kindergarten, first-grade, second-grade, third-grade, fourth-grade, and fifth-grade
Attentional Focusing and Inhibitory Control scales, the score on each scale is the mean rating on the items
included in the scale. A score was computed when the respondent provided a rating on at least 4 of the 6
or 7 items that made up the scale. Higher scale scores on the Attentional Focus scale indicate that the
child exhibited more behaviors that demonstrate the ability to focus attention on cues in the environment
that are relevant to the task. Higher scale scores on the Inhibitory Control scale indicate that the child
exhibited more behaviors that demonstrate the ability to hold back or suppress a behavior as necessary for
a particular situation. The variable names, descriptions, value ranges, weighted means, and standard
deviations for these scales are shown in tables 3-11 and 3-12.
3-53Table 3-11. Children’s Behavior Questionnaire variable names, descriptions, value ranges, weighted
means, and standard deviations for fall and spring kindergarten and spring first grade:
School year 2010–11 and spring 2012
Variable name Description n
Value
ranges
Weighted
mean
Standard
deviation
X1ATTNFS X1 Teacher Report Attentional Focus 14,562 1–7 4.68 1.323
X1INBCNT X1 Teacher Report Inhibitory Control 14,556 1–7 4.88 1.291
X2ATTNFS X2 Teacher Report Attentional Focus 15,937 1–7 4.90 1.329
X2INBCNT X2 Teacher Report Inhibitory Control 15,925 1–7 5.06 1.292
X4ATTNFS X4 Teacher Report Attentional Focus 13,390 1–7 4.84 1.292
X4INBCNT X4 Teacher Report Inhibitory Control 13,399 1–7 5.04 1.287
X4KATTNFS X4K Teacher Report Attentional Focus 417 1–7 4.61 1.323
X4KINBCNT X4K Teacher Report Inhibitory Control 417 1–7 4.88 1.267
NOTE: Fall kindergarten estimates (X1) and spring kindergarten estimates (X2) are weighted by W1C0. Spring first-grade estimates (X4) are
weighted by W4CS4P_2T0. Items contributing to these scales come from the Children’s Behavior Questionnaire (Putnam and Rothbart 2006).
Variables that begin with “X4K” are for data collected in the spring first grade data collection for children who were retained in kindergarten. The
unweighted sample n indicates the number of cases with valid data regardless of the presence of a valid analytic weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, and spring 2012.
Table 3-12. Temperament in Middle Childhood Questionnaire variable names, descriptions, value
ranges, weighted means, and standard deviations for spring second grade, spring third
grade, spring fourth grade, and spring fifth grade: Spring 2013, spring 2014, spring 2015,
and spring 2016
Variable
name Description n
Value
ranges
Weighted
mean
Standard
deviation
X6ATTMCQ X6 TMCQ Teacher Report Attentional Focus 12,661 1–5 3.47 1.122
X6INTMCQ X6 TMCQ Teacher Report Inhibitory Control 12,659 1–5 3.67 0.845
X7ATTMCQ X7 TMCQ Teacher Report Attentional Focus 11,879 1–5 3.48 1.119
X7INTMCQ X7 TMCQ Teacher Report Inhibitory Control 11,882 1–5 3.69 0.825
X8ATTMCQ X8 TMCQ Teacher Report Attentional Focus 11,008 1–5 3.54 1.112
X8INTMCQ X8 TMCQ Teacher Report Inhibitory Control 11,002 1–5 3.73 0.812
X9ATTMCQ X9 TMCQ Teacher Report Attentional Focus 10,367 1–5 3.61 1.083
X9INTMCQ X9 TMCQ Teacher Report Inhibitory Control 10,355 1–5 3.80 0.802
NOTE: Spring second-grade estimates (X6) are weighted by W6CS6P_2T0. Spring third-grade estimates (X7) are weighted by W7C27P_7T70.
Spring fourth-grade estimates (X8) are weighted by W8C28P_8T80. Spring fifth-grade estimates (X9) are weighted by W9C29P_9T90. Items
contributing to these scales come from the Children’s Behavior Questionnaire (Putnam and Rothbart 2006) and the Temperament in Middle
Childhood Questionnaire (Simonds and Rothbart 2004).The unweighted sample n indicates the number of cases with valid data regardless of the
presence of a valid analytic weight. The respondent in kindergarten through third grades (rounds 1-7) was the child’s classroom teacher. The
respondent in fourth grade (round 8) and in fifth grade (round 9) was the child’s reading and language arts teacher.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2013, spring 2014, spring 2015, and spring 2016.
3-54Table 3-13 presents the internal consistency reliability coefficients (Cronbach’s alpha) for
the teacher-reported Attentional Focus and Inhibitory Control scales for kindergarten through fifth grade.
The Attentional Focus scale for the fall and spring kindergarten data collections (X1ATTNFS,
X2ATTNFS) has an internal consistency reliability coefficient of .87, and the Inhibitory Control scale for
the fall and spring kindergarten data collections (X1INBCNT, X2INBCNT) has a reliability estimate of
.87. For the spring of first grade, the Attentional Focus scale (X4ATTNFS) has an internal consistency
reliability coefficient of .83 for children in first grade and .86 for children retained in kindergarten, and
the Inhibitory Control scale (X4INBCNT) has an internal consistency reliability coefficient of .86 for
both children in first grade and those retained in kindergarten. For the spring of second grade, the
Attentional Focus scale (X6ATTMCQ29) has an internal consistency reliability coefficient of .96, and the
Inhibitory Control scale (X6INTMCQ30) has an internal consistency reliability coefficient of .87. For the
spring of third grade, the Attentional Focus scale (X7ATTMCQ) has an internal consistency reliability
coefficient of .96, and the Inhibitory Control scale (X7INTMCQ) has an internal consistency reliability
coefficient of .85. In the spring of fourth grade, the internal consistency reliability coefficient is .96 for
the Attentional Focus scale (X8ATTMCQ) and .85, for the Inhibitory Control scale (X8INTMCQ). In the
spring of fifth grade, the internal consistency reliability coefficient is .96 for the Attentional Focus scale
(X9ATTMCQ) and .85 for the Inhibitory Control scale (X9INTMCQ).
The study received copyright permission to include item-level data from both the CBQ and
the TMCQ in the ECLS-K:2011 data files. Therefore, these data have been included in the kindergarten
through fifth-grade data file with the other child-level teacher questionnaire data. Variable names for the
item-level data from the fall of kindergarten, the spring of kindergarten, the spring of first grade, the
spring of second grade, and the spring of third grade begin with “T1,” “T2,” “T4,” “T6,” and “T7,”
respectively. Variable names from the item-level data begin with “G8” for the spring of fourth grade and
“G9” for the spring of fifth grade. Variable names that begin with “T4K” are for item-level data from the
spring of first grade for students retained in kindergarten in spring 2012.
29 The variable name for the Attentional Focus composite was changed from X*ATTNFS to X*ATTMCQ starting in second grade. Although the
construct is believed to be the same, the items used to derive the composite were from the CBQ for kindergarten and first grade but were from the
TMCQ starting at second grade. Thus, the name of the composite variable was changed.
30 The variable name for the Inhibitory Control composite was changed from X*INBCNT to X*INTMCQ starting in second grade. Although the
construct is believed to be the same, the items used to derive the composite were from the CBQ for kindergarten and first grade but were from the
TMCQ starting at second grade. Thus, the name of the composite variable was changed.
3-55Table 3-13. Reliability estimates for the teacher-reported Attentional Focus and Inhibitory Control
scales for fall and spring kindergarten, spring first grade, spring second grade, spring third
grade, spring fourth grade, and spring fifth grade: School year 2010–11, spring 2012, spring
2013, spring 2014, spring 2015, and spring 2016
Variable name Description
Number
of items
Reliability
coefficient
X1ATTNFS X1 Teacher Report Attentional Focus 6 .87
X1INBCNT X1 Teacher Report Inhibitory Control 6 .87
X2ATTNFS X2 Teacher Report Attentional Focus 6 .87
X2INBCNT X2 Teacher Report Inhibitory Control 6 .87
X4ATTNFS X4 Teacher Report Attentional Focus 6 .83
X4INBCNT X4 Teacher Report Inhibitory Control 6 .86
X4KATTNFS X4 Teacher Report Attentional Focus 6 .86
X4KINBCNT X4 Teacher Report Inhibitory Control 6 .86
X6ATTMCQ X6 TMCQ Teacher Report Attentional Focus 6 .96
X6INTMCQ X6 TMCQ Teacher Report Inhibitory Control 7 .87
X7ATTMCQ X7 TMCQ Teacher Report Attentional Focus 6 .96
X7INTMCQ X7 TMCQ Teacher Report Inhibitory Control 7 .85
X8ATTMCQ X8 TMCQ Teacher Report Attentional Focus 6 .96
X8INTMCQ X8 TMCQ Teacher Report Inhibitory Control 7 .85
X9ATTMCQ X9 TMCQ Teacher Report Attentional Focus 6 .96
X9INTMCQ X9 TMCQ Teacher Report Inhibitory Control 7 .85
NOTE: Items contributing to these scales come from the Children’s Behavior Questionnaire (Putnam and Rothbart 2006) and the Temperament
in Middle Childhood Questionnaire (Simonds and Rothbart 2004). The respondent in kindergarten through third grades (Rounds 1-7) was the
child’s classroom teacher. The respondent in fourth grade (round 8) and in fifth grade (round 9) was the child’s reading and language arts teacher.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010, spring 2011, spring 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
3.4.4 Teacher- and Parent-Reports of Children’s Peer Relationships
Teachers reported their perceptions of the child’s peer relationships in the child-level teacher
questionnaire in spring of second grade and spring of third grade and in the reading subject-specific child-
level teacher questionnaire in spring of fourth grade and spring of fifth grade. Parents reported their
perceptions of the child’s peer relationships in the parent interview.
Exhibit 3-5 shows the constructs on peer relationships included in the second-, third-,
fourth-, and fifth-grade child-level teacher questionnaires and the corresponding item-level variables
along with their sources. In second, third, fourth, and fifth grade, teachers provided information on peer
victimization, both with the child as the victim and with the child as the aggressor. In the spring of third
grade, spring of fourth grade, and spring of fifth grade, teachers were asked about whether the child was
excluded or ignored by peers and about whether the child exhibited prosocial behaviors with peers. In the
spring of fourth grade and the spring of fifth grade, teachers were asked about the behaviors of the peers
3-56in the child’s peer group and about the child’s social skills with peers. These items were adapted from
existing scales and were used with the permission of the authors. Data for the individual items are
included in the K-5 data file. Variable names for the item-level data from the child-level teacher
questionnaire in the spring of second grade and the spring of third grade begin with “T6” and “T7,”
respectively. Variable names from the item-level data from the reading subject-specific child-level
teacher questionnaire for the spring of fourth grade and the spring of fifth grade begin with “G8,” and
“G9,” respectively. Composite variables for each construct are not provided; it is left to analysts to decide
how best to use these data in their analyses.
3-57Exhibit 3-5. Teacher-reported item-level variables on peer relationships in spring second grade, spring
third grade, spring fourth grade, and spring fifth grade: Spring 2013, spring 2014, spring
2015, and spring 2016
Construct/scale
Grade
administered
Number
of items Item-level variable names
Peer Victimization
(child as victim)1
2-5 4 T6OSTEAS/T7OSTEAS/G8OSTEAS/G9OSTEAS;
T6OSLIES/T7OSLIES/G8OSLIES/G9OSLIES;
T6OSPUSH/T7OSPUSH/G8OSPUSH/G9OSPUSH;
T6OSLFTO/T7OSLFTO/G8OSLFTO/G9OSLFTO
Peer Victimization
(child as aggressor)1
2-5 4 T6TSTEAS/T7TSTEAS/G8TSTEAS/G9TSTEAS;
T6TSLIES/T7TSLIES/G8TSLIES/G9TSLIES;
T6TSPUSH/T7TSPUSH/G8TSPUSH/G9TSPUSH;
T6TSLFTO/T7TSLFTO/G8TSLFTO/G9TSLFTO
Excluded by Peers2 3-5 4 T7PLYMTE/G8PLYMTE/G9PLYMTE;
T7PAVOID/G8PAVOID/G9PAVOID;
T7EXLUED/G8EXLUED/G9EXLUED;
T7IGNRED/G8IGNRED/G9IGNRED
Prosocial with Peers2 3-5 5 T7OTDIST/G8OTDIST/G9OTDIST;
T7ISKIND/G8ISKIND/G9ISKIND;
T7COPRTV/G8COPRTV/G9COPRTV;
T7CNMORL/G8CNMORL/G9CNMORL;
T7HLPUPS/G8HLPUPS/G9HLPUPS
Positive Peer Group3 4-5 9 G8GOODGP/G9GOODGP;
G8WORYGP/G9WORYGP;
G8BADINF/G9BADINF;
G8SUPVIS/G9SUPVIS;
G8TRBLGP/G9TRBLGP;
G8EXCSTU/G9EXCSTU;
G8HRDWKR/G9HRDWKR;
G8FUNGRP/G9FUNGRP;
G8KINDGP/G9KINDGP
Social Skills with Peers4 4-5 4 G8UNDFEL/G9UNDFEL;
G8INTPER/G9INTPER,
G8SOLINT/G9SOLINT,
G8EFFBEV/G9EFFBEV
1 Peer victimization items were adapted from a 21-item scale by Espelage, D.L. and Holt, M. (2001). Bullying and victimization during early
adolescence: Peer influences and psychosocial correlates. Journal of Emotional Abuse, 2: 123–142.
2 Adapted from the Child Behavior Scale © Gary W. Ladd. Used with permission. A subset of items from the Excluded by Peers and Prosocial
with Peers scales from the Child Behavior Scale were adapted and used in the spring of third grade.
3 Adapted from Vandell, D L. (2001). Relationships With Peers: Part D (Teacher). Unpublished scale, NICHD Study of Early Child Care and
Youth Development, Form FSV10G3. These items reflect positive and negative peer group characteristics. The NICHD Study of Early Child
Care and Youth Development decided to form one composite for “Positive Peer Group” with these items, reverse coding 4 of the 9 items when
creating a composite.
4 Adapted from Pierce, K.M., Hamm, J.V., and Vandell, D.L. (1999). Experiences in after-school programs and children’s adjustment in first-
grade classrooms. Child Development, 70: 756-767. These items include 4 of 7 items from the “Mock Report Card” (e.g., Form FSV08G3) used
in the NICHD Study of Early Child Care and Youth Development and were originally adapted from Coie and Dodge (1988).
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2013, spring 2014, spring 2015, and spring 2016.
3-58There are questions in the parent interview that complement the teacher-reported information
on peer relationships. Exhibit 3-6 shows the constructs on peer relationships included in the second-
,
third-, fourth-, and fifth-grade parent interviews and the corresponding item-level variables along with
their sources. The teacher- and parent-provided information complements information collected from
children on peer victimization, which is described above in section 3.3, however children were asked only
about their experiences as a victim, not as the aggressor. In fourth and fifth grade, parents were asked how
many close friends the child had and about the influence of the child’s best friend.
Exhibit 3-6. Parent-reported item-level variables on peer relationships and friendships in spring second
grade, spring third grade, spring fourth grade, and spring fifth grade: Spring 2013, spring
2014, spring 2015, and spring 2016
Construct/scale Number of items (grade) Response categories Item-level variable names
Peer Victimization 1 3 (second grade) Yes, No P*OTHTEA
(child as victim) 4 (third grade) P7OTHLIE2
P*OTHHIT
P*OTHEXC
Peer Victimization 1
(child as victim)
3 (second grade)
4 (third grade)
Rarely, Sometimes, Often,
Very Often
P*OFTTEA
P7OFTLIE2
P*OFTHIT
P*OFTEXC
Number of Close Friends 1 (fourth/fifth grade) Number P*NUMFRD
Influence of Best Friend 1 (fourth/fifth grade) Always a good influence,
P*FRINFL
Usually a good influence,
Neither a good nor a bad
influence, Usually a bad
influence, Always a bad
influence
1 Peer victimization items were adapted from a 21-item scale by Espelage, D.L., and Holt, M. (2001). Bullying and victimization during early
adolescence: Peer influences and psychosocial correlates. Journal of Emotional Abuse, 2: 123–142.
2 In second grade, parents were not asked about whether other children told lies or untrue stories about their child. An item was added in third
grade so that parents, teachers, and children were asked about the same forms of peer victimization.
NOTE: An asterisk “*” is a placeholder for round number in variable names. Third grade is round 7, fourth grade is round 8, and fifth grade is
round 9. For example, the variable P*OTHTEA is listed in the table; this indicates that the variables P7OTHTEA, P8OTHTEA, and P9OTHTEA
are available in the dataset.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2013, spring 2014, spring 2015, and spring 2016.
3-593.4.5 Teacher- and Parent-Reports of Children’s School Liking and Avoidance
In the spring of fourth grade and the spring of fifth grade, teachers and parents reported their
perceptions of the child’s school liking and avoidance behaviors using items adapted from the parent and
teacher versions of the School Liking and Avoidance Questionnaire (SLAQ) (Ladd and Price 1987; Ladd
1990). Teachers rated perceptions of school liking with seven items, four positively worded items (e.g.,
“Likes to come to school”) and three negatively worded items (e.g., “Dislikes school”), on a 3-point
Likert-type scale to indicate whether the item “doesn’t apply,” “applies sometimes,” or “certainly
applies.” Ladd used these seven items to create a single teacher-reported school liking construct by
combining these seven items (reverse scoring the negatively worded items). Parents rated five items about
the parent’s perception of school avoidance behaviors on a 5-point Likert-type scale, using response items
similar to the SLAQ (almost never, rarely, sometimes, a lot, almost always). Ladd used these five items to
create a single parent-reported school avoidance scale (exhibit 3-7). Composite variables for these teacher
and parent constructs are not provided; it is left to analysts to decide how best to use these data in their
analyses.
Exhibit 3-7. Teacher- and parent-reported item-level variables on school liking and avoidance in spring
fourth grade and spring fifth grade: Spring 2015 and spring 2016
Construct/scale
Grade
administered
Number
of items Item-level variable names
Teacher-reported School Liking1 4, 5 7 G*LIKSCH, G*DISLSH,
G*FUNSCH, G*LBESCH,
G*UNHAPY, G*ENJACT,
G*GRNACT
Parent-reported School Avoidance1 4, 5 5 P*MKREAS, P*CDREAD,
P*CUPSET, P*STAYHM,
P*CMPLNS
1 Adapted from the parent and teacher versions of the School Liking and Avoidance Questionnaire (SLAQ; Adapted from Ladd and Price, 1987;
Ladd, 1990)
NOTE: An asterisk “*” is a placeholder for round number in variable names. Fourth grade is round 8, and fifth grade is round 9. For example, the
variable G*LIKSCH is listed in the table; this indicates that the variables G8LIKSCH and G9LIKSCH are available in the dataset.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2015 and spring 2016.
3-604. SAMPLE DESIGN AND SAMPLING WEIGHTS
The Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011)
provides national data on children’s characteristics as they progressed from kindergarten through the
2015–16 school year, when most of the children were in fifth grade. In the 2010–11 school year, the
ECLS-K:2011 collected data from a nationally representative sample of 18,174 children enrolled in 968
schools.1 This chapter summarizes the process used to select the sample for the study in the base year
(i.e., kindergarten), describes how the sample design changed for the first- through fifth-grade years, and
provides information necessary to properly analyze the data that were collected.
4.1 Sample Design
The optimal sample design for collecting data to produce national child-level estimates is to
sample children with probabilities that are approximately the same for each child. In most studies, this is
achieved using a multistage sampling design that involves sampling primary sampling units (PSUs) and
schools with probabilities proportional to the targeted number of children attending the school and
selecting a fixed number of children per school. Such a sampling procedure was used for the
ECLS-K:2011. Additionally, a clustered design was used to minimize data collection costs, which are
strongly related to the dispersion of the children in the sample. Restricting data collection to a limited
number of geographic areas and to as few schools as possible helps to minimize costs while still
achieving an acceptable level of precision in the estimates produced with the data.
The sample for the ECLS-K:2011 was selected using a three-stage process. In the first stage
of sampling, the country was divided into PSUs, or geographic areas that are counties or groups of
contiguous counties, and 90 PSUs were sampled for inclusion in the study. In the second stage, samples
of public and private schools with kindergarten programs or that educated children of kindergarten age
(i.e., 5-year-old children) in ungraded settings were selected within the sampled PSUs. Both PSUs and
schools were selected with probability proportional to measures of size (defined as the population of 5-
year-old children) that took into account a desired oversampling of Asians, Native Hawaiians, and Other
Pacific Islanders (APIs).2 In the third stage of sampling, children enrolled in kindergarten and 5-year-old
1 This is the number of schools with at least one child or parent respondent at the end of the spring data collection; this number includes originally
sampled schools and substitute schools. Children who transferred from the school in which they were originally sampled during the kindergarten
year were retained in the study and followed into their new school; this number does not include schools to which study children transferred
during the kindergarten year.
2 API children were oversampled as one group, not as three groups that were distinct from one another.
4-1children in ungraded schools or classrooms were selected within each sampled school. For a detailed
description of the three stages of sampling, see chapter 4 of the Early Childhood Longitudinal Study,
Kindergarten Class of 2010–11 (ECLS-K:2011), User’s Manual for the ECLS-K:2011 Kindergarten Data
File and Electronic Codebook, Public Version (NCES 2015-074) (Tourangeau et al. 2015a), hereinafter
referred to as the base-year User’s Manual.
4.1.1 ECLS-K:2011 School Sample
A total of 1,221 clusters of schools3 were originally selected for the ECLS-K:2011, of which
1,003 were clusters of public schools and 218 were clusters of private schools. This resulted in 1,036
sampled public schools and 283 sampled private schools, for a total of 1,319 sampled schools.
The sample frames used to select schools were the 2006–07 Common Core of Data (CCD)
and the 2007–08 Private School Survey (PSS), which were the most recent CCD and PSS data available
at the time of sampling. Because the 2006–07 CCD and the 2007–08 PSS school frames were several
years old, additional schools were sampled from supplemental frames that included newly opened schools
and existing schools that added a kindergarten program after the 2006–07 CCD and the 2007–08 PSS data
were collected. These additional schools were added to the original school sample. In total, 33 new
schools were added, of which 16 were public, 4 were Catholic, and 13 were non-Catholic private schools.
The total number of sampled schools after updating was 1,352 (1,052 public schools and 300 private
schools). For a detailed discussion of the supplemental school sample, see section 4.1.2.7 of the base-year
User’s Manual.
Early in the process of recruiting schools that had been sampled for the study, it was
determined that the rate at which public schools were agreeing to participate was lower than expected and
it would be difficult to meet the target number of participating schools by the end of the recruitment
period. The decision was made to select public schools not selected into the original ECLS-K:2011
sample that would replace those sampled public schools that had already refused to participate. For a
detailed discussion of school substitution, see section 4.1.2.8 of the base-year User’s Manual. The
characteristics of the school sample are presented in table 4-1. This table includes characteristics for
sampled schools after substitution, which makes it different from table 4-2 in the base-year User’s
Manual, which shows characteristics for the originally sampled schools before substitution.
3 Public schools with fewer than 23 children and private schools with fewer than 12 children were clustered together for sampling. Thus, clusters
of schools were sampled, each cluster comprising one or more schools. For a discussion of school clustering, see section 4.1.2.3 of the base-year
User’s Manual.
4-2Table 4-1. The ECLS-K:2011 school sample after school substitution
Characteristic Total Public Private
Total 1,352 1,052 300
Census region1,2
Northeast
Midwest
South
West
Locale
City
Suburb
Town
Rural
Kindergarten enrollment
Fewer than 25
25–49
50–99
100–149
150–199
200–249
250–299
300 or more
Religious affiliation
Catholic
Other religious
Nonreligious, private
240
280
480
350
421
522
113
296
252
197
490
267
91
24
7
24
74
136
90
Percent of students eligible for the free lunch program
0–25 percent
26–50 percent
51–75 percent
Greater than 75 percent
472
267
188
125
Other school types
Bureau of Indian Affairs school
Ungraded school
3
177
170
220
390
270
314
400
91
247
75
119
451
264
89
23
7
24
†
†
†
472
267
188
125
3
168
70
60
90
80
107
122
22
49
177
78
39
3
2
1
0
0
74
136
90
†
†
†
†
†
9
† Not applicable.
1 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these
characteristics cannot be replicated with variables on the released data file.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010 and spring 2011.
4-34.1.2 The Sample of Children
The goal of the sample design was to obtain an approximately self-weighting sample of
children, with the exception of APIs who needed to be oversampled to meet sample size goals. Table 4-2
shows the distribution of the eligible children sampled for the ECLS-K:2011, by selected characteristics.
Table 4-3 shows the distribution of the children who were respondents in the base year, by selected
characteristics. To be considered a base-year respondent, a student had to have child assessment data
(defined as having at least one set of scoreable mathematics/reading/science data OR a height or weight
measurement, or having been excluded from the assessment due to lack of accommodation for a
disability) or parent interview data from the fall or spring data collection, or both, in the base year. Later
rounds of data collection were conducted only with base-year respondents. Sampled students who did not
participate in the base year were not recontacted for later rounds of data collection, and no new students
were added to the study sample after the base year.
As mentioned in the base-year User’s Manual, operational problems prevented the study
from conducting data collection activities in some areas of the country where API and American
Indian/Alaska Native students sampled for the study resided. For this reason, base-year response rates for
these groups of students were lower than response rates for students of other racial/ethnic backgrounds.
As a result, a relatively small number of ECLS-K:2011 sample children in the Native Hawaiian/Other
Pacific Islander group resided in Hawaii. Additionally, nonresponse on the child assessment, parent
interview, or both, leads to some of these sampled cases not being included in weighted analyses
depending on the weight used. Also, none of the ECLS-K:2011 sample children in the American
Indian/Alaska Native group resided in Alaska at the time of sampling. Users are encouraged to consider
these sample characteristics when making statements about children in these two racial groups. As a
reminder, however, the study was not designed to be representative at the state level or for subgroups
within any specific racial or ethnic group.
4-4Table 4-2. Number (unweighted) of eligible children sampled for the ECLS-K:2011, by selected
characteristics: School year 2010–11
Characteristic Total Public school Private school
Total 20,234 17,733 2,501
Census region1,2,3
Northeast
Midwest
South
West
Locale1,4
City
Suburb
Town
Rural
Religious affiliation1
Catholic
Other religious
Nonreligious, private
Child’s race/ethnicity5
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific Islander,
non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Other, non-Hispanic 6
3,500
4,240
7,230
5,270
6,675
7,657
1,557
4,345
974
1,002
525
9,673
2,619
4,832
1,830
152
218
910
2,930
3,520
6,620
4,660
5,822
6,461
1,383
4,067
†
†
†
8,167
2,357
4,491
1,597
130
207
784
570
710
610
610
853
1,196
174
278
974
1,002
525
1,506
262
341
233
22
11
126
† Not applicable.
1 Data for this school characteristic are taken from the original school sampling frame. Therefore, the table estimates for this characteristic cannot
be replicated with variables on the released data file.
2 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
5 Race/ethnicity information was obtained from schools at the time of sampling.
6 This category includes children who are more than one race (non-Hispanic) and children whose race/ethnicity is unknown.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2010 and spring 2011.
4-5Table 4-3. Number (unweighted) of sampled children who are base-year respondents, by selected
characteristics: School year 2010–11
Characteristic Total Public school Private school
Total 18,174 15,953 2,221
Census region1,2,3
Northeast
Midwest
South
West
Locale1,4
City
Suburb
Town
Rural
Religious affiliation1
Catholic
Other religious
Nonreligious, private
3,010
3,870
6,640
4,660
6,014
6,793
1,405
3,962
863
903
455
Child’s race/ethnicity5
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific Islander,
non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
8,488
2,396
4,592
1,543
117
168
827
43
2,540
3,220
6,070
4,130
5,252
5,746
1,254
3,701
†
†
†
7,174
2,159
4,269
1,357
100
159
709
26
470
650
570
530
762
1,047
151
261
863
903
455
1,314
237
323
186
17
9
118
17
† Not applicable.
1 Data for this school characteristic are taken from the original school sampling frame. Therefore, the table estimates for this characteristic cannot
be replicated with variables on the released data file.
2 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
5 Race/ethnicity information is from the fifth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly
different from the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), fall 2010, spring 2011, and spring 2016.
4-64.2 Sample Design for the First- Through Fifth-Grade Years
4.2.1 Fall First Grade and Fall Second Grade
This section describes the sample design for the fall data collections that occurred in first
and second grades. Beginning with third grade, data collections occurred only in the spring of the school
year. A subsample of students was selected for the fall first-grade and second-grade data collections from
the full study sample described above via a three-step procedure. This subsample was designed to be
representative of the full sample. In the first step, 30 PSUs were sampled from the 90 PSUs selected for
the base year. Within the 30 subsampled PSUs, the 10 self-representing PSUs are large in population size
and were included in the fall first-grade sample with certainty. The remaining 20 PSUs were selected
from the 80 non-self-representing PSUs in 40 strata. To select the 20 non-self-representing PSUs, 20
strata were sampled with equal probability, and then one PSU was sampled within each stratum also with
equal probabilities. This is equivalent to selection with probability proportional to size since the original
PSU sample was selected with probability proportional to size.
In the second step, all schools within the 30 subsampled PSUs that were eligible for the
base-year collection were included in the fall subsample for both first and second grades. However, data
collection was not conducted in the subsampled schools in which no children participated in the base year
because the study did not try to recruit base-year nonrespondents for later rounds of data collections.
Table 4-4 shows the characteristics of all fall subsampled schools in the 30 PSUs selected in the first stage
of sampling.4 Table 4-5 shows the characteristics for the subsampled schools with base-year respondents;
these are the schools in which data collection was conducted. Transfer schools (those schools that
children moved into after the fall of kindergarten) are not included in this table. Of the 346 original
sampled schools at the start of the fall data collections, 306 schools still cooperated in fall second grade. 5
In the third step of sampling, students attending the subsampled schools who were
respondents in the base year and who had not moved outside of the United States or died before the day
assessments began in their school for the fall first-grade data collection were included as part of the fall
sample for the first-grade data collection. This sample formed the base sample for the fall second-grade
data collection as well, though subsampled children who had died or moved outside of the United States
before the day assessments began in their school for the fall second-grade data collection were excluded.
4 The fall second-grade data collection also included schools to which the children sampled for the fall collections in the third step of sampling
had moved after sampling. These schools were not part of the original subsample selected in the second step of sampling and, therefore, are not
included in table 4-4.
5 After the base year, some original sampled schools no longer have students originally sampled in them, but the schools remain in the study
because students originally sampled in other schools have moved into them. Other original sampled schools include both students originally
sampled in them and transfer students.
4-7Table 4-6 shows the characteristics of base-year respondents in the fall subsample who were selected in
the third sampling step.
Table 4-4. Number (unweighted) of original sampled schools in the 30 PSUs selected for the fall data
collections, by selected characteristics: Fall 2011 and fall 2012
Characteristic Total Public Private
Total 568 462 106
Census region1,2
Northeast
Midwest
South
West
90
100
170
210
60
90
150
170
30
10
30
40
Locale3
City
Suburb
Town
Rural
241
224
19
84
202
175
15
70
39
49
4
14
Religious affiliation
Catholic
Other religious
Nonreligious, private
29
43
34
†
†
†
29
43
34
† Not applicable.
1 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
3 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these
characteristics cannot be replicated with variables on the released data file.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2011 and fall 2012.
4-8Table 4-5. Number (unweighted) of original sampled schools with base-year respondents at the start of
the fall data collections, by selected characteristics: Fall 2011 and fall 2012
Characteristic Total Public Private
Total 346 305 41
Census region1,2
Northeast
Midwest
South
West
50
60
120
120
40
50
110
100
10
10
10
20
Locale3
City
Suburb
Town
Rural
144
134
15
53
132
112
12
49
12
22
3
4
Religious affiliation
Catholic
Other religious
Nonreligious, private
16
12
13
†
†
†
16
12
13
† Not applicable.
1 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
3 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these
characteristics cannot be replicated with variables on the released data file.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2011 and fall 2012.
4-9Table 4-6. Number (unweighted) of base-year respondents in the fall first- and second-grade sample, by
selected characteristics: Fall 2011 and fall 2012
Characteristic Total Public Private
Total 6,109 5,458 651
Census region1,2,3
Northeast
Midwest
South
West
Locale1,4
City
Suburb
Town
Rural
820
1,120
2,000
2,170
2,549
2,461
250
849
730
1,010
1,840
1,880
2,295
2,101
227
835
90
110
170
280
254
360
23
14
Religious affiliation1
Catholic
Other religious
Nonreligious, private
Race/ethnicity5
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific Islander,
non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
242
233
176
2,260
675
2,290
476
33
117
245
13
†
†
†
1,916
611
2,157
422
27
110
208
7
242
233
176
344
64
133
54
6
7
37
6
† Not applicable.
1 Data for this school characteristic are taken from the original school sampling frame. Therefore, the table estimates for this characteristic cannot
be replicated with variables on the released data file.
2 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
4 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
5 Race/ethnicity is from the fifth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different
from the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment
due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2011, fall 2012, and spring 2016.
4-10Tables 4-7 and 4-8 show the characteristics of base-year respondents in the fall samples, by
whether the students were still in the original sampled schools or had transferred to other schools by the
end of first grade and second grade, respectively.
Table 4-7 shows that 81 percent of students were still attending their original sampled
schools in the fall of first grade. Table 4-8 shows that 70 percent of students were still attending their
original sampled schools in the fall of second grade. In the fall of first grade, the lowest percentages of
students who were still attending their original sample schools are for students in non-Catholic private
schools, students in the West, students in the suburbs, and Black students. The same is true for the fall of
second grade with the percentage of students in non-Catholic private schools even lower than in first
grade.6
6 Significance tests were not conducted for the comparisons in this chapter because the differences discussed were based on the same sample of
base-year respondents.
4-11Table 4-7. Number (unweighted) of base-year respondents in fall first grade, by type of sampled school
and selected characteristics: Fall 2011
Characteristic Total
Original
sampled school Transfer school
Percent in original
sampled school
Total 6,109 4,945 1,164 80.9
School type1
Public
Private
Catholic
Other private
Unknown/home school
Census region1,2,3
Northeast
Midwest
South
West
Unknown
Locale1,4
City
Suburb
Town
Rural
Unknown
Race/ethnicity5
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific
Islander, non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
4,900
552
232
320
657
760
980
1,780
1,960
640
4,414
468
208
260
63
486
84
24
60
594
90.1
84.8
89.7
81.3
9.6
660
900
1,620
1,720
50
90
80
160
240
590
87.8
91.6
90.8
87.9
7.2
2,354
2,057
217
781
700
2,260
675
2,290
476
33
117
245
13
2,127
1,831
198
718
71
227
226
19
63
629
90.4
89.0
91.2
91.9
10.1
1,905
487
1,826
400
26
97
197
7
355
188
464
76
7
20
48
6
84.3
72.1
79.7
84.0
78.8
82.9
80.4
53.8
1 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school
characteristics. Data for school census region and school locale are taken from the first-grade composite variables X3REGION and X3LOCALE.
There was no school administrator questionnaire in the fall of first grade. Therefore, the composite for school type, X3SCTYP, was constructed
specially for the User’s Manual and not included in the data file.
2 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
4 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
5 Race/ethnicity is from the fifth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different
from the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment
due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2011.
4-12Table 4-8. Number (unweighted) of base-year respondents in the fall second grade, by type of sampled
school and selected characteristics: Fall 2012
Characteristic Total
Original
sampled school Transfer school
Percent in original
sampled school
Total 6,109 4,274 1,835 70.0
School type1
Public
Private
Catholic
Other private
Unknown/home school
Census region1,2,3
Northeast
Midwest
South
West
Unknown
Locale1,4
City
Suburb
Town
Rural
Unknown
Race/ethnicity5
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific
Islander, non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
5,036
424
220
204
649
760
950
1,700
1,930
770
3,951
323
161
162
0
630
760
1,410
1,480
#
1,786
1,617
159
687
25
1,085
101
59
42
649
78.5
76.2
73.2
79.4
0.0
130
190
300
460
770
83.4
80.0
82.6
76.3
0.3
2,201
2,032
182
801
893
2,260
675
2,290
476
33
117
245
13
415
415
23
114
868
81.1
79.6
87.4
85.8
2.8
1,700
387
1,574
347
22
75
162
7
560
288
716
129
11
42
83
6
75.2
57.3
68.7
72.9
66.7
64.1
66.1
53.8
# Rounds to zero.
1 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school
characteristics. Data for school census region and school locale are taken from the second-grade composite variables X5REGION and
X5LOCALE. There was no school administrator questionnaire in the fall of second grade; therefore, the composite for school type, X5SCTYP,
was constructed specially for the User’s Manual and not included in the data file.
2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and
Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and
Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma,
South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii,
Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
4 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new
school procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was
imputed for the estimates in this table. Imputed values for locale are not included in the data file.
5 Race/ethnicity is from the fifth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different
from the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment
due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), fall 2012 and spring 2016.
4-134.2.2 Spring First Grade Through Spring Fifth Grade
All base-year respondents were statistically eligible for the spring data collections from first
grade through fifth grade, with the exception of those who moved outside the United States or died before
the assessments began in their school. Table 4-9 shows the characteristics of the original sample schools
with base-year respondents in all 90 study PSUs. This sample constituted the starting school sample,
exclusive of transfer schools, for each spring round of data collection after the base year. Transfer schools
(those schools that children moved into after the fall of kindergarten) are not included in this table. Of the
989 original sampled schools at the start of the spring data collections, 910 cooperated in spring first
grade, 896 cooperated in spring second grade, 891 cooperated in spring third grade, 854 cooperated in
spring fourth grade, and 830 cooperated in spring fifth grade.
Table 4-9. Number (unweighted) of original sampled schools in the 90 PSUs selected for the spring data
collections with base-year respondents, by selected characteristics: Spring 2012, spring
2013, spring 2014, spring 2015, and spring 2016
Characteristic Tota1 Public Private
Total 989 858 131
Census region1,2
Northeast
Midwest
South
West
170
200
360
260
150
150
330
230
30
40
40
30
Locale3
City
Suburb
Town
Rural
321
357
86
225
278
302
73
205
43
55
13
20
Religious affiliation
Catholic
Other religious
Nonreligious, private
52
55
24
†
†
†
52
55
24
† Not applicable.
1 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and
Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and
Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma,
South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii,
Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
3 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these
characteristics cannot be replicated with variables on the released data file.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2012, spring 2013, spring 2014, spring 2015, and spring 2016.
4-14The characteristics of base-year respondents who were eligible for the spring data collections
for first through fifth grade are those presented above in table 4-3; since there was no subsampling for the
spring rounds of data collection, all base-year respondents were initially eligible for data collection if they
had not moved outside the United States or died prior to data collection. By the end of the fifth-grade data
collections, about 210 base-year respondents had moved out the country and 10 had died.
Tables 4-10 to 4-18 show the characteristics of base-year respondents in the spring samples,
by whether the students were still in their original sampled schools or had transferred to other schools. In
the spring of first grade, 78 percent of base-year respondents were still attending their original sampled
schools. This percent is 68 for the spring of second grade, 59 for the spring of third grade, 52 for the
spring of fourth grade, and 45 for the spring of fifth grade. As is seen with the fall subsample, the lowest
percentages of students who were still attending their original sample schools in the spring of first grade
are for students in non-Catholic private schools, students in the West, students in the suburbs, and Black
students. For the spring of second grade, for third grade, and fifth grade the pattern is the same except that
students in different types of private schools moved at about the same rate, while students in public
schools moved at a higher rate than students in Catholic schools and in non-Catholic private schools, and
students in the Northeast moved at a higher rate than students in other census regions. In fourth grade, the
pattern is similar to the first-grade data collection; namely, Black students moved at a higher rate, and so
did students in the suburbs, students in the West, and students in non-Catholic private schools.
As discussed in chapter 2, in the spring of fifth grade, as in fourth grade, separate child-
/classroom-level questionnaires were given to reading, mathematics, and science teachers to
accommodate variations in the organization of instruction, with study children having different teachers
for the different subject areas. Reading teacher questionnaires were distributed for all children.
Mathematics teacher questionnaires were distributed for half of the children, and science teacher
questionnaires were distributed for the other half. Selection was done with equal probability, using the
third-grade response status of child and parent for stratification (respondent, nonrespondent/unknown
eligibility, and ineligible/non-followed movers). There is a flag variable (X9MSFLAG) on the data file
that indicates whether a child case was selected for mathematics (X9MSFLAG=0) or science
(X9MSFLAG=1). These flags have the same values as for fourth grade. Each teacher linked to a study
child was also asked to complete a teacher-level questionnaire. Every teacher received the same teacher-
level questionnaire; it was not tailored to a specific subject. Tables 4-17 and 4-18 show the characteristics
of base-year respondents in fifth grade who were selected for the mathematics teacher questionnaires, and
those who were selected for the science teacher questionnaires, respectively.
4-15Table 4-10. Number (unweighted) of base-year respondents in spring first grade, by type of sampled
school and selected characteristics: Spring 2012
Characteristic Total
Original
sampled school
Transfer
school1
Percent in original
sampled school
Total 18,174 14,104 4,070 77.6
School type2
Public
Private
Catholic
Other private
Unknown/home school
Census region2,3,4
Northeast
Midwest
South
West
Unknown
Locale2,5
City
Suburb
Town
Rural
Unknown
Race/ethnicity6
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific
Islander, non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
13,772
1,946
774
1,172
2,456
2,600
3,280
5,690
4,160
2,460
12,361
1,736
726
1,010
7
2,350
2,960
5,190
3,600
10
1,411
210
48
162
2,449
250
320
490
560
2,500
89.8
89.2
93.8
86.2
0.3
90.5
90.2
91.3
86.5
0.3
5,231
5,613
1,221
3,344
2,765
8,488
2,396
4,592
1,543
117
168
827
43
4,643
4,961
1,140
3,162
198
588
652
81
182
2,567
88.8
88.4
93.4
94.6
7.2
6,821
1,623
3,542
1,254
87
122
635
20
1,667
773
1,050
289
30
46
192
23
80.4
67.7
77.1
81.3
74.4
72.6
76.8
46.5
1 Transfer school totals include those children who became ineligible after the base year.
2 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school
characteristics. Data for school census region and school locale are taken from the first-grade composite variables X4SCTYP, X4REGION, and
X4LOCALE.
3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and
Vermont.Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and
Wisconsin.South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma,
South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii,
Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
5 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
6 Race/ethnicity is from fifth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from
the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment
due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2012 and spring 2016.
4-16Table 4-11. Number (unweighted) of base-year respondents in spring second grade, by type of sampled
school and selected characteristics: Spring 2013
Characteristic Total
Original
sampled school
Transfer
school1
Percent in original
sampled school
Total 18,174 12,274 5,900 67.5
School type2
Public
Private
Catholic
Other private
Unknown/home school
Census region2,3,4
Northeast
Midwest
South
West
Unknown
Locale2,5
City
Suburb
Town
Rural
Unknown
Race/ethnicity6
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific
Islander, non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
13,116
1,388
655
733
3,670
2,400
3,020
5,180
3,860
3,700
11,029
1,245
587
658
0
2,087
143
68
75
3,670
84.1
89.7
89.6
89.8
0.0
2,060
2,570
4,500
3,150
#
350
450
690
720
3,700
85.6
85.0
86.8
81.5
0.1
4,762
5,139
1,070
3,149
4,054
3,968
4,248
976
2,906
176
794
891
94
243
3,878
83.3
82.7
91.2
92.3
4.3
8,488
2,396
4,592
1,543
117
168
827
43
6,078
1,298
3,095
1,101
73
98
516
15
2,410
1,098
1,497
442
44
70
311
28
71.6
54.2
67.4
71.4
62.4
58.3
62.4
34.9
# Rounds to zero.
1 Transfer school totals include those children who became ineligible after base-year.
2 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school
characteristics. Data for school type, school census region, and school locale are taken from the second-grade composite variables X6SCTYP,
X6REGION, and X6LOCALE.
3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and
Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and
Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma,
South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii,
Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
5 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
6 Race/ethnicity is from fifth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from
the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment
due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2013 and spring 2016.
4-17Table 4-12. Number (unweighted) of base-year respondents in spring third grade, by type of sampled
school and selected characteristics: Spring 2014
Characteristic Total
Original
sampled school
Transfer
school1
Percent in original
sampled school
Total 18,174 10,641 7,533 58.6
School type2
Public
Private
Catholic
Other private
Unknown/home school
Census region2,3,4
Northeast
Midwest
South
West
Unknown
Locale2,5
City
Suburb
Town
Rural
Unknown
12,369
1,286
631
655
4,519
Race/ethnicity6
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific
Islander, non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
2,280
2,850
4,840
3,700
4,520
4,467
4,841
990
2,993
4,883
8,488
2,396
4,592
1,543
117
168
827
43
9,532
1,109
545
564
0
1,740
2,210
3,860
2,840
0
3,503
3,594
814
2574
156
5,317
1,058
2,686
978
63
85
440
14
2,837
177
86
91
4519
550
640
970
860
4520
964
1247
176
419
4727
3,171
1,338
1,906
565
54
83
387
29
77.1
86.2
86.4
86.1
0.0
76.1
77.6
79.9
76.7
0.0
78.4
74.2
82.2
86.0
3.2
62.6
44.2
58.5
63.4
53.8
50.6
53.2
32.6
1 Transfer school totals include those children who became ineligible after base-year.
2 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school
characteristics. Data for school type, school census region, and school locale are taken from the third-grade composite variables X7SCTYP,
X7REGION, and X7LOCALE.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma,
South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
5 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
6 Race/ethnicity is from the fifth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different
from the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment
due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2014 and spring 2016.
4-18Table 4-13. Number (unweighted) of base-year respondents in spring fourth grade, by type of sampled
school and selected characteristics: Spring 2015
Characteristic Total
Original
sampled school
Transfer
school1
Percent in original
sampled school
Total 18,174 9,496 8,678 52.3
School type2
Public
Private
Catholic
Other private
Unknown/home school
Census region2,3,4
Northeast
Midwest
South
West
Unknown
Locale2,5
City
Suburb
Town
Rural
Unknown
11,770
1,198
590
608
5,206
Race/ethnicity6
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific
Islander, non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
2,160
2,710
4,560
3,540
5,210
4,113
5,422
851
2,237
5,551
8,488
2,396
4,592
1,543
117
168
827
43
8,493
1,003
503
500
0
1,470
2,010
3,440
2,570
0
3,071
3,824
630
1,848
123
4,766
907
2428
862
54
79
387
13
3,277
195
87
108
5,206
690
700
1,120
970
5,210
1,042
1,598
221
389
5,428
3,722
1,489
2,164
681
63
89
440
30
72.2
83.7
85.3
82.2
0.0
68.2
74.3
75.4
72.7
0.0
74.7
70.5
74.0
82.6
2.2
56.1
37.9
52.9
55.9
46.2
47.0
46.8
30.2
1 Transfer school totals include those children who became ineligible after base-year.
2 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school
characteristics. Data for school type, school census region, and school locale are taken from the fourth-grade composite variables X8SCTYP,
X8REGION, and X8LOCALE.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma,
South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
5 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
6 Race/ethnicity is from the fifth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different
from the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment
due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2015 and spring 2016.
4-19Table 4-14. Number (unweighted) of base-year respondents in spring fourth grade who were selected
for the mathematics teacher questionnaire, by type of sampled school and selected
characteristics: Spring 2015
Characteristic Total
Original
sampled school
Transfer
school1
Percent in original
sampled school
Total 9,087 4,724 4,363 52.0
School type2
Public
Private
Catholic
Other private
Unknown/home school
Census region2,3,4
Northeast
Midwest
South
West
Unknown
Locale2,5
City
Suburb
Town
Rural
Unknown
Race/ethnicity6
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific
Islander, non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
5,899
589
292
297
2,599
1,080
1,350
2,290
1,770
2,600
2,051
2,709
439
1,118
2,770
4,206
1,198
2,310
764
56
83
443
27
4,235
489
245
244
0
730
1,010
1,710
1,280
0
1,664
100
47
53
2,599
71.8
83.0
83.9
82.2
0.0
350
340
580
490
2,600
67.4
74.7
74.7
72.3
0.0
1,517
1,894
329
922
62
534
815
110
196
2,708
74.0
69.9
74.9
82.5
2.2
2,357
454
1,203
428
25
40
208
9
1,849
744
1,107
336
31
43
235
18
56.0
37.9
52.1
56.0
44.6
48.2
47.0
33.3
1 Transfer school totals include those children who became ineligible after base-year.
2 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school
characteristics. Data for school type, school census region, and school locale are taken from the fourth-grade composite variables X8SCTYP,
X8REGION, and X8LOCALE.
3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and
Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and
Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma,
South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii,
Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
5 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
6 Race/ethnicity is from the fourth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different
from the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment
due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2015.
4-20Table 4-15. Number (unweighted) of base-year respondents in spring fourth grade who were selected
for the science teacher questionnaire, by type of sampled school and selected
characteristics: Spring 2015
Characteristic Total
Original
sampled school
Transfer
school1
Percent in original
sampled school
Total 9,087 4,772 4,315 52.5
School type2
Public
Private
Catholic
Other private
Unknown/home school
Census region2,3,4
Northeast
Midwest
South
West
Unknown
Locale2, 5
City
Suburb
Town
Rural
Unknown
Race/ethnicity6
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific
Islander, non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
5,871
609
298
311
2,607
1,080
1,360
2,280
1,770
2,610
2,062
2,713
412
1,119
2,781
4,282
1,198
2,282
779
61
85
384
16
4,258
514
258
256
0
740
1,000
1,740
1,290
0
1,613
95
40
55
2,607
330
350
540
480
2,610
72.5
84.4
86.6
82.3
0.0
68.9
73.9
76.2
73.1
0.0
1,554
1,930
301
926
61
508
783
111
193
2,720
75.4
71.1
73.1
82.8
2.2
2,409
453
1,225
434
29
39
179
4
1,873
745
1,057
345
32
46
205
12
56.3
37.8
53.7
55.7
47.5
45.9
46.6
25.0
1 Transfer school totals include those children who became ineligible after base-year.
2 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school
characteristics. Data for school type, school census region, and school locale are taken from the fourth-grade composite variables X8SCTYP,
X8REGION, and X8LOCALE.
3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and
Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and
Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma,
South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii,
Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
5 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
6 Race/ethnicity is from the fourth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different
from the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment
due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2015.
4-21Table 4-16. Number (unweighted) of base-year respondents in spring fifth grade, by type of sampled
school and selected characteristics: Spring 2016
Characteristic Total
Original
sampled school
Transfer
school1
Percent in original
sampled school
Total 18,174 8,157 10,017 44.9
School type2
Public
Private
Catholic
Other private
Unknown/home school
Census region2,3,4
Northeast
Midwest
South
West
Unknown
Locale2,5
City
Suburb
Town
Rural
Unknown
11,260
1,148
568
580
5,766
Race/ethnicity6
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific
Islander, non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
2,060
2,580
4,340
3,430
5,770
3,873
5,214
850
2,114
6,123
8,488
2,396
4,592
1,543
117
168
827
43
7,227
930
464
466
0
1,180
1,630
2,990
2,350
0
2,735
3,240
514
1,556
112
4,011
769
2,129
781
47
76
332
12
4,033
218
104
114
5,766
880
950
1,350
1,080
5,770
1,138
1,974
336
558
6,011
4,477
1,627
2,463
762
70
92
495
31
64.2
81.0
81.7
80.3
0.0
57.4
63.2
68.9
68.6
0.0
70.6
62.1
60.5
73.6
1.8
47.3
32.1
46.4
50.6
40.2
45.2
40.1
27.9
1 Transfer school totals include those children who became ineligible after base-year.
2 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school
characteristics. Data for school type, school census region, and school locale are taken from the fifth-grade composite variables X9SCTYP,
X9REGION, and X9LOCALE.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma,
South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
5 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
6 Race/ethnicity is from the fifth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different
from the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment
due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2016.
4-22Table 4-17. Number (unweighted) of base-year respondents in spring fifth grade who were selected for
the mathematics teacher questionnaire, by type of sampled school and selected
characteristics: Spring 2016
Characteristic Total
Original
sampled school
Transfer
school1
Percent in original
sampled school
Total 9,087 4,037 5,050 44.4
School type2
Public
Private
Catholic
Other private
Unknown/home school
Census region2,3,4
Northeast
Midwest
South
West
Unknown
Locale2,5
City
Suburb
Town
Rural
Unknown
Race/ethnicity6
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific
Islander, non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
5,645
562
277
285
2,880
1,040
1,290
2,180
1,710
2,880
1,937
2,596
434
1,055
3,065
4,206
1,198
2,310
764
56
83
443
27
3,587
450
224
126
0
590
820
1,470
1,160
0
2,058
112
53
59
2,880
450
470
700
540
2,880
63.5
80.1
80.9
44.2
0.0
56.4
63.5
67.6
68.1
0.0
1,354
1,587
267
772
57
583
1,009
167
283
3,008
69.9
61.1
61.5
73.2
1.9
1,986
379
1,046
384
23
37
173
9
2,220
819
1,264
380
33
46
270
18
47.2
31.6
45.3
50.3
41.1
44.6
39.1
33.3
1 Transfer school totals include those children who became ineligible after base-year.
2 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school
characteristics. Data for school type, school census region, and school locale are taken from the fifth-grade composite variables X9SCTYP,
X9REGION, and X9LOCALE.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma,
South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
5 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
6 Race/ethnicity is from the fifth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different
from the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment
due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2016.
4-23Table 4-18. Number (unweighted) of base-year respondents in spring fifth grade who were selected for
the science teacher questionnaire, by type of sampled school and selected characteristics:
Spring 2016
Characteristic Total
Original
sampled school
Transfer
school1
Percent in original
sampled school
Total 9,087 4,120 4,967 45.3
School type2
Public
Private
Catholic
Other private
Unknown/home school
Census region2,3,4
Northeast
Midwest
South
West
Unknown
Locale2,5
City
Suburb
Town
Rural
Unknown
Race/ethnicity6
White, non-Hispanic
Black, non-Hispanic
Hispanic
Asian, non-Hispanic
Native Hawaiian/Other Pacific
Islander, non-Hispanic
American Indian or Alaska Native,
non-Hispanic
Two or more races, non-Hispanic
Unknown
5,615
586
291
295
2,886
1,020
1,300
2,160
1,720
2,890
1,936
2,618
416
1,059
3,058
4,282
1,198
2,282
779
61
85
384
16
3,640
480
240
240
0
600
820
1,520
1,190
0
1,975
106
51
55
2,866
420
480
650
530
2,890
64.8
81.9
82.5
81.4
0.0
58.4
62.9
70.2
69.1
0.0
1,381
1,653
247
784
55
555
965
169
275
3,003
71.3
63.1
59.4
74.0
1.8
2,025
390
1,083
397
24
39
159
3
2,257
808
1,199
382
37
46
225
13
47.3
32.6
47.5
51.0
39.3
45.9
41.4
18.8
1 Transfer school totals include those children who became ineligible after base-year.
2 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school
characteristics. Data for school type, school census region, and school locale are taken from the fifth-grade composite variables X9SCTYP,
X9REGION, and X9LOCALE.
3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and
Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and
Wisconsin.South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma,
South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii,
Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
5 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school
procedure (see section 4.1.2.7 of the base-year User’s Manual), locale information was not available in the school frame and was imputed for the
estimates in this table. Imputed values for locale are not included in the data file.
6 Race/ethnicity is from the fifth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different
from the counts in similar tables in the user’s manuals from previous years. X_RACETH_R was revised after every data collection.
NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment
due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2016.
4-244.2.3 Following Movers
Sections 4.2.1 and 4.2.2 discuss the samples of eligible students included in the fall and
spring data collections in first and second grades and in the spring data collections in third, fourth, and
fifth grades. As noted, students who moved outside the United States or died prior to data collection in
their schools became ineligible for the study. Their exclusion represents a limitation on the population to
which the study generalizes in later rounds of data collection. For example, the data collected in spring
2016 are representative of the experiences of children in the kindergarten class of 2010–11 who were
living in the United States in the spring of 2016.
In order to control data collection costs, there are some students who are part of the
statistical samples for the first-, second-, third-, fourth-, and fifth-grade data collections but were excluded
from actual data collection. These students, while statistically eligible for the study, were operationally
ineligible. Specifically, not all students who moved away from their original base-year schools after the
spring base-year data collection (known as “movers”) were followed into their new schools. While some
movers were followed with certainty, some subsampling of other movers occurred, as described below.
Although information was not collected from all students in every round, the study sampling procedures,
combined with the use of sampling weights that include mover subsampling adjustments (described
below in section 4.3.2.2) in data analysis, result in the collected data being representative of the students
in the kindergarten class of 2010–11 who remain living in the United States.
Homeschooled children (i.e., those who were enrolled in a school at the time of sampling in
the base year but left school to become homeschooled) were followed with certainty; they were assessed
in their home if there was parental consent to do so.
Destination schools. When four or more students moved from an original sampled school
into the same transfer school, all those movers were followed into the new school, which is referred to as
a destination school. This type of movement occurred for children who attended sampled schools that
ended at a particular grade, which are referred to as terminal schools. For example, study students who
attended an original sample school that ended with third grade would move as a group to a new school for
fourth grade. In some cases, an original sample school did not terminate in a particular grade, but for
some reason four or more students from that school moved together into the same transfer school for the
subsequent data collections. For example, this would happen if an original sample school closed after the
spring third-grade data collection. More than one destination school may be identified for an original
school if separate clusters of four or more students moved into different transfer schools.
4-25Language minority (LM) students, students with an Individualized Education Program
(IEP), and students who had an Individualized Family Service Plan (IFSP). Students who were
identified as language minority (LM) based on parent report of home language in the base year, as well as
students identified as currently having an Individualized Education Program (IEP), or who had an
Individualized Family Service Plan (IFSP) were followed at a rate of 100 percent in fifth grade. The IEP
status of the child was obtained during the preassessment call when the team leader asked the school
coordinator whether the child had an IEP or equivalent program on record with the school. The school
records also may have indicated that a child had an IFSP when he or she was younger, even if the child
did not have an IEP at the time of data collection, which the team leader could have noted during the call.
Additionally, information about whether a child had had an IFSP prior to kindergarten was collected in
the base-year parent interview. Due to an identification error before third grade, a number of these
children who moved from their originally sampled school were not flagged to be followed with certainty
in first grade and second grade. Despite this lack of sample protection, approximately 92 percent of the
students who had had an IFSP were followed into second grade, either because they did not change
schools, they had an IEP and became part of the protected group as a result of the IEP, or because they
were already identified as part of the mover subsample that was followed at a rate of 50 percent.7 In third
grade, the identification error was corrected, and an additional 350 students who had had an IFSP were
identified and followed with certainty. In fourth grade, about 590 students who had had an IFSP were
followed with certainty, and about 520 had child or parent data. In fifth grade, about 590 students8 who
had had an IFSP were followed with certainty, and about 510 had child or parent data.
7 There are some differences between the group of IFSP children who were followed and those who were not. However, some of these differences
appear to be related to the likelihood that a child had an IEP (and, therefore, whether the child became part of the protected group as a result of
the IEP). For example, compared to those IFSP children who were not followed, a higher percentage of IFSP children who were followed
attended public schools, which are required to provide disability services through an IEP.
The subsampling process itself should not have introduced bias into the sample of IFSP children who were followed, because cases were
randomly flagged to be followed. Additionally, the sampling weights developed for use with second-grade data account for this random
subsampling. A comparison of key weighted estimates (such as school type, region of residence, school locale, percent of students in the school
who were races other than White, and student race/ethnicity, gender, and year of birth) between kindergarten and first grade generally suggests
the loss of those children who were not followed has little impact on the overall estimates for children who had IFSPs before age 3. Where slight
differences between the kindergarten and first-grade estimates were noticed (for example, in the percent of students of race other than White in a
school), the pattern with the sample of IFSP children is reflective of differences seen in the full ECLS-K:2011 sample. Also, it should be kept in
mind that identifying a child to be followed with certainty does not necessarily mean that the child would have participated in the round(s) in
which he or she was followed. Due to general sample attrition, the IFSP students who were not flagged to be followed with certainty constitute
only about half of all IFSP children who did not participate in first grade and second grade. It is unlikely that differences in weighted estimates
for the entire group of IFSP children (about 680) are due solely to the absence of the approximately 60 IFSP cases that were not followed neither
in first grade nor in second grade.
Nonparticipation of IFSP children in later rounds of the study for any reason does reduce the IFSP sample available for analysis. As is the case
for analysis of any small subgroup, users should consider the size of their analytic sample and whether there is enough power in the data to make
generalizations about the groups being examined.
8 Of the 590 students who had an IFSP and who were followed with certainty in fourth grade, less than 10 moved out of the country prior to the
fifth-grade data collection.
4-26General procedures for all other movers. Fifty percent of students who did not meet one
of the criteria described above (i.e., did not move to a destination school, were not LM, and did not have
an IEP) were sampled with equal probability to be flagged as “follow” if they moved from their original
sample school. If a student was flagged as “do not follow,” no data were collected for him or her once he
or she moved school. Students flagged as “do not follow” were not sought for participation in any further
data collection unless they were part of the fall subsample, as explained further below. If a student was
flagged as “follow,” and
1. the student moved into any school in a study PSU (whether or not the school
participated in the study), the student was included in all aspects of data collection
(child assessment, child questionnaire, parent interview, school administrator
questionnaire, and teacher questionnaires);
2. the student moved into a school outside a study PSU: only a parent interview was
attempted; and,
3. the student moved into a school outside the country: the student was out of scope and
considered ineligible for continuation in the study.
Procedures for students in the fall subsample. Fifty percent of all students in the
subsample had their follow flag set to “follow” after the base-year data collection. Children were sampled
with equal probability to be flagged as “follow,” meaning that if they transferred to a new school they
would be followed into that new school for the fall first- and second- grade data collections. As explained
in detail below, all students who were subsampled in the fall, regardless of their mover status, were
followed in the spring data collections. As a result of these procedures, some subsample students were not
followed in the fall collections, because their follow flag applicable to the fall collections was set to “not
follow,” but they were followed in the spring collections.
Procedures for students in the spring main sample. Fifty percent of the schools in the
main sample were subsampled with equal probability to have follow flags (i.e., all students in the 50
percent subsample of schools have flags set to “follow”) applicable for the spring data collections. All fall
schools in the 30 sampled PSUs were included in the “mover follow” sample for the spring of first,
second, third, fourth, and fifth grade. An additional sample of schools that were not part of the fall
subsample was selected to arrive at 50 percent of the entire sample of schools being included in the
“mover follow” subsample in the spring first-, second-, third-, fourth-
, and fifth-grade data collections. In
this way, students who were originally sampled for fall data collections were included in the spring data
collections with certainty. These fall subsample cases were followed for the spring data collections even if
they were movers in the fall and had their fall mover flag set to “not follow” or they were nonrespondents
in the fall. Also, this method allows fall subsample movers to continue to be followed in each subsequent
4-27round of data collection, as well as more clustering of the movers to be followed, thus cutting down on
field costs.
4.3 Calculation and Use of Sample Weights
The ECLS-K:2011 data should be weighted to account for differential probabilities of
selection at each sampling stage and to adjust for the effect nonresponse can have on the estimates. For
the base year, weights were provided at the child and school levels. Estimates produced using the base-
year child-level weights are representative of children who attended kindergarten or who attended an
ungraded school or classroom and were of kindergarten age in the United States in the 2010–11 school
year. Estimates produced using the base-year school-level weight are representative of schools with
kindergarten programs or schools that educate children of kindergarten age in an ungraded setting.
For all data collections after the kindergarten year, weights are provided only at the child
level, to produce estimates for the kindergarten cohort during the 2011–12 school year, the 2012–13
school year, the 2013–14 school year, the 2014–15 school year, and the 2015–16 school year,
respectively. There are no school-level weights because the school sample is no longer nationally
representative; it is not representative of schools with first-grade students, second-grade students, third-
grade students, fourth-grade students, fifth-grade students or ungraded schools serving children of first-
grade, second-grade, third-grade, fourth-grade, or fifth-grade age. The school sample is simply a set of
schools attended by the children in the ECLS-K:2011 cohort during the 2011–12, the 2012–13, the 2013–
14, the 2014–15, and the 2015–16 school years.
The use of weights is essential to produce estimates that are representative of the cohort of
children who were in kindergarten in 2010–11. Main sampling weights should be used to produce survey
estimates. When testing hypotheses (e.g., conducting t tests, regression analyses, etc.) using weighted data
from a study such as the ECLS-K:2011 that has a complex design, analysts also should use methods to
adjust the standard errors. Two such methods are jackknife replication variance estimation and the Taylor
series linearization method. Replicate weights are provided in the data file for use with the paired
jackknife replication procedure, and PSU and stratum identifiers are provided for use with the Taylor
series method.
4-284.3.1 Types of Sample Weights
Main sampling weights designed for use with data from a complex sample survey serve two
primary purposes. When used in analyses, the main sampling weight weights the sample size up to the
population total of interest. In the ECLS-K:2011, weighting produces national-level estimates. Also, the
main sampling weight adjusts for differential nonresponse patterns that can lead to bias in the estimates. If
people with certain characteristics are systematically less likely than others to respond to a survey, the
collected data may not accurately reflect the characteristics and experiences of the nonrespondents, which
can lead to bias. To adjust for this, respondents are assigned weights that, when applied, result in
respondents representing their own characteristics and experiences as well as those of nonrespondents
with similar attributes.
A sample weight could be produced for use with data from every component of the study
(e.g., data from the fifth-grade parent interview; the fifth-grade child assessment and child questionnaire;
the fifth-grade teacher teacher-level questionnaire; the fifth-grade teacher child- and classroom-level
reading, mathematics, or science teacher questionnaire; or the fifth-grade school administrator
questionnaire) and for every combination of components for the study (e.g., data from the fifth-grade
child assessment with data from the fifth-grade school administrator questionnaire, or data from the
spring kindergarten child assessment with data from the fifth-grade child assessment or child
questionnaire and the fifth-grade parent interview). However, creating all possible weights for a study
with as many components as the ECLS-K:2011 would be impractical, especially as the study progresses
and the number of possible weights increases. In order to determine which weights would be most useful
for researchers analyzing data from fifth grade, completion rates for each fifth-grade component (e.g.,
response to the child assessment and child questionnaire, the parent interview, various parts of the teacher
questionnaire) were reviewed in combination with completion rates from the kindergarten, first-grade,
second-grade, third-grade, and fourth-grade years, and consideration was given to how analysts are likely
to use the data.
The best approach to choosing a sample weight for a given analysis is to select one that
maximizes the number of sources of data included in the analyses for which nonresponse adjustments are
made, which in turn minimizes bias in estimates, while maintaining as large an unweighted sample size as
possible. Exhibits 4-1 and 4-2 show the 21 weights computed for the analyses of fifth-grade data. It also
identifies the survey component(s), or sources of data, for which nonresponse adjustments are made for
each weight.
4-29Note that for five sets of weights involving the fifth-grade teacher data, separate weights
were computed for the analyses of the teacher child- and classroom-level reading, mathematics, and
science questionnaires. Analytic weights that adjust for nonresponse to the reading teacher questionnaire
apply to all children enrolled in school since they were all eligible for a reading teacher questionnaire. As
discussed above, half of the study children were eligible for a mathematics teacher questionnaire and half
were eligible for a science teacher questionnaire. Weights that adjust for nonresponse for each of these
questionnaires are not provided in separate mathematics and science weighting variables. Instead, the
mathematics and science weight values are combined in the same weight variables. To use weights
applicable only to the set of children selected for a mathematics teacher or only to the set of children
selected for a science teacher, the user needs to subset the data to a specific subject using the flag variable
X9MSFLAG. When analyzing information provided by the mathematics teacher, the user needs to subset
data to mathematics by setting the flag X9MSFLAG to 0. When analyzing data provided by science
teachers the user needs to subset the data to science by setting the flag X9MSFLAG to 1. When analyzing
data that include the reading teacher questionnaire, no subsetting is necessary.
Many of the weights that adjust for nonresponse to the reading teacher questionnaire have
parallel weights that adjust for nonresponse to the mathematics/science teacher questionnaires. However,
some weights that adjust for nonresponse to the reading teacher questionnaire do not have a similar
weight that has mathematics or science nonresponse adjustments. This is because the reading teacher
questionnaire contained child-level questions that were not included in the mathematics or science teacher
questionnaires. The mathematics and science questionnaires contained only a few child-level questions
specifically related to mathematics or science. The reading teacher questionnaire contained questions
related not only to reading but also to the child’s academic and social skills, classroom behaviors, and
peer relationships. To help users better understand the series of weights include nonresponse adjustments
for teacher data, those weights are presented separately in exhibit 4-2.
Since every child who was assessed also had child questionnaire data, the response rates
have the same pattern. Therefore, nonresponse adjustments for the child questionnaire did not need to be
made separately from nonresponse adjustments for the child assessment. Analyses that include either
child assessment data or child questionnaire data should be done with a weight that includes the C9
component.
4-30Exhibit 4-1. ECLS-K:2011 fifth-grade main sampling weights for analysis not including data from
teachers
Weight Description
W9C9P_2 W9C19P_2 W9C19P_9 W9C29P_9A W9C29P_9B W9C79 Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from spring fifth grade, and parent data from either fall
kindergarten or spring kindergarten
(C9)(P1_P2)
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from both kindergarten rounds and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten
(C1C2C9)(P1_P2)
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from both kindergarten rounds and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, spring first grade, spring
second grade, spring third grade, spring fourth grade, and spring fifth grade
(C1C2C9)(P1_P2)(P4P6P7P8P9)
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from spring kindergarten, spring first grade, spring second
grade, spring third grade, spring fourth grade, and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, and parent data from
spring fifth grade
(C2C4C6C7C8C9)(P1_P2)(P9)
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from spring kindergarten, spring first grade, spring second
grade, spring third grade, spring fourth grade, and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, spring first grade, spring
second grade, spring third grade, spring fourth grade, and spring fifth grade
(C2C4C6C7C8C9)(P1_P2)(P4P6P7P8P9)
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from spring third grade, spring fourth grade, and spring fifth
grade
(C7C8C9)
NOTE: Having child assessment/child questionnaire data includes (1) having reading and/or mathematics and/or science scores, (2) having at
least one executive function score, (3) having a height or weight measurement, or (4) being excluded from assessment due to lack of
accommodation for a disability. In spring fifth grade, every child who has questionnaire data was assessed. The weight designations (C1, C2, etc.)
use the same prefixes that are used for other variables in the kindergarten–fifth grade data file. The prefixes are listed in exhibit 7-1.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), kindergarten–fifth grade (K-5) restricted-use data file.
4-31Exhibit 4-2. ECLS-K:2011 fifth-grade main sampling weights associated with data from teachers
Weight Description
W9C19P_2T29 W9C19P_9T29A W9C19P_9T29B Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from both kindergarten rounds and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, and either
teacher-/classroom- or child-level teacher data from spring kindergarten (from a
core or supplemental teacher questionnaire), spring first grade (from a first-grade
or a kindergarten teacher questionnaire), spring second grade, and spring third
grade, and either reading teacher-/classroom- or child-level reading teacher data
from spring fourth grade and spring fifth grade
(C1C2C9)(P1_P2)(T2T4T6T7T8T9)
Note: This weight was created with nonresponse adjustments for the reading
teacher only. There is no similar weight with nonresponse adjustments for the
mathematics or science teacher.
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from both kindergarten rounds and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, parent data from fifth
grade, and either teacher-/classroom- or child-level teacher data from spring
kindergarten (from a core or supplemental teacher questionnaire), spring first grade
(from a first-grade or a kindergarten teacher questionnaire), spring second grade,
and spring third grade, and either reading teacher-/classroom- or child-level
reading teacher data from spring fourth grade and spring fifth grade
(C1C2C9)(P1_P2)(P9)(T2T4T6T7T8T9)
Note: This weight was created with nonresponse adjustments for the reading
teacher only. There is no similar weight with nonresponse adjustments for the
mathematics or science teacher.
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from both kindergarten rounds and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, spring first grade, spring
second grade, spring third grade, spring fourth grade, and spring fifth grade, and
either teacher-/classroom- or child-level teacher data from spring kindergarten
(from a core or supplemental teacher questionnaire), spring first grade (from a
first-grade or a kindergarten teacher questionnaire), spring second grade, and
spring third grade, and either reading teacher-/classroom- or child-level reading
teacher data from spring fourth grade and spring fifth grade
(C1C2C9)(P1_P2)(P4P6P7P8P9)(T2T4T6T7T8T9)
Note: This weight was created with nonresponse adjustments for the reading
teacher only. There is no similar weight with nonresponse adjustments for the
mathematics or science teacher.
See notes at end of exhibit.
4-32Exhibit 4-2. ECLS-K:2011 fifth-grade main sampling weights associated with data from teachers—
Continued
Weight Description
W9C29P_2T29 W9C19P_9T9 W9C19P_9T9Z Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from spring kindergarten, spring first grade, spring second
grade, spring third grade, spring fourth grade, and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, and either teacher-
/classroom- or child-level teacher data from spring kindergarten (from a core or
supplemental teacher questionnaire), spring first grade (from a first-grade or a
kindergarten teacher questionnaire), spring second grade, and spring third grade,
and either reading teacher-/classroom- or child-level reading teacher data from
spring fourth grade and spring fifth grade
(C2C4C6C7C8C9)(P1_P2)(T2T4T6T7T8T9)
Note: This weight was created with nonresponse adjustments for the reading
teacher only. There is no similar weight with nonresponse adjustments for the
mathematics or science teacher.
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from both kindergarten rounds and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, parent data from fifth
grade, and either reading teacher-/classroom- or child-level reading teacher data
from spring fifth grade
(C1C2C9)(P1_P2)(P9)(T9)
Note: This weight was created with nonresponse adjustments for the reading
teacher. The similar weight with nonresponse adjustments for the mathematics or
science teacher is W9C19P_9T9Z.
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from both kindergarten rounds and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, parent data from fifth
grade, and either mathematics/science teacher-/classroom- or child-level
mathematics/science teacher data from spring fifth grade
(C1C2C9)(P1_P2)(P9)(T9Z)
Note: Users must subset records to include cases with mathematics teacher data
only (X9MSFLAG=0) or science teacher data only (X9MSFLAG=1) when using
this weight.
See notes at end of exhibit.
4-33Exhibit 4-2. ECLS-K:2011 fifth-grade main sampling weights associated with data from teachers—
Continued
Weight Description
W9C19P_9T29C W9C19P_9T29Z W9C29P_9T9 Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from both kindergarten rounds and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, parent data from fifth
grade, and either teacher-/classroom- or child-level teacher data from spring
kindergarten (from a core or supplemental teacher questionnaire) and either
reading teacher-/classroom- or child-level reading teacher data from spring fifth
grade
(C1C2C9)(P1_P2)(P9)(T2T9)
Note: This weight was created with nonresponse adjustments for the reading
teacher. The similar weight with nonresponse adjustments for the mathematics or
science teacher is W9C19P_9T29Z.
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from both kindergarten rounds and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, parent data from fifth
grade, and either teacher-/classroom- or child-level teacher data from spring
kindergarten (from a core or supplemental teacher questionnaire) and either
mathematics/science teacher-/classroom- or child-level mathematics/science
teacher data from spring fifth grade
(C1C2C9)(P1_P2)(P9)(T2T9Z)
Note: Users must subset records to include cases with mathematics teacher data
only (X8MSFLAG=0) or science teacher data only (X8MSFLAG=1) when using
this weight.
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from spring kindergarten and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, parent data from fifth
grade, and either reading teacher-/classroom- or child-level reading teacher data
from spring fifth grade
(C2C9)(P1_P2)(P9)(T9)
Note: This weight was created with nonresponse adjustments for the reading
teacher. The similar weight with nonresponse adjustments for the mathematics or
science teacher is W9C29P_9T9Z.
See notes at end of exhibit.
4-34Exhibit 4-2. ECLS-K:2011 fifth-grade main sampling weights associated with data from teachers—
Continued
Weight Description
W9C29P_9T9Z W9C29P_2T9 W9C29P_2T9Z Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from spring kindergarten and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, parent data from fifth
grade, and either mathematics/science teacher-/classroom- or child-level
mathematics/science teacher data from spring fifth grade
(C2C9)(P1_P2)(P9)(T9Z)
Note: Users must subset records to include cases with mathematics teacher data
only (X9MSFLAG=0) or science teacher data only (X9MSFLAG=1) when using
this weight.
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from spring kindergarten, spring first grade, spring second
grade, spring third grade, spring fourth grade, and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, and either reading
teacher-/classroom- or child-level reading teacher data from spring fifth grade
(C2C4C6C7C8C9)(P1_P2)(T9)
Note: This weight was created with nonresponse adjustments for the reading
teacher. The similar weight with nonresponse adjustments for the mathematics or
science teacher is W9C29P_2T9Z.
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from spring kindergarten, spring first grade, spring second
grade, spring third grade, spring fourth grade, and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, and either
mathematics/science teacher-/classroom- or child-level mathematics/science
teacher data from spring fifth grade
(C2C4C6C7C8C9)(P1_P2)(T9Z)
Note: Users must subset records to include cases with mathematics teacher data
only (X9MSFLAG=0) or science teacher data only (X9MSFLAG=1) when using
this weight.
See notes at end of exhibit.
4-35Exhibit 4-2. ECLS-K:2011 fifth-grade main sampling weights associated with data from teachers—
Continued
Weight Description
W9C29P_9T29 W9C29P_9T29Z Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from spring kindergarten, spring first grade, spring second
grade, spring third grade, spring fourth grade, and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, spring first grade, spring
second grade, spring third grade, spring fourth grade, and spring fifth grade, and
either teacher-/classroom- or child-level teacher data from spring kindergarten
(from a core or supplemental teacher questionnaire), spring first grade (from a
first-grade or a kindergarten teacher questionnaire), spring second grade, and
spring third grade, and either reading teacher-/classroom- or child-level reading
teacher data from spring fourth grade and spring fifth grade
(C2C4C6C7C8C9)(P1_P2)(P4P6P7P8P9)(T2T4T6T7T8T9)
Note: This weight was created with nonresponse adjustments for the reading
teacher. The similar weight with nonresponse adjustments for the mathematics or
science teacher is W9C29P_9T29Z.
Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from spring kindergarten, spring first grade, spring second
grade, spring third grade, spring fourth grade, and spring fifth grade, as well as
parent data from fall kindergarten or spring kindergarten, spring first grade, spring
second grade, spring third grade, spring fourth grade, and spring fifth grade, and
either teacher-/classroom- or child-level teacher data from spring kindergarten
(from a core or supplemental teacher questionnaire), spring first grade (from a
first-grade or a kindergarten teacher questionnaire), spring second grade, and
spring third grade, and either mathematics/science teacher-/classroom- or child-
level mathematics/science teacher data from spring fourth grade and spring fifth
grade
(C2C4C6C7C8C9)(P1_P2)(P4P6P7P8P9)(T2T4T6T7T8ZT9Z)
Note: Users must subset records to include cases with mathematics teacher data
only (X9MSFLAG=0) or science teacher data only (X9MSFLAG=1) when using
this weight.
See notes at end of exhibit.
4-36Exhibit 4-2. ECLS-K:2011 fifth-grade main sampling weights associated with data from teachers—
Continued
Weight Description
W9C79P_9T79 Child base weight adjusted for nonresponse associated with child assessment/child
questionnaire data from spring third grade, spring fourth grade, and spring fifth
grade, as well as parent data from fall kindergarten or spring kindergarten, , spring
third grade, spring fourth grade, and spring fifth grade, and either
teacher-/classroom- or child-level teacher data from spring third grade, and either
reading teacher-/classroom- or child-level reading teacher data from spring fourth
grade and spring fifth grade
(C7C8C9)(P1_P2)(P7P8P9)(T7T8T9)
Note: This weight was created with nonresponse adjustments for the reading
teacher only. There is no similar weight with nonresponse adjustments for the
mathematics or science teacher.
NOTE: Having child assessment/child questionnaire data includes (1) having reading and/or mathematics and/or science scores, (2) having at
least one executive function score, (3) having a height or weight measurement, or (4) being excluded from assessment due to lack of
accommodation for a disability. In spring fifth grade, every child who has questionnaire data was assessed. The weight designations (C1, C2, etc.)
use the same prefixes that are used for other variables in the kindergarten–fifth grade data file. For the teacher nonresponse adjustments, T1
indicates adjustments for nonresponse associated with teacher/classroom- or child-level teacher data from the fall kindergarten data collection; T2
indicates adjustments for nonresponse associated with teacher/classroom- or child-level teacher data from a teacher questionnaire or supplemental
teacher questionnaire from the spring kindergarten data collection; T3 indicates adjustments for nonresponse associated with child-level teacher
data from the fall first-grade data collection; T4 indicates adjustments for nonresponse associated with teacher/classroom- or child-level teacher
data from a first-grade or a kindergarten teacher questionnaire in the spring first-grade data collection; T5 indicates adjustments for nonresponse
associated with child-level teacher data from the fall second-grade data collection; T6 indicates adjustments for nonresponse associated with
teacher/classroom- or child-level teacher data from the spring second-grade data collection; T7 indicates adjustments for nonresponse associated
with teacher/classroom- or child-level teacher data from the spring third-grade data collection; T8 when not paired with a “z” (T8) indicates
adjustments for nonresponse associated with reading teacher-/classroom- or child-level reading teacher data from the spring fourth-grade data
collection; and T8 when paired with a “z” (T8Z) indicates adjustments for nonresponse associated with mathematics/science teacher-/classroom-
or child-level mathematics/science teacher data from the spring fourth-grade data collection. T9 when not paired with a “z” (T9) indicates
adjustments for nonresponse associated with reading teacher-/classroom- or child-level reading teacher data from the spring fifth-grade data
collection; and T9 when paired with a “z” (T9Z) indicates adjustments for nonresponse associated with mathematics/science teacher-/classroom-
or child-level mathematics/science teacher data from the spring fifth-grade data collection.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), kindergarten–fifth grade (K-5) restricted-use data file.
Exhibit 4-3, which presents the same information as exhibits 4-1 and 4-2 but in matrix
format, was developed to further assist researchers in deciding which weight to use for analyses. In
exhibit 4-3, the components for which nonresponse adjustments are made for each weight are noted with
a “Yes.” Researchers should choose a weight that has a “Yes” in the column(s) for the source(s) of data
they are using in their analyses. The best weight would have a “Yes” for each and every source used and
only those sources. For example, if a researcher is conducting an analysis that includes fifth-grade child
assessment/child questionnaire data, and fall kindergarten or spring kindergarten parent interview data,
the weight W9C9P_20 should be used since it adjusts for nonresponse on all of those components (i.e.,
exhibit 4-3 shows a “Yes” in the fall kindergarten and spring kindergarten parent columns and the spring
fifth-grade child assessment/child questionnaire column; the italicized Yes indicates an “or” condition).
4-374-38
Exhibit 4-3. Weights developed for use with the ECLS-K:2011 fifth-grade data, by components for which nonresponse adjustments
were made: Spring 2016
Fall
Spring
Spring
Spring
Spring
Spring
Spring
kindergarten
kindergarten
first grade
second grade
third grade
fourth grade
fifth grade
Weight
C1 P1 T11 C2 P2 T22 C4 P4 T43 C6 P6 T64 C7 P7 T75 C8 P8 T86 C9 P9 T97
W9C9P_20 † Yes † † Yes † † † † † † † † † † † † † Yes † †
W9C19P_20 Yes Yes † Yes Yes † † † † † † † † † † † † † Yes † †
W9C19P_90 Yes Yes † Yes Yes † † Yes † † Yes † † Yes † † Yes † Yes Yes †
W9C29P_9A0 † Yes † Yes Yes † Yes † † Yes † † Yes † † Yes † † Yes Yes †
W9C29P_9B0 † Yes † Yes Yes † Yes Yes † Yes Yes † Yes Yes † Yes Yes † Yes Yes †
W9C19P_2T290 Yes Yes † Yes Yes Yes † † Yes † † Yes † † Yes † † Yes Yes † Yes
W9C19P_9T29A0 Yes Yes † Yes Yes Yes † † Yes † † Yes † † Yes † † Yes Yes Yes Yes
W9C19P_9T29B0 Yes Yes † Yes Yes Yes † Yes Yes † Yes Yes † Yes Yes † Yes Yes Yes Yes Yes
W9C29P_2T290 † Yes † Yes Yes Yes Yes † Yes Yes † Yes Yes † Yes Yes † Yes Yes † Yes
W9C19P_9T90 Yes Yes † Yes Yes † † † † † † † † † † † † † Yes Yes Yes
W9C19P_9T9Z08 Yes Yes † Yes Yes † † † † † † † † † † † † † Yes Yes Yes
W9C19P_9T29C0 Yes Yes † Yes Yes Yes † † † † † † † † † † † † Yes Yes Yes
W9C19P_9T29Z08 Yes Yes † Yes Yes Yes † † † † † † † † † † † † Yes Yes Yes
W9C29P_9T90 † Yes † Yes Yes † † † † † † † † † † † † † Yes Yes Yes
W9C29P_9T9Z08 † Yes † Yes Yes † † † † † † † † † † † † † Yes Yes Yes
W9C29P_2T90 † Yes † Yes Yes † Yes † † Yes † † Yes † † Yes † † Yes † Yes
W9C29P_2T9Z08 † Yes † Yes Yes † Yes † † Yes † † Yes † † Yes † † Yes † Yes
W9C29P_9T290 † Yes † Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes
W9C29P_9T29Z08 † Yes † Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes
W9C790 † † † † † † † † † † † † Yes † † Yes † † Yes † †
W9C79P_9T790 † Yes † † † Yes † † † † † † † † † Yes Yes Yes Yes Yes Yes Yes Yes Yes
† Not applicable.
1 A case had to have either teacher/classroom- or child-level teacher data from the fall kindergarten data collection to have a valid weight.
2 A case had to have either teacher/classroom- or child-level teacher data from a teacher questionnaire or supplemental teacher questionnaire from the spring kindergarten data collection to have a valid
weight.
3 A case had to have either teacher/classroom- or child-level teacher data from a first-grade or a kindergarten teacher questionnaire in the spring first-grade data collection to have a valid weight.
4 A case had to have either teacher/classroom- or child-level teacher data from the spring second-grade data collection to have a valid weight.
5 A case had to have either teacher/classroom- or child-level teacher data from the third-grade data collection to have a valid weight.
6 A case had to have either teacher/classroom- or child-level teacher data from the fourth-grade data collection to have a valid weight.
7 A case had to have either teacher/classroom- or child-level teacher data from the fifth-grade data collection to have a valid weight.
8 This weight is for the analysis of data that include the mathematics/science teacher/classroom or child-level mathematics/science teacher data from the fifth grade.
NOTE: C indicates child assessment/child questionnaire data. P indicates parent interview data. T indicates teacher data. “Yes” indicates that the weight includes nonresponse adjustments for that
component. An italicized Yes indicates an “or” condition.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), kindergarten–fifth grade (K-5)
restricted-use data file.However, for many analyses, there will be no weight that adjusts for nonresponse to all the
sources of data that are included and for only those sources. When no weight corresponds exactly to the
combination of components included in the desired analysis, researchers might prefer to use a weight that
includes nonresponse adjustments for more components than they are using in their analysis (i.e., a weight
with “Yes” in columns corresponding to components that are not included in their analyses) if that weight
also includes nonresponse adjustments for the components they are using. Although such a weight may
result in a smaller analytic sample than would be available when using a weight that corresponds exactly
to the components from which the analyst is using data, it will adjust for the potential differential
nonresponse associated with the components. If researchers instead choose a weight with nonresponse
adjustments for fewer components than they are using in their analysis, missing data should be examined
for potential bias.
4.3.2 Computation of Sample Weights
To compute sample weights, first a base weight is computed to reflect the sample design,
and then the base weight is adjusted for nonresponse and unknown eligibility. When there is an
intermediate adjustment (e.g., a mover subsampling adjustment), it is the intermediate weight that is
adjusted for nonresponse and not the base weight.
The nonresponse adjustment was computed as the sum of the base weights for all eligible
units in a nonresponse class divided by the sum of the base weights of the respondent units in that
nonresponse class. Nonresponse classes were formed separately for students in each type of school
(public/Catholic/non-Catholic private). Within school type, analysis of school response propensity was
done using school characteristics such as census region, locale, school enrollment size, and percent
minority in school.9 Nonresponse classes were created based on this analysis of response propensity.
Similarly, student characteristics such as sex and race/ethnicity were used to analyze response propensity
and create nonresponse classes. Rules for collapsing nonresponse adjustment cells were adopted; for
example, cells had to have a maximum adjustment factor of 2 and a minimum cell size of 30.
Main sampling weights (indicated by the suffix 0) and replicate weights (indicated by the
suffix 1 to 80) were computed and included in the data file. In the sections that follow, only the main
sampling weight is discussed, but any adjustment done to the main sampling weight was done to the
replicate weights as well.
9 This was part of the school nonresponse adjustment that was done in the base year.
4-394.3.2.1 Student Base Weights
Only base-year respondents were eligible to participate in the fifth-grade data collection. The
fifth-grade student base weight is the base-year student base weight adjusted for base-year nonresponse.
The adjustment factor for base-year nonresponse is the sum of the base weights of the eligible students in
the base year divided by the sum of the base weights of the base-year respondents within nonresponse
adjustment classes.10 For a description of the computation of the base-year student base weights, see
section 4.2.2.3.1 of the base-year User’s Manual.
For weights needed to analyze the child-level mathematics or science data from their
teachers, a separate base weight was computed to account for the sampling of children to have
mathematics or science teacher data. Only half of the students were selected for the mathematics teacher
questionnaire, and the other half for the science teacher questionnaire. Because selection was with equal
probability, the base-year student base weight was multiplied by 2 to get the mathematics/science base
weight which was then adjusted for base-year nonresponse.
4.3.2.2 Student Weights Adjusted for Mover Subsampling
The student base weight described in section 4.3.2.1 was adjusted to reflect the subsampling
of movers described in section 4.2.3. For every student who is a base-year respondent, a “follow” flag
was assigned a value of 0 (do not follow if student moves) or 1 (follow if student moves). A mover-
subsampling adjustment factor was set to 1 if the student has never moved out of an original sampled
school, 2 if the student moved out of the original sampled school at any time after the base year and was
followed into his or her new school, and 0 if the student moved out of the original sampled school at any
time after the base year and was not followed. The mover-subsampling adjusted weight is the product of
the base weight described in section 4.3.2.1 and this mover-subsampling adjustment factor. Note that
child assessments were not conducted and school staff questionnaires were not fielded for students who
moved into nonsampled PSUs even if their flag was set to “follow”; such students are counted as
nonrespondents in the adjustment for nonresponse on weights involving child assessment or teacher
data.11 However, an attempt was made to complete a parent interview for students who moved into
nonsampled PSUs if their flag was set to “follow”; therefore, their parents would be counted as
10 A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to
lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year.
11 Only homeschooled children were considered “not eligible” for the collection of teacher data; they are the only students who were not included
in the adjustment for nonresponse for teacher data.
4-40respondents in the adjustment for parent nonresponse if a parent interview was completed and as
nonrespondents if a parent interview was not completed.
4.3.2.3 Student Nonresponse-Adjusted Weights
The mover-subsampling adjusted weight described in section 4.3.2.2 was adjusted for
nonresponse to produce each of the student-level weights described in exhibit 4-1. For each weight, a
response status was defined based on the presence of data for the particular component(s) and round(s)
covered by the weight.
For example, for the weight W9C9P_20, an eligible respondent is a base-year respondent
who satisfies both of these criteria: (1) the student has child assessment/child questionnaire data12 from
fifth grade, and (2) the student has parent interview data from either the fall or spring of kindergarten. An
ineligible student is one who moved out of the country or is deceased or moved to another school and was
not assigned to be followed. A student of unknown eligibility is one who could not be located. The
remaining students are eligible nonrespondents.
Nonresponse adjustment was done in two steps: (1) adjustment for children whose eligibility
was not determined (i.e., those who could not be located, or those who moved to another sampled PSU
and who did not have parent interview data because the parent could not be contacted); and (2)
adjustment for eligible nonrespondents. In the first step, a portion of cases with unknown eligibility was
assumed to be ineligible. This proportion varied between 1.1 and 2.1 percent for the weights that do not
include data from the fall collections, and between 1.6 and 3 percent for the weights that include data
from the fall collections; it is highest for those weights that adjusted for teacher nonresponse. The latter is
because children who were homeschooled were considered not eligible to have teacher data. Nonresponse
classes were created using school and child characteristics and used in adjustments for both unknown
eligibility and nonresponse.
4.3.2.4 Raking to Sample Control Totals
To reduce the variability due to the subsampling of movers and to ensure that the final
weights continue to sum to the base-year population total, the student nonresponse-adjusted weights were
12 Having child assessment data includes (1) having reading and/or mathematics and/or science scores, (2) having at least one executive function
score, (3) having a height or weight measurement, or (4) being excluded from assessment due to lack of accommodation for a disability.
4-41raked to sample-based control totals using the fifth-grade student base weights. Raking is a calibration
estimator that is closely related to poststratification. The poststratification adjustment procedure involves
applying a ratio adjustment to the weights. Respondents are partitioned into groups, known as poststrata
cells, and a single ratio adjustment factor is applied to the weights of all units in a given poststratification
cell. The numerator of the ratio is a “control total” usually obtained from a secondary source; the
denominator is a weighted total for the survey data. Therefore at the poststratum level, estimates obtained
using the poststratified survey weights will correspond to the control totals used. If either the cell-level
population counts are not available for all cells or the majority of the cell sample sizes are too small,
raking is used to adjust the survey estimates to the known marginal totals of several categorical variables.
Raking is essentially a multivariate poststratification. In the ECLS-K:2011, multiple background
characteristics from schools, students, and parents were combined to create raking cells.
The student records included in the file used for computing the control totals are records of
base-year eligible children. The sum of the base weights from this file is the estimated number of children
who were in kindergarten in 2010–11. Raking was done within raking cells (also known as raking
dimensions). The raking dimensions were based on single characteristics (e.g., locale) or a combination of
characteristics (e.g., age and race/ethnicity). Chi-Square Automatic Interaction Detector (CHAID)
analysis was used to determine the best set of raking cells.
The final weight is the product of the raking factor and the student nonresponse-adjusted
weight. The raking factor was computed as the ratio of the base-year sample control total for a raking cell
over the sum of the nonresponse-adjusted fifth-grade weights in that raking cell.
4.3.3 Characteristics of Sample Weights
The statistical characteristics of the sample weights are presented in table 4-19. For each
weight, the number of cases with a nonzero weight is presented along with the mean weight, the standard
deviation, the coefficient of variation (i.e., the standard deviation as a percentage of the mean weight), the
minimum weight, the maximum weight, the design effect of the final weight, the skewness, the kurtosis,
and the sum of weights. The procedure for raking to control totals included respondents and ineligible
cases. Afterwards, weights of ineligible cases were set to zero. Because a portion of children of unknown
eligibility was assumed to be ineligible (as discussed in section 4.3.2.3) and this adjustment for unknown
eligibility was done within adjustment cells, there are small differences in the sums of weights.
4-42Table 4-19. Characteristics of the fifth-grade weights: Spring 2016
Weight
Number
of cases Mean
Standard
deviation
CV
(× 100) Minimum Maximum
DEFF
of the final
weight Skewness Kurtosis Sum
W9C9P_20 10,472 380.37 257.09 67.59 26.49 2,215.92 1.46 2.45 9.24 3,983,239.68
W9C19P_20 9,227 431.56 289.82 67.16 31.07 2,365.28 1.45 2.37 8.50 3,981,963.70
W9C19P_90 6,556 606.87 448.22 73.86 45.81 3,436.25 1.55 2.33 7.89 3,978,623.64
W9C29P_9A0 8,542 466.32 318.20 68.24 38.90 2,808.42 1.47 2.36 8.06 3,983,299.06
W9C29P_9B0 7,191 553.47 414.35 74.86 46.51 3,360.17 1.56 2.51 9.14 3,980,023.59
W9C19P_2T290 7,326 537.25 365.33 68.00 37.38 3,303.32 1.46 2.38 9.44 3,935,882.85
W9C19P_9T29A0 6,227 631.36 428.63 67.89 31.73 3,525.92 1.46 2.23 7.69 3,931,508.24
W9C19P_9T29B0 5,320 739.24 568.13 76.85 51.89 4,202.70 1.59 2.21 6.77 3,932,781.24
W9C29P_2T290 7,973 493.88 334.52 67.73 33.30 2,963.51 1.46 2.35 8.76 3,937,697.71
W9C19P_9T90 7,182 548.50 379.61 69.21 39.60 3,331.35 1.48 2.45 9.77 3,939,360.62
W9C19P_9T9Z01 3,559 1,107.03 779.31 70.40 95.15 6,075.95 1.50 2.44 8.36 3,939,934.65
W9C19P_9T9Z02 3,622 1,085.46 765.27 70.50 79.29 5,770.20 1.50 2.38 7.94 3,931,538.13
W9C19P_9T29C0 6,933 568.08 393.66 69.30 39.10 3,420.28 1.48 2.35 8.86 3,938,478.11
W9C19P_9T29Z03 3,435 1,146.77 804.16 70.12 96.96 6,451.27 1.49 2.42 8.26 3,939,172.03
W9C19P_9T29Z04 3,497 1,124.09 791.95 70.45 68.32 6,092.04 1.50 2.28 7.43 3,930,933.78
W9C29P_9T90 7,960 494.86 345.31 69.78 41.85 2,995.05 1.49 2.53 9.96 3,939,125.03
W9C29P_9T9Z05 3,947 998.17 712.43 71.37 97.10 5,586.97 1.51 2.52 8.78 3,939,775.65
W9C29P_9T9Z06 4,017 978.67 686.57 70.15 78.19 5,117.29 1.49 2.45 8.35 3,931,321.85
W9C29P_2T90 9,219 427.23 290.24 67.93 27.41 2,431.65 1.46 2.42 8.86 3,938,673.73
W9C29P_2T9Z07 4,589 858.80 584.92 68.11 101.22 4,647.78 1.46 2.52 8.89 3,941,030.63
W9C29P_2T9Z08 4,633 848.74 585.86 69.03 50.96 4,385.07 1.48 2.41 7.96 3,932,214.68
W9C29P_9T290 5,792 679.50 514.73 75.75 41.30 3,747.39 1.57 2.17 6.45 3,935,669.02
W9C29P_9T29Z09 2,874 1,370.52 1,035.28 75.54 131.95 7,069.77 1.57 2.22 6.58 3,938,887.01
W9C29P_9T29Z010 2,921 1,344.06 1,016.20 75.61 81.12 6,825.56 1.57 2.10 5.98 3,926,012.20
W9C790 11,373 350.56 228.31 65.13 16.64 2,109.55 1.42 2.51 10.17 3,986,950.03
W9C79P_9T790 6,945 566.88 406.23 71.66 40.02 3,378.47 1.51 2.42 8.99 3,936,984.03
1 This is the same weight as W9C19P_9T90 but for cases where X9MSFLAG=0 (i.e., mathematics).
2 This is the same weight as W9C19P_9T90 but for cases where X9MSFLAG=1 (i.e., science).
3 This is the same weight as W9C19P_9T29C0 but for cases where X9MSFLAG=0 (i.e., mathematics).
4 This is the same weight as W9C19P_9T29C0 but for cases where X9MSFLAG=1 (i.e., science).
5 This is the same weight as W9C29P_9T90 but for cases where X9MSFLAG=0 (i.e., mathematics).
6 This is the same weight as W9C29P_9T90 but for cases where X9MSFLAG=1 (i.e., science).
7 This is the same weight as W9C29P_2T90 but for cases where X9MSFLAG=0 (i.e., mathematics).
8 This is the same weight as W9C29P_2T90 but for cases where X9MSFLAG=1 (i.e., science).
9 This is the same weight as W9C29P_9T290 but for cases where X9MSFLAG=0 (i.e., mathematics).
10 This is the same weight as W9C29P_9T290 but for cases where X9MSFLAG=1 (i.e., science).
NOTE: CV is the coefficient of variation. DEFF is the design effect.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), kindergarten–fifth grade (K-5) restricted-use data file.
A simple random sample (SRS) is completely self-weighting (i.e., no weights are necessary
to produce estimates from this sample). In the ECLS-K:2011, the sample design is multistaged, with
nonresponse encountered at both school and student levels. Weighting adjustments were necessary, but
they tend to increase the variance of the estimates. As described in section 4.3, the design effect
(DEFF)—defined as the ratio of the variance estimate under the actual sample design to the variance
4-43estimate obtained with an SRS of the same sample size—shows an estimate of the variance increase. One
way of approximating this increase due to weighting is by way of the coefficient of variation (CV):
𝐷𝐷𝐷𝐷𝐷𝐷𝐷𝐷 𝑑𝑑𝑑𝑑𝑑𝑑 𝑡𝑡𝑡𝑡 𝑤𝑤𝑑𝑑𝑤𝑤𝑤𝑤ℎ𝑡𝑡𝑤𝑤𝑡𝑡𝑤𝑤 = 1 + 𝐶𝐶𝐶𝐶2
In table 4-19, the design effect due to weighting is included for each weight. For example,
for weight W9C9P_20, the design effect due to weighting is 1+(0.6759)2 = 1.46 (i.e., the variance is
increased by 46 percent due to weight adjustments). The design effect due to weighting varies between
1.42 and 1.59.
4.3.4 Variance Estimation
The precision of the sample estimates derived from a survey can be evaluated by estimating
the variances of these estimates. For a complex sample design such as the one employed in the
ECLS-K:2011, replication and Taylor Series methods have been developed to correctly estimate variance.
These methods take into account the clustered, multistage sampling design and the use of differential
sampling rates to oversample targeted subpopulations. For the ECLS-K:2011, in which the first-stage
self-representing sampling units (i.e., PSUs) were selected with certainty and the first-stage non-self-
representing sampling units were selected with two units per stratum, the paired jackknife replication
method (JK2) is recommended. This section describes the JK2 and the Taylor series methods, which can
be used to compute correct standard errors for any analysis.
4.3.4.1 Jackknife Method
The final main sampling and replicate weights can be used to compute estimates of variance
for survey estimates using the jackknife method with two PSUs per stratum (JK2) using several software
packages, including WesVar, AM, SUDAAN, SAS, Stata, and R. In the jackknife method, each survey
estimate of interest is calculated for the full sample as well as for each of the g replicates, where g is 80
for the spring weights, and 40 for the fall weights.
13 The variation of the replicate estimates around the
full-sample estimate is used to estimate the variance for the full sample. The variance estimator is
computed as the sum of squared deviations of the replicate estimates from the full sample estimate:
13 The values of g (80 for spring weights and 40 for fall weights) indicate the degrees of freedom possible when using these weights in analysis.
4-44where θ is the survey estimate of interest,
is the estimate of θ based on the full sample,
G is the number of replicates, and
(g) is the gth replicate estimate of θ based on the observations included in the gth replicate.
Each main sampling weight that does not include adjustments for nonresponse to
components from the fall data collections has 80 corresponding replicate weights for use with the JK2
method. The replicate weights begin with the same characters as the main sampling weight and end with
the numbers 1 to 80. For example, the replicate weights corresponding to weight W9C9P_20 are
W9C9P_21 through W9C9P_280.
4.3.4.2 Taylor Series Method
Variance stratum and variance unit (first-stage sample unit [i.e., PSU]) identifiers were also
created to be used in statistical software that computes variance estimates based on the Taylor series
method (for example, AM, SUDAAN, SAS, SPSS, and Stata). In this method, a linear approximation of a
statistic is formed and then substituted into the formula for calculating the variance of a linear estimate
appropriate for the sample design.
If Y Y= (1,...,Yp )'
denotes a p-dimensional vector of population parameters,
is the corresponding vector of estimators based on a sample s of size n(s), parameter of interest, and is an estimator of θ, then
ˆ ˆ
Y Y= (1,...,
is the population
ˆ
Yp )'
and
where is the estimate of θ based on the full sample,
θ is the survey estimate of interest,
Y is a p-dimensional vector of population parameters,
is a p-dimensional vector of estimators,
y is an element of the vector Y,
i is 1,…, p,
j is 1,…, p, and
𝑤𝑤(𝑌𝑌) is an estimator of θ.
4-45The Taylor series method relies on a simplified procedure for estimating the variance for a
linear statistic even with a complex sample design and is valid when analyzing data from large samples in
which the first-stage units are sampled with replacement.14 The stratum and first-stage unit identifiers
needed to use the Taylor series method were assigned as follows: all independent sampling strata were
numbered sequentially from 1 to h; within each sampling stratum, first-stage sampling units were
numbered from 1 to nh. Care was taken to ensure that there were at least two responding units in each
stratum. For instances in which a stratum did not have at least two responding units, the stratum was
combined with an adjacent stratum. Stratum and first-stage unit identifiers are provided in the data file.
Each main sampling weight has corresponding stratum and PSU identifiers for use with the Taylor series
method. The stratum and PSU identifiers begin with the same characters as the main sampling weight and
end with either STR or PSU. For example, the stratum and PSU identifiers corresponding to weight
W9C9P_20 are W9C9P_2STR and W9C9P_2PSU, respectively.
4.3.4.3 Specifications for Computing Standard Errors
For the jackknife replication method, the main sampling weight, the replicate weights, and
the method of replication must be specified. All analyses of the ECLS-K:2011 data using the replication
method should be done using JK2. As an example, an analyst using the main sample weight W9C9P_20
to compute child-level estimates of mean reading scores for fifth grade would need to specify W9C9P_20
as the main sampling weight, W9C9P_21 to W9C9P_280 as the replicate weights, and JK2 as the method
of replication.
For the Taylor series method, the main sampling weight, the sample design, the nesting
stratum, and PSU variables must be specified. As an example, an analyst using the main sample weight
W9C9P_20 to compute child-level estimates of mean reading scores for fifth grade must specify the main
sampling weight (W9C9P_20), the stratum variable (W9C9P_2STR), and the PSU variable
(W9C9P_2PSU). The “with replacement” sample design option, WR, must also be specified if using
SUDAAN.
14 For the ECLS-K:2011, the sample of PSUs was selected using the Durbin method. In this method, two PSUs were selected per stratum without
replacement with probability proportional to size and known joint probability of inclusion in such a way to allow variances to be estimated as if
the units had been selected with replacement.
4-464.3.5 Use of Design Effects
An important analytic device is to compare the statistical efficiency of survey estimates from
a complex sample survey such as the ECLS-K:2011 with what would have been obtained in a
hypothetical and usually impractical SRS of the same size. In a stratified clustered design, stratification
generally leads to a gain in efficiency over simple random sampling, but clustering has the opposite effect
because of the positive intracluster correlation of the units in the cluster. The basic measure of the relative
efficiency of the sample is the DEFF, defined as the ratio, for a given statistic, of the variance estimate
under the actual sample design to the variance estimate that would be obtained with an SRS of the same
sample size:
DEFF
VAR
DESIGN
VAR= .
SRS
The root design effect (DEFT) is the square root of the design effect:
SE
DESIGN
DEFT DEFF
SE= =
SRS
where SE is the standard error of the estimate.
As discussed above, jackknife replication and Taylor Series can be used to compute more
precise standard errors for data from complex surveys. If statistical analyses are conducted using software
packages that assume the data were collected using simple random sampling (i.e., adjustments are not
made using jackknife replication or the Taylor series method), the standard errors will be calculated under
this assumption and will be incorrect. They can be adjusted using the average DEFT, although this
method is less precise than JK2 or Taylor series.15 The standard error of an estimate under the actual
sample design can be approximated as the product of the DEFT and the standard error assuming simple
random sampling.
In the ECLS-K:2011, a large number of data items were collected from children, parents,
teachers, school administrators, and before- and after-school care providers. Each item has its own design
effect that can be estimated from the survey data. Standard errors and design effects are presented in the
tables below for selected items from the study to allow analysts to see the range of standard errors and
15 Common procedures in SAS, SPSS, and Stata assume simple random sampling. Data analysts should use the SURVEY procedure (SAS), the
Complex Samples module (SPSS), or the SVY command (Stata) to account for complex samples.
4-47design effects for the study variables. They were computed using the paired jackknife replication method
in the statistical software package WesVar.
However, as discussed in section 4.3.4, not all statistical analysis software packages have
procedures to compute the variance estimate or standard error using the replication method, and some analysts
may not have access to software packages that do have such procedures. In such situations the correct variance
estimate or standard error can be approximated using the design effect or the root design effect.
As the first step in the approximation of a standard error, the analyst should normalize the
overall sample weights for packages that use the weighted population size (N) in the calculation of
standard errors. The normalized weight will sum to the sample size (n) and is calculated as
𝑛𝑛
𝑡𝑡𝑡𝑡𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑛𝑤𝑤𝑛𝑛𝑑𝑑𝑑𝑑 𝑤𝑤𝑑𝑑𝑤𝑤𝑤𝑤ℎ𝑡𝑡= 𝑤𝑤𝑑𝑑𝑤𝑤𝑤𝑤ℎ𝑡𝑡 ×
𝑁𝑁
where n is the sample size (i.e., the number of cases with a valid main sampling weight) and N is the sum
of weights. See table 4-19 for the sample size n and the sum of weights N.
As the second step in the approximation, the standard errors produced by the statistical
software, the test statistics, or the sample weight used in analysis can be adjusted to reflect the actual
complex design of the study. To adjust the standard error of an estimate, the analyst should multiply the
standard error produced by the statistical software by the square root of the DEFF or the DEFT as follows:
SE DEFF VAR DEFT SE = × = ×
DESIGN SRS SRS
A standard statistical analysis package can be used to obtain VARSRS and SESRS. The DEFF
and DEFT used to make adjustments can be calculated for specific estimates, can be the median DEFF
and DEFT across a number of variables, or can be the median DEFF and DEFT for a specific subgroup in
the population.
Adjusted standard errors can then be used in hypothesis testing, for example, when
calculating t and F statistics. A second option is to adjust the t and F statistics produced by statistical
software packages using unadjusted (i.e., SRS) standard errors. To do this, first conduct the desired
analysis weighted by the normalized weight and then divide a t statistic by the DEFT or divide an F
statistic by the DEFF. A third alternative is to create a new analytic weight variable in the data file by
dividing the normalized analytic weight by the DEFF and using the adjusted weight in the analyses.
4-48Table 4-20 shows estimates, standard errors, and design effects for 58 means and proportions
selected from the fifth-grade data collection. Table 4-21 shows the median design effects for the same items
but for subgroups. For each survey item, table 4-20 presents the number of cases for which data are
nonmissing, the estimate, the standard error taking into account the actual sample design (Design SE), the
standard error assuming SRS (SRS SE), the root design effect (DEFT), and the design effect (DEFF). Standard
errors (Design SE) were produced in WesVar using JK2 based on the actual ECLS-K:2011 complex design.
For each survey item, the variable name as it appears in the data file is also provided in the table.
Table 4-20. Standard errors and design effects for selected survey items, fifth grade: Spring 2016
Survey item Variable n Estimate SE SESRS DEFT DEFF
Scores (mean)1, 2
Mathematics scale score X9MSCALK5 10,390 119.45 0.380 0.170 2.234 4.992
Reading scale score X9RSCALK5 10,391 136.26 0.293 0.151 1.945 3.784
Science scale score X9SSCALK5 10,385 73.38 0.284 0.125 2.274 5.172
Mathematics theta score X9MTHETK5 10,390 1.83 0.010 0.005 2.215 4.908
Reading theta score X9RTHETK5 10,391 1.45 0.007 0.004 1.957 3.829
Science theta score X9STHETK5 10,385 1.87 0.015 0.007 2.257 5.096
Difference in mathematics scale score between
X9MSCALK5 –
10,309 7.11 0.121 0.077 1.577 2.488
spring fourth grade and spring fifth grade
X8MSCALK5
Difference in reading scale score between
X9RSCALK5 –
10,305 7.05 0.126 0.080 1.581 2.499
spring fourth grade and spring fifth grade
X8RSCALK5
Difference in science scale score between
X9SSCALK5 –
10,298 6.64 0.112 0.069 1.620 2.626
spring fourth grade and spring fifth grade
X8SSCALK5
Difference in mathematics theta score between
X9MTHETK5 –
10,309 0.19 0.003 0.002 1.598 2.553
spring fourth grade and spring fifth grade
X8MTHETK5
Difference in reading theta score between
X9RTHETK5 –
10,305 0.17 0.003 0.002 1.655 2.739
spring fourth grade and spring fifth grade
X8RTHETK5
Difference in science theta score between
X9STHETK5 –
10,298 0.34 0.006 0.004 1.663 2.765
spring fourth grade and spring fifth grade
X8STHETK5
Approaches to Learning-Teacher X9TCHAPP 7,899 3.11 0.010 0.008 1.303 1.697
Externalizing Problem Behaviors -Teacher X9TCHEXT 7,870 1.63 0.008 0.006 1.273 1.620
Internalizing Problem Behaviors -Teacher X9TCHINT 7,817 1.57 0.009 0.006 1.462 2.137
Interpersonal Skills -Teacher X9TCHPER 7,755 3.13 0.012 0.008 1.572 2.472
Self-control -Teacher X9TCHCON 7,774 3.29 0.011 0.007 1.535 2.356
Student characteristics from parent interview
(percent)3
Parent is currently married/in civil union/in
domestic partnership
At least one parent has a high school diploma
or equivalent
Child cares for self Child participated in organized athletic
activities
P9CURMAR 8,523 70.90 0.921 0.492 1.872 3.505
X9PAR1ED_I,
8,542 91.79 0.480 0.297 1.617 2.615
X9PAR2ED_I
P9SELFCA 8,042 9.78 0.583 0.331 1.759 3.095
P9ATHLET 8,129 61.51 0.844 0.539 1.565 2.448
Child participated in performing arts programs P9PERFRM 8,117 23.04 0.525 0.468 1.123 1.261
See notes at end of table.
4-49Table 4-20. Standard errors and design effects for selected survey items, fifth grade: Spring 2016—
Continued
Survey item Variable n Estimate SE SESRS DEFT DEFF
Student characteristics from parent interview
(percent)3
—Continued
Child has art classes or lessons Parent volunteered at school Parent used computer to get information from
school
Often or sometimes true that parent could not
afford balanced meals in last 12 months
P9ARTLSN 8,119 11.55 0.417 0.355 1.175 1.380
P9VOLSCH 8,500 44.01 1.325 0.538 2.461 6.056
P9CMPSCH 8,504 83.31 0.914 0.404 2.261 5.112
P9BLMEAL 7,854 8.03 0.444 0.307 1.446 2.092
Student characteristics from teacher
questionnaire (percent)2
Teacher took course to address using
A9DATRD 7,857 68.36 1.266 0.525 2.412 5.819
assessment data for teaching reading
Teacher has regular or standard state certificate
A9STATCT 7,872 89.86 0.786 0.340 2.310 5.336
or advanced professional certificate
Teacher has bachelor’s degree or higher A9HGHSTD 7,899 99.91 0.051 0.034 1.501 2.253
Teacher agreed/strongly agreed that school
A9ENCOUR 7,898 82.60 0.998 0.426 2.341 5.479
administrator was encouraging of staff
Teacher agreed/strongly agreed that child
A9MISBHV 7,885 26.44 1.052 0.497 2.118 4.487
misbehavior interfered with teaching
More than 50 percent of parents volunteered
A9REGHLP 7,833 8.01 0.550 0.307 1.794 3.219
regularly
Student reading skills were below grade level
G9RTREAD 7,880 24.34 0.807 0.484 1.669 2.785
as rated by reading teacher
Student received individual tutoring in
G9TTRRD 7,851 23.57 0.898 0.479 1.875 3.516
reading/language arts
Parent was very involved at the school G9PARIN 7,865 26.39 0.875 0.497 1.760 3.098
Student was in program to learn English skills G9PRGES 1,416 37.06 2.847 1.283 2.218 4.921
Student usually worked to best ability in math M9BESABL 3,895 50.05 1.198 0.801 1.495 2.236
Student math skills were below grade level as
M9RTMAT 3,896 23.25 0.950 0.677 1.404 1.970
rated by math teacher
Student solved math problems in small groups
M9PRBGRP 3,891 60.82 1.397 0.783 1.785 3.187
almost every day
Student used computer for math almost every
M9COMPMT 3,884 23.31 1.385 0.679 2.041 4.166
day
Student usually worked to best ability in
N9BESABL 3,969 51.48 0.930 0.794 1.172 1.373
science
Student science skills were below grade level as
N9RTSCI 3,970 19.43 0.803 0.628 1.278 1.633
rated by science teacher
Student worked with others on science project
N9SCIPRJ 3,956 15.20 1.390 0.571 2.435 5.931
almost every day
Student used science equipment almost every
N9SCIEQP 3,962 6.96 0.820 0.404 2.029 4.118
day
See notes at end of table.
4-50Table 4-20. Standard errors and design effects for selected survey items, fifth grade: Spring 2016—
Continued
Survey item Variable n Estimate SE SESRS DEFT DEFF
School characteristics from school
administrator questionnaire (percent)2
Taught classroom programs provided by school
S9CLASPR 7,488 97.66 0.579 0.175 3.315 10.992
at least once a year
School had staff in computer technology S9CTECYN 7,447 76.01 2.001 0.495 4.044 16.354
School used electronic communication with
S9ELECOM 7,509 39.11 2.036 0.563 3.616 13.076
parents several times a month
School used Response to Intervention S9RTLUSE 7,418 83.99 1.759 0.426 4.131 17.062
Received Title I funding S9TT1 6,695 71.80 2.099 0.550 3.816 14.564
Bullying happened on occasion S9BULLY 7,473 72.32 1.467 0.518 2.834 8.029
Crime in the area of the school was somewhat
S9CRIME 7,450 35.26 1.710 0.554 3.088 9.538
of a problem or a big problem
Other student characteristics (mean)1,3 10,408 133.07 0.101 0.044 2.286 5.228
Student’s age (in months) X9AGE 10,108 58.11 0.048 0.031 1.530 2.341
Student’s height X9HEIGHT 10,016 99.40 0.371 0.295 1.256 1.578
Student’s weight X9WEIGHT 9,995 20.50 0.063 0.049 1.281 1.640
Student’s body mass index (BMI) X9BMI 8,542 4.65 0.027 0.015 1.767 3.123
Total number of persons in household X9HTOTAL 8,542 1.63 0.023 0.012 1.849 3.419
Total number of siblings in household X9NUMSIB 8,520 2.53 0.023 0.013 1.831 3.352
Total number of persons in household less than
X9LESS18 7,488 97.66 0.579 0.175 3.315 10.992
18 years of age
1 Estimates of assessment scores (X9), age (X9), height (X9), weight (X9), and BMI (X9) computed using weight W9C9P_20.
2 Estimates of variables from the teacher (A9), reading teacher (G9), and school administrator questionnaires (S9) computed using weight
W9C29P_9T90. Estimates of variables from the math (M9) or science (N9) teacher computed using weight W9C29P_9T9Z0.
3 Estimates of variables from the parent interview (P9) computed using weight W9C29P_9A0.
NOTE: SE is the standard error based on the sample design. SEsrs is the standard error assuming simple random sampling. DEFT is the root
design effect. DEFF is the design effect. Estimates produced with the restricted-use file. Due to top- and bottom-coding, the same estimates may
not be obtained from the public-use file.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
4-51Table 4-21. Median design effects for the spring fifth-grade survey items, by school characteristic:
Spring 2016
Characteristic1 DEFT DEFF
All schools 1.790 3.203
School affiliation
Public 1.767 3.123
Private 1.527 2.333
Catholic private 1.563 2.443
Other private 1.364 1.860
Census region2
Northeast 1.869 3.493
Midwest 1.918 3.680
South 1.786 3.190
West 1.769 3.129
Locale
City 1.584 2.508
Suburb 1.783 3.178
Town 1.575 2.481
Rural 1.639 2.688
School enrollment
1 to 149 students 1.627 2.648
149 to 299 students 1.495 2.234
300 to 499 students 1.544 2.385
500 to 749 students 1.690 2.855
750 or more students 1.667 2.778
Percent minority enrolled 1.937 3.754
0 to 50 1.619 2.620
16 to 45 1.610 2.592
46 to 85 1.603 2.571
86 to 100 1.790 3.203
1 School characteristics are from the composites X9SCTYP (school affiliation), X9REGION (census region), X9LOCALE (locale),
X9ENRLS (school enrollment), and X9RCETH (percent minority enrolled).
2 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South
Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
NOTE: DEFT is the root design effect. DEFF is the design effect.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten
Class of 2010–11 (ECLS-K:2011), spring 2016.
4-525. RESPONSE RATES
This chapter presents unit response rates and overall response rates for the different
instruments included in the fifth-grade round of data collection (spring 2016) for the Early Childhood
Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011). A unit response rate is the ratio of
the number of units with a completed interview, questionnaire, or assessment (for example, the units are
students with a completed assessment) to the number of units sampled and eligible for the interview,
questionnaire, or assessment. Unit response rates are used to describe the outcomes of data collection
activities and to measure the quality of the study. The overall response rate indicates the percentage of
eligible units with a completed interview, questionnaire, or assessment, taking all survey stages into
account.
5.1 Study Instruments
For the ECLS-K:2011 fifth-grade data collection, there were several survey instruments, as
shown in exhibit 5-1. Exhibit 5-1 also indicates how much information had to be collected for each
instrument for it to be considered “complete” and, therefore, for a case to be considered a respondent to
that instrument for the purpose of calculating response rates. Response rates are presented in section 5.2
for all of these instruments.
5-1Exhibit 5-1. ECLS-K:2011 survey instruments and definition of completed instrument: Spring 2016
Survey instrument Spring 2016 Definition of completed instrument
Child assessment Yes Parent interview Yes Teacher teacher-level
questionnaire
Teacher child- and
classroom-level
questionnaire
Teacher-level special
education teacher
questionnaire
Child-level special
education teacher
questionnaire
School administrator
questionnaire
Yes Yes Student has at least one of the following: (1) at least one
assessment score (mathematics, reading, or science); (2) at
least one executive function score (DCCS, numbers reversed,
or Flanker)1; (3) at least one completed item in the child
questionnaire (CQ); or (4) has height or weight measurement
Parent answered all applicable items in the family structure
section of the questionnaire (FSQ) through item FSQ200 on
current marital status
Teacher (linked to sampled children) completed at least one
item2 in this questionnaire
Teacher (linked to sampled children) completed at least one
item2 in this questionnaire
Yes Yes Yes Student has special education teacher or related service
provider, and teacher completed at least one item2 in this
questionnaire
Student has special education teacher or related service
provider, and teacher completed at least one item2 in this
questionnaire
School administrator completed at least one item in this
questionnaire
1 In first, second, and third grade, numbers reversed and DCCS were the only executive function scores included in this criterion.
2 The one item that needed to be completed could have been anywhere in the child- and classroom-level questionnaire.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5.2 Unit Response Rates and Overall Response Rates
The tables in this section present both weighted and unweighted response rates for the
different components of data collection shown above in exhibit 5-1 (the child assessment, parent
interview, teacher teacher-level questionnaire, teacher child- and classroom-level questionnaire, school
administrator questionnaire (SAQ), and special education teacher questionnaires) computed at the student
level. Response rates for all students and response rates by selected school and student background
characteristics are provided.
5-2Only weighted rates are discussed in this section. The unweighted rate provides a useful
description of the success of the operational aspects of the survey. The weighted rate gives a better
description of the success of the survey with respect to the population sampled since the weights allow for
inference of the sample data (including response status) to the population level. Both rates are usually
similar unless the probabilities of selection and the unit response rates in the categories with different
selection probabilities vary considerably. All of the unit response rates discussed in this chapter are
weighted unless noted specifically in the text, since the main purpose of this chapter is to describe the
success of the survey with respect to the survey population. The weights used in the computation of the
student-level unit response rates are the fifth-grade student base weights. For a description of these
weights, see chapter 4.
In order to compute response rates by different characteristics, the selected characteristics
must be known for both respondents and nonrespondents. Multiple sources were used to obtain
information on school characteristics in order to have data that were as complete as possible for the
calculation of response rates. For respondents, data for school census region, school locale, school type,
and school enrollment come from the composite variables derived for the data file. For nonrespondents,
school characteristic variables were computed for use in the response rate calculations using the same
process that was used to compute the data file composite variables. Information on the derivation of
variables indicating school region (X9REGION) and school locale (X9LOCALE) is provided in section
7.5.4.7. Information on the derivation of the variable indicating school type (X9SCTYP) is provided in
section 7.5.4.1. Information on the derivation of the variable indicating school enrollment (X9ENRLS) is
provided in section 7.5.4.3. Information on the derivation of the variable indicating percent minority
enrollment (X9RCETH) is provided in section 7.5.4.4.
Information on the child characteristics presented in the tables comes from the fifth-grade
data collection. Information on student sex comes from the composite variable X_CHSEX_R (described
in section 7.5.1.3). Information on student race/ethnicity comes from the composite variable
X_RACETH_R (described in section 7.5.1.4). Information on student year of birth comes from the
composite variable X_DOBYY_R (described in section 7.5.1.1). These composites were derived for all
base-year respondents; therefore, they exist for fifth-grade respondents as well as nonrespondents.
When necessary, comparisons in this chapter were examined to ensure that the differences
discussed were statistically significant at the 95 percent level of confidence. For example, this was done
for tables in section 5.3 when comparing characteristics of the data using different weights, or when
comparing data from different years. Significance tests were not conducted for statements related to
5-3response rates in section 5.2 because the base weights were used to produce all rates, which are calculated
over the same sample of eligible cases.
The overall response rate indicates the percentage of possible interviews, questionnaires, or
assessments completed, taking all survey stages into account. In the base-year data collection, children
were identified for assessment in a two-stage process. The first stage involved the recruitment of sampled
schools to participate in the study. Assessments were then conducted for the sampled children whose
parents consented to the children’s participation. In fifth grade, children were contacted for follow-up
unless they (1) became ineligible for the study because they had moved out of the country or had died, or
(2) were movers who were not sampled for follow-up and, therefore, were excluded from data collection.
The response rate for the child assessment is the percentage of sampled and eligible children not
subsampled out as an unfollowed mover who completed the assessment. The overall weighted response
rate is the product of the base-year before-substitution school response rate for all schools (62.7 percent)
and the fifth-grade weighted child assessment response rate. The overall unweighted response rate is the
product of the unweighted base-year before-substitution response rate for all schools (61.3 percent) and
the fifth-grade unweighted child assessment response rate. In the overall response rate tables, the response
rates by characteristic are also a product of the fifth-grade response rate by the corresponding (weighted
or unweighted) overall base-year rate.
Because children were sampled in the base year and school participation after the base year
was not required for the children to stay in the study, the school response rates used to calculate the
student-level response rates in these tables are those from the base year (the base-year response rates are
presented in table 5-2 of the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11
(ECLS-K:2011), User’s Manual for the ECLS-K:2011 Kindergarten Data File and Electronic Codebook,
Public Version (NCES 2015-074) (Tourangeau et al. 2015a, hereinafter referred to as the base-year User’s
Manual).
In the fifth-grade data collection, all 18,174 base-year respondents were part of the sample.
Of these, about 210 became ineligible for the data collection because they had moved out of the country
sometime between the base year and the start of the fifth-grade data collection and approximately 10 had
died. An additional 3,350 students were not included in the data collection because they were movers who
were subsampled out of the study (see section 4.2.3 for information on mover subsampling). After these
exclusions for ineligibility and subsampling, the number of children followed for data collection in fifth
grade was approximately 14,610. This number is the denominator used to calculate the unweighted parent
interview response rate. This is also the basis of the denominator used to calculate the unweighted child
assessment response rate. However, children who were excluded from the assessment because the study
5-4did not provide needed accommodations for a disability, such as an assessment in Braille, are not included
in the calculation of response rates for the child assessment. Therefore, the denominator used to calculate
the unweighted child assessment response rate is about 14,530. All children enrolled in school were
eligible for a reading teacher questionnaire. Therefore, the denominator used to calculate the reading
teacher response rate is 12,285. Similarly, all children enrolled in school were eligible for a school
administrator questionnaire; therefore, the denominator used to calculate the school administrator
response rate also is 12,285. This denominator is lower than the ones used to calculate response rates for
the child assessment and parent interview because it excludes students who were not eligible for the
reading teacher and administrator questionnaire components: homeschooled children and children who
did not have either a complete child assessment score or parent interview (per the definition of complete
provided in exhibit 5-1) for the fifth-grade collection. Because half of the cases were selected for a math
teacher questionnaire and the other half for a science teacher questionnaire, the denominators used to
calculate the mathematics/science teacher response rates are 6,139 and 6,146, respectively. Again, these
numbers vary because while a child may have been selected for a particular questionnaire, the child may
not have been eligible because of the exclusion of homeschooled children and children who did not have
either a complete child assessment score or parent interview (per the definition of complete provided in
exhibit 5-1) for the fifth-grade collection. The parent and teacher rates are computed at the student level,
meaning they indicate the percentages of students for whom a parent interview was completed or for
whom a teacher questionnaire was received. The school administrator rate is also computed at the student
level and indicates the percentage of students whose school administrator completed a questionnaire.
Table 5-1 presents weighted and unweighted response rates for the child assessment and the
parent interview in the fifth-grade data collection by selected school characteristics. Response rates for
the child questionnaire are the same as for the child assessment because all children with assessment data
have child questionnaire data and vice-versa. Researchers should note that the “unknown/homeschool
group” has a low response rate, in large part because this group includes unlocatable cases who are, by
default, nonrespondents. This unknown/homeschool group (2,200 cases) is about 15 percent of the overall
sample of eligible cases. Because their school characteristics are unknown, cases in this group cannot be
included in a specific school characteristics category. This may have an impact on the calculation of the
response rates by school characteristics that should be considered. Specifically, including these
unlocatable cases in a separate category likely results in response rates by different school characteristics
being higher than they would be if the unlocatable cases were included as nonrespondents when
calculating response rates for the different school characteristic categories. Not including the “unknown”
subgroups, the lowest weighted response rate by type of school for the child assessment/child
questionnaire was for students in non-Catholic private schools (79.9 percent). For other school
characteristics, response rates ranged from 87.7 (students in schools with enrollment less than 150) to
5-596.1 percent (students in schools in towns). For the parent interview, the lowest weighted response rate by
type of school was also for students in non-Catholic private schools (70.9 percent). For all other school
characteristics, parent interview response rates ranged from 69.2 (students in schools with minority
enrollment greater than 85 percent) to 80.7 percent (students in schools in town).
Table 5-1. Response rates for child assessment and parent interview, by selected school characteristics,
fifth grade: Spring 2016
Child assessment1 Parent interview2
School characteristic3
Number of
respondents
Response rates Number of
Response rates
respondents
Weighted Unweighted Weighted Unweighted
All students 11,445 72.4 78.8 10,220 67.6 70.0
School type
Public 10,380 92.6 92.8 8,517 75.7 75.6
Private 1,007 85.6 87.7 884 76.4 77.0
Catholic 524 92.4 92.3 464 83.0 81.7
Other private 483 79.9 83.3 420 70.9 72.4
Homeschool/
Unknown
school type 58 2.8 2.6 819 38.4 37.2
Census region4,5
Northeast 1,880 90.5 91.5 1,540 74.0 74.8
Midwest 2,410 94.2 94.0 2,000 77.1 77.3
South 3,970 92.1 92.3 3,320 76.4 76.5
West 3,130 91.2 91.6 2,550 74.7 74.3
Unknown 60 2.8 2.6 820 38.4 37.2
Locale
City 3,511 90.5 91.0 2,836 73.2 73.2
Suburb 4,758 91.4 91.8 3,944 75.8 75.6
Town 809 96.1 95.7 690 80.7 81.2
Rural 1,977 93.9 94.5 1,656 77.6 78.3
Unknown 390 13.1 15.3 1,094 42.6 42.8
See notes at end of table.
5-6Table 5-1. Response rates for child assessment and parent interview, by selected school characteristics,
fifth grade: Spring 2016—Continued
Child assessment1 Parent interview2
School characteristic3
Number of
respondents
Response rates Number of
Response rates
respondents
Weighted Unweighted Weighted Unweighted
School enrollment
1 to 149 students 373 87.7 91.4 321 74.2 78.3
150 to 299 students 1,318 90.2 91.7 1,104 76.1 76.7
300 to 499 students 3,064 91.3 91.4 2,563 76.5 75.9
500 to 749 students 4,201 93.3 93.2 3,468 76.4 76.3
750 or more students 2,405 93.4 93.4 1,923 74.5 74.4
Unknown 84 3.9 3.7 841 38.6 37.3
Percent minority
enrolled
0 to 15 2,492 93.4 93.5 2,156 80.0 80.2
16 to 45 3,121 92.6 93.4 2,682 79.4 79.8
46 to 85 2,963 91.5 91.5 2,418 74.2 74.3
86 to 100 2,777 91.7 91.7 2,116 69.2 69.5
Unknown 92 4.1 4.1 848 38.7 37.5
1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, or a
completed item from the child questionnaire.
2 Parent answered all applicable items in the family structure section of the questionnaire (FSQ) through item FSQ200 on current marital status.
3 School characteristics were taken from the fifth-grade school administrator questionnaire (SAQ) when available. When fifth-grade SAQ data
were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey
(PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User’s Manual and are not
included in the data file.
4 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
5 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The weighted response rates were calculated using the fifth-grade student base weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
Table 5-2 presents weighted and unweighted response rates for the child assessment and the
parent interview in the fifth-grade data collection by selected student characteristics. For the child
assessment, Hispanic students had the highest weighted response rate at 79.4 percent, while Black
students had the lowest child assessment response rates at 59.6 percent. Parents of Pacific Islander
children had the lowest response rate (53.8 percent) while parents of White children had the highest
weighted response rate (72.0 percent).
5-7Table 5-2. Response rates for child assessment and parent interview, by selected student characteristics,
fifth grade: Spring 2016
Child assessment1 Parent interview2
Student characteristic
Number of
respondents
Response rates Number of
Response rates
respondents
Weighted Unweighted Weighted Unweighted
All students 11,445 72.4 78.8 10,220 67.6 70.0
Sex
Male 5,851 72.1 78.3 5,212 66.9 69.3
Female 5,581 73.0 79.5 5,008 68.6 71.0
Unknown 13 35.7 41.9 0 0.0 0.0
Race/ethnicity
White, non-Hispanic 5,428 72.8 80.4 5,102 72.0 75.2
Black, non-Hispanic 1,117 59.6 66.2 964 53.9 56.7
Hispanic 3,259 79.4 82.3 2,747 67.6 69.0
Asian, non-Hispanic 1,004 76.6 80.9 827 65.7 66.4
Native Hawaiian/
Other Pacific
Islander, non-
Hispanic 62 70.0 71.3 46 53.8 52.9
American Indian or
Alaska Native,
non-Hispanic 101 64.1 67.8 82 57.3 54.7
Two or more races,
non-Hispanic 465 66.6 74.2 452 66.6 71.7
Unknown 9 28.5 36.0 0 0.0 0.0
Year of birth3
2003 50 66.6 72.1 50 66.9 68.1
2004 3,520 72.6 79.5 3,180 68.4 71.3
2005 7,870 72.5 78.6 6980 67.3 69.5
2006 10 63.2 65.0 10 56.3 60.0
Unknown 1 7.0 14.3 0 0.0 0.0
1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, or a
completed item from the child questionnaire.
2 Parent answered all applicable items in the Family Structure Questions (FSQ) section of the questionnaire through item FSQ200 on current
marital status.
3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The weighted response rates were calculated using the fifth-grade student base weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2016.
5-8Table 5-3 and table 5-4 present weighted and unweighted response rates for the reading
teacher questionnaires by selected school characteristics and student characteristics, respectively. The
weighted response rates are 82.0 percent for the teacher-level questionnaire and 81.9 percent for the child-
and classroom-level teacher questionnaire. In fifth grade (as in fourth grade), teacher questionnaires were
separate for reading, mathematics, and science. If a teacher taught both reading and mathematics, he or
she would have to fill out the child- and classroom-level questionnaires for each subject (although there
were half as many questionnaires for mathematics as for reading). The pattern of response rates is almost
the same for both teacher questionnaires. By school characteristics, the highest rates were for students in
schools in rural areas (98.0 percent at the teacher level and 97.7 percent at the child and classroom level).
The lowest rates were for students in schools with at least 86 percent of students who were racial/ethnic
minorities (84.7 percent at the teacher level and 85.3 percent at the child and classroom level). By
selected student characteristics, the highest subgroup rates were observed for White students for the
teacher-level data (83.6 percent) and for Hispanic students for the child- and classroom-level data (83.8
percent). The subgroup with the lowest rates was Asian students (75.1 percent at the teacher level and
72.6 percent at the child and classroom level), not accounting for subgroups with very small sample size
(fewer than 100 children).
Table 5-3. Response rates for reading teacher questionnaires, by selected school characteristics, fifth
grade: Spring 2016
Reading teacher questionnaire
(child and classroom level)1
School characteristic2
Number of
respondents
Reading teacher questionnaire
(teacher level)1
Response rates Number of
Response rates
respondents
Weighted Unweighted Weighted Unweighted
All students 10,460 82.0 85.1 10,445 81.9 85.0
School type
Public 9,505 90.5 90.4 9,486 90.4 90.2
Private 955 93.9 94.4 959 94.5 94.8
Catholic 506 94.1 96.2 508 94.7 96.6
Other private 449 93.6 92.4 451 94.2 92.8
Homeschool/
Unknown
school type 0 0.0 0.0 0 0.0 0.0
Census region3, 4
Northeast 1,710 89.6 89.9 1,710 89.7 90.0
Midwest 2,330 95.3 95.3 2,300 94.6 94.3
South 3,680 90.9 91.3 3,680 91.0 91.4
West 2,750 87.5 87.0 2,760 87.6 87.1
Unknown 0 0.0 0.0 0 0.0 0.0
See notes at end of table.
5-9Table 5-3. Response rates for reading teacher questionnaires, by selected school characteristics, fifth
grade: Spring 2016—Continued
Reading teacher questionnaire
(child and classroom level)1
School characteristic2
Number of
respondents
Reading teacher questionnaire
(teacher level)1
Response rates Number of
respondents
Weighted Unweighted Weighted Unweighted
Response rates
Locale
City 3,040 86.9 85.7 3,056 87.4 86.2
Suburb 4,354 89.7 90.4 4,349 89.6 90.3
Town 778 94.4 95.2 754 92.7 92.3
Rural 1,979 98.0 98.4 1,975 97.7 98.2
Unknown 309 22.9 28.2 311 23.3 28.4
School enrollment
1 to 149 students 369 95.9 97.4 352 93.6 92.9
150 to 299 students 1,222 92.4 92.1 1,222 92.4 92.1
300 to 499 students 2,914 93.9 93.5 2,914 93.9 93.5
500 to 749 students 3,803 89.1 89.4 3,809 89.2 89.5
750 or more students 2,149 89.5 88.7 2,143 89.2 88.4
Unknown 3 0.4 0.4 5 0.8 0.6
Percent minority
enrolled
0 to 15 2,459 97.3 97.3 2,442 96.9 96.7
16 to 45 3,005 94.9 95.2 3,003 94.8 95.2
46 to 85 2,634 86.9 87.8 2,619 86.4 87.3
86 to 100 2,352 84.7 83.7 2,368 85.3 84.3
Unknown 10 0.9 1.3 13 1.5 1.6
1 A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child
assessment or parent interview data.
2 School characteristics were taken from the fifth-grade school administrator questionnaire (SAQ) when available. When fifth-grade SAQ data
were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey
(PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User’s Manual and are not
included in the data file.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The weighted response rates were calculated using the fifth-grade student base weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-10Table 5-4. Response rates for reading teacher questionnaires, by selected student characteristics, fifth
grade: Spring 2016
Reading teacher questionnaire
(child and classroom level)1
Student characteristic
Number of
respondents
Reading teacher questionnaire
(teacher level)1
Response rates Number of
Response rates
respondents
Weighted Unweighted Weighted Unweighted
All students 10,460 82.0 85.1 10,445 81.9 85.0
Sex
Male 5,345 82.0 85.0 5,336 81.9 84.9
Female 5,103 81.9 85.2 5,098 81.9 85.2
Unknown 12 86.3 92.3 11 80.6 84.6
Race/ethnicity
White, non-Hispanic 5,184 83.6 88.8 5,164 83.4 88.5
Black, non-Hispanic 985 75.5 79.6 989 76.1 79.9
Hispanic 2,915 83.3 84.8 2,932 83.8 85.3
Asian, non-Hispanic 811 75.1 75.8 796 72.6 74.4
Native Hawaiian/
Other Pacific
Islander, non-
Hispanic 50 72.8 76.9 49 71.1 75.4
American Indian or
Alaska Native,
non-Hispanic 96 83.0 86.5 95 81.9 85.6
Two or more races,
non-Hispanic 410 77.0 79.6 411 77.1 79.8
Unknown 9 93.0 90.0 9 93.0 90.0
Year of birth2
2003 50 84.3 89.8 50 84.3 89.8
2004 3,300 83.5 87.3 3,290 83.4 87.2
2005 7,100 81.3 84.2 7,090 81.3 84.1
2006 10 71.0 66.7 10 71.0 66.7
Unknown # 100.0 100.0 0 0.0 0.0
# Rounds to zero.
1 A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child
assessment or parent interview data.
2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The weighted response rates were calculated using the fifth-grade student base weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2016.
5-11Table 5-5 and table 5-6 present weighted and unweighted response rates for the mathematics
teacher questionnaires by selected school characteristics and student characteristics, respectively. The
weighted response rates are 82.0 percent for the teacher-level questionnaire and 81.8 percent for the child-
and classroom-level teacher questionnaire. By school type, the highest rates are for students in private
non-Catholic schools, both at the teacher level (93.0 percent) and at the child and classroom level (93.9
percent). The lowest rates are for students in public schools: 90.8 percent at the teacher level, and 90.5
percent at the child and classroom level. By other school characteristics, the pattern is also similar for the
two mathematics and science instruments: lowest for schools in the West and highest for schools in the
Midwest; lowest for schools in the cities and highest for schools in rural areas; lowest for schools with
minority enrollment of more than 85 percent and highest for schools with minority enrollment of 15
percent or less. By school enrollment, however, response rates are lowest for schools with between 500
and 749 students for the two mathematics instruments, but highest for schools with between 300 and 499.
By selected student characteristics, the rates range from 71.6 percent for Asian students for the child- and
classroom- level questionnaire to 84.0 percent for students born in 2004 for the teacher-level
questionnaire, not accounting for subgroups with very small sample size (fewer than 100 children).
Table 5-5. Response rates for mathematics teacher questionnaires, by selected school characteristics,
fifth grade: Spring 2016
Mathematics teacher questionnaire
(child and classroom level)1
School characteristic2
Number of
respondents
Mathematics teacher questionnaire
(teacher level)1
Response rates Number of
Response rates
respondents
Weighted Unweighted Weighted Unweighted
All students 5,234 82.0 85.3 5,213 81.8 84.9
School type
Public 4,772 90.8 90.8 4,750 90.5 90.4
Private 462 92.7 93.7 463 93.2 93.9
Catholic 242 92.4 95.3 242 92.4 95.3
Other private 220 93.0 92.1 221 93.9 92.5
Homeschool/
Unknown
school type 0 0.0 0.0 0 0.0 0.0
Census region3,4
Northeast 890 89.7 89.9 860 89.7 89.9
Midwest 1,150 95.5 95.6 1,140 95.1 94.6
South 1,860 91.3 92.0 1,860 91.3 92.1
West 1,370 87.5 87.0 1,360 87.0 86.3
Unknown 0 0.0 0.0 0 0.0 0.0
See notes at end of table.
5-12Table 5-5. Response rates for mathematics teacher questionnaires, by selected school characteristics,
fifth grade: Spring 2016—Continued
Mathematics teacher questionnaire
(child and classroom level)1
School characteristic2
Number of
respondents
Mathematics teacher questionnaire
(teacher level)1
Response rates Number of
respondents
Weighted Unweighted Weighted Unweighted
Response rates
Locale
City 1,522 86.8 86.2 1,517 87.0 85.9
Suburb 2,178 90.2 90.9 2,172 89.8 90.6
Town 393 95.1 95.6 384 94.1 93.4
Rural 976 96.9 97.7 974 96.7 97.5
Unknown 165 23.8 29.2 166 24.0 29.3
School enrollment
1 to 149 students 178 95.8 97.3 170 93.6 92.9
150 to 299 students 612 90.8 90.8 612 90.8 90.8
300 to 499 students 1,418 93.9 93.6 1,418 94.1 93.6
500 to 749 students 1,946 89.5 90.2 1,934 89.1 89.7
750 or more students 1,078 90.0 89.2 1,077 89.7 89.1
Unknown 2 0.3 0.5 2 0.3 0.5
Percent minority
enrolled
0 to 15 1,215 97.9 97.7 1,206 97.4 97.0
16 to 45 1,503 94.1 95.1 1,502 94.2 95.0
46 to 85 1,335 87.0 88.1 1,323 86.4 87.3
86 to 100 1,176 85.6 84.4 1,176 85.7 84.4
Unknown 5 0.8 1.2 6 1.1 1.5
1 A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child
assessment or parent interview data.
2 School characteristics were taken from the fifth-grade school administrator questionnaire (SAQ) when available. When fifth-grade SAQ data
were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey
(PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User’s Manual and are not
included in the data file.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The weighted response rates were calculated using the fifth-grade student base weight for the sample of students selected for the
mathematics teacher questionnaires.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-13Table 5-6. Response rates for mathematics teacher questionnaires, by selected student characteristics,
fifth grade: Spring 2016
Mathematics teacher questionnaire
(child and classroom level)1
Student characteristic
Number of
respondents
Mathematics teacher questionnaire
(teacher level)1
Response rates Number of
Response rates
respondents
Weighted Unweighted Weighted Unweighted
All students 5,234 82.0 85.3 5,213 81.8 84.9
Sex
Male 2,657 83.1 85.8 2,647 82.9 85.5
Female 2,571 80.9 84.7 2,560 80.7 84.3
Unknown 6 67.4 85.7 6 67.4 85.7
Race/ethnicity
White, non-Hispanic 2,576 83.9 89.1 2,563 83.6 88.7
Black, non-Hispanic 488 74.2 78.2 493 75.5 79.0
Hispanic 1,472 83.6 85.1 1,466 83.3 84.7
Asian, non-Hispanic 401 73.0 75.1 396 71.6 74.2
Native Hawaiian/
Other Pacific
Islander, non-
Hispanic 23 76.3 79.3 23 76.3 79.3
American Indian or
Alaska Native,
non-Hispanic 42 82.2 84.0 42 82.2 84.0
Two or more races,
non-Hispanic 224 79.5 81.8 222 78.8 81.0
Unknown 8 100.0 100.0 8 100.0 100.0
Year of birth
2003 20 79.7 85.7 20 79.7 85.7
2004 1,680 84.0 87.7 1,670 83.6 87.2
2005 3,520 81.0 84.1 3,510 81.0 83.9
2006 10 92.6 87.5 10 92.6 87.5
Unknown # 100.0 100.0 # 100.0 100.0
# Rounds to zero.
1 A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child
assessment or parent interview data.
2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The weighted response rates were calculated using the fifth-grade student base weight for the sample of students selected for the
mathematics teacher questionnaires.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-14Table 5-7 and table 5-8 present weighted and unweighted response rates for the science
teacher questionnaires by selected school characteristics and student characteristics, respectively. The
weighted response rates have the same pattern for both instruments. They are lowest for students in public
schools (90.5 percent at the teacher level and 90.2 percent at the child and classroom level), and highest
for students in Catholic schools (95.4 percent at the teacher level and 96.2 percent at the child and
classroom level). By other school characteristics, the highest rates are 98.3 percent at the teacher level and
97.5 percent at the child and classroom level for students in schools in rural areas. The lowest rates are for
students in schools with more than 85 percent minority enrollment (85.1 percent at the teacher level and
84.7 percent at the child and classroom level). By selected student characteristics, the highest rates are for
Hispanic students (83.8 percent at both the teacher level and at the child and classroom level), and the
lowest rates are for students of two or more races (74.3 at the teacher level, and 73.7 at the child and
classroom level) not accounting for subgroups with very small sample size (fewer than 100 children).
Table 5-7. Response rates for science teacher questionnaires, by selected school characteristics, fifth
grade: Spring 2016
Science teacher questionnaire
(child and classroom level)1
School characteristic2
Number of
respondents
Science teacher questionnaire
(teacher level)1
Response rates Number of
Response rates
respondents
Weighted Unweighted Weighted Unweighted
All students 5,247 82.1 85.4 5,234 81.9 85.2
School type
Public 4,758 90.5 90.5 4,743 90.2 90.2
Private 489 94.5 94.2 491 95.0 94.6
Catholic 262 95.4 96.3 263 96.2 96.7
Other private 227 93.5 91.9 228 93.9 92.3
Homeschool/
Unknown
school type 0 0.0 0.0 0 0.0 0.0
Census region3,4
Northeast 850 89.8 90.2 850 89.3 89.9
Midwest 1,180 95.3 95.2 1,170 95.0 94.8
South 1,840 90.9 91.7 1,840 90.6 91.5
West 1,380 87.5 86.6 1,380 87.5 86.6
Unknown 0 0.0 0.0 0 0.0 0.0
Locale
City 1,521 87.1 85.4 1,515 86.7 85.1
Suburb 2,208 90.5 91.3 2,203 90.3 91.1
Town 383 92.5 94.3 381 91.7 93.8
Rural 996 98.3 98.4 992 97.5 98.0
Unknown 139 21.1 26.3 143 22.1 27.1
See notes at end of table.
5-15Table 5-7. Response rates for science teacher questionnaires, by selected school characteristics, fifth
grade: Spring 2016—Continued
Science teacher questionnaire
(child and classroom level)1
School characteristic2
Number of
respondents
Science teacher questionnaire
(teacher level)1
Response rates Number of
respondents
Weighted Unweighted Weighted Unweighted
Response rates
School enrollment
1 to 149 students 187 94.4 95.4 188 95.3 95.9
150 to 299 students 607 93.5 93.0 605 93.0 92.6
300 to 499 students 1,480 92.3 92.4 1,481 92.2 92.5
500 to 749 students 1,889 89.7 90.1 1,881 89.3 89.7
750 or more students 1,083 90.8 89.2 1,076 90.2 88.6
Unknown 1 0.5 0.3 3 1.3 0.8
Percent minority
enrolled
0 to 15 1,243 96.5 96.9 1,247 96.8 97.2
16 to 45 1,499 94.8 95.2 1,493 94.1 94.9
46 to 85 1,305 87.7 87.9 1,297 87.2 87.3
86 to 100 1,196 85.1 84.5 1,191 84.7 84.1
Unknown 4 0.5 1.0 6 1.4 1.5
1 A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child
assessment or parent interview data.
2 School characteristics were taken from the fifth-grade school administrator questionnaire (SAQ) when available. When fifth-grade SAQ data
were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey
(PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User’s Manual and are not
included in the data file.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The weighted response rates were calculated using the fifth-grade student base weight for the sample of students selected for the science
teacher questionnaires.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-16Table 5-8. Response rates for science teacher questionnaires, by selected student characteristics, fifth
grade: Spring 2016
Science teacher questionnaire
(child and classroom level)1
Student characteristic
Number of
respondents
Science teacher questionnaire
(teacher level)1
Response rates Number of
Response rates
respondents
Weighted Unweighted Weighted Unweighted
All students 5,247 82.1 85.4 5,234 81.9 85.2
Sex
Male 2,700 81.3 84.6 2,689 80.8 84.3
Female 2,541 83.0 86.1 2,539 83.0 86.1
Unknown 6 100.0 100.0 6 100.0 100.0
Race/ethnicity
White, non-Hispanic 2,618 83.6 88.8 2,613 83.4 88.7
Black, non-Hispanic 501 77.2 81.6 497 76.7 80.9
Hispanic 1,462 83.8 85.5 1,460 83.8 85.4
Asian, non-Hispanic 401 75.0 74.8 402 74.5 75.0
Native Hawaiian/
Other Pacific
Islander,
non-Hispanic 28 73.8 77.8 27 65.0 75.0
American Indian or
Alaska Native,
non-Hispanic 52 82.0 85.2 52 82.0 85.2
Two or more races,
non-Hispanic 185 74.3 76.8 183 73.7 75.9
Unknown 0 0.0 0.0 0 0.0 0.0
Year of birth
2003 30 87.4 93.5 30 87.4 93.5
2004 1,630 83.1 87.3 1,630 83.2 87.4
2005 3,590 81.7 84.6 3,580 81.3 84.2
2006 # 34.2 42.9 # 34.2 42.9
# Rounds to zero.
1 A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child
assessment or parent interview data.
2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The weighted response rates were calculated using the fifth-grade student base weight for the sample of students selected for the science
teacher questionnaires.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-17Table 5-9 presents weighted and unweighted overall response rates for the child assessment
and the parent interview in the fifth-grade data collection by selected school characteristics. The overall
response rate is the percentage of possible assessments, interviews, or questionnaires completed, taking
into account the base-year school response rate. Of the 2,896 original and transfer schools that were
initially eligible for the fifth-grade data collection, 2,789 schools participated in the study, 15 schools
refused, and 92 became ineligible because all ECLS-K:2011 students in the school had moved to other
schools. The school response rates used in the overall rates are from the base year because children were
sampled in the base year and were eligible to stay in the study regardless of school participation after the
base year. The overall weighted response rate is the product of the base-year before-substitution school
response rate for all schools (62.7 percent) and the fifth-grade weighted response rate. The overall
unweighted response rate is the product of the unweighted base-year before-substitution response rate for all
schools (61.3 percent) and the fifth-grade unweighted response rate. In the overall response rate tables, the
response rates by characteristic are also a product of the fifth-grade response rate by the corresponding
(weighted or unweighted) overall base-year rate.
The overall weighted response rate for the child assessment was 45.4 percent. For the parent
interview, the overall weighted response rate was 42.4 percent. Because the driving factor of the overall
response rate is the base-year school response rate for all schools, the pattern of overall response rates by
subgroups is the same as the pattern for the fifth-grade response rates.
Table 5-9. Overall response rates for child assessment and parent interview, by selected school
characteristics, fifth grade: Spring 2016
Child assessment1 Parent interview2
School characteristic3
Number of
respondents
Overall response rates Number of
Overall response rates
respondents
Weighted Unweighted Weighted Unweighted
All students 11,445 45.4 48.3 10,220 42.4 42.9
School type
Public 10,380 58.1 56.9 8,517 47.5 46.3
Private 1,007 53.7 53.8 884 47.9 47.2
Catholic 524 57.9 56.6 464 52.0 50.1
Other private 483 50.1 51.1 420 44.5 44.4
Homeschool/
Unknown
school type 58 1.8 1.6 819 24.1 22.8
See notes at end of table.
5-18Table 5-9. Overall response rates for child assessment and parent interview, by selected school
characteristics, fifth grade: Spring 2016—Continued
Child assessment1 Parent interview2
School characteristic3
Number of
respondents
Overall response rates
Overall response rates Number of
respondents
Weighted Unweighted Weighted Unweighted
Census region4,5
Northeast 1,880 56.7 56.1 1,540 46.4 45.9
Midwest 2,410 59.1 58.4 2,000 48.3 47.4
South 3,970 57.7 58.4 3,320 47.9 46.9
West 3,130 57.2 57.3 2,550 46.8 45.5
Unknown 60 1.8 1.6 820 24.1 22.8
Locale
City 3,511 56.7 55.8 2,836 45.9 44.9
Suburb 4,758 57.3 56.3 3,944 47.5 46.3
Town 809 60.3 58.7 690 50.6 49.8
Rural 1,977 58.9 57.9 1,656 48.7 48.0
Unknown 390 8.2 9.4 1,094 26.7 26.2
School enrollment
1 to 149 students 373 55.0 56.0 321 46.5 48.0
150 to 299 students 1,318 56.6 56.2 1,104 47.7 47.0
300 to 499 students 3,064 57.2 56.0 2,563 48.0 46.5
500 to 749 students 4,201 58.5 57.1 3,468 47.9 46.8
750 or more students 2,405 58.6 57.3 1,923 46.7 45.6
Unknown 84 2.4 2.3 841 24.2 22.9
Percent minority
enrolled
0 to 15 2,492 58.6 57.3 2,156 50.2 49.2
16 to 45 3,121 58.1 57.3 2,682 49.8 48.9
46 to 85 2,963 57.4 56.1 2,418 46.5 45.5
86 to 100 2,777 57.5 56.2 2,116 43.4 42.6
Unknown 92 2.6 2.5 848 24.3 23.0
1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, or a
completed item from the child questionnaire.
2 Parent answered all applicable items in the family structure section of the questionnaire (FSQ) through item FSQ200 on current marital status.
3 School characteristics were taken from the fifth-grade school administrator questionnaire (SAQ) when available. When fifth-grade SAQ data
were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey
(PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User’s Manual and are not
included in the data file.
4 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
5 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The weighted overall response rates were calculated using the school base weight for the school response rate component and the student
base weight for the student response rate component. The counts of students by subgroups do not sum to the total because homeschooled students
and students with unknown school characteristics are not included in this table.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-19Tables 5-10 to 5-12 present weighted and unweighted overall response rates for teacher
questionnaires in the fifth-grade data collection, by selected school characteristics. The overall response
rates for the teacher-level teacher questionnaire were 51.4 percent for the students linked to reading and
mathematics teachers and 51.5 percent for students linked to science teachers. The overall response rates
for the child- and classroom-level teacher questionnaire were 51.4 percent for students linked to reading
and science teachers, and 51.3 percent for those linked to mathematics teachers. The response rates by
subgroup follow the same pattern as was seen for the fifth-grade teacher response rates.
Table 5-10. Overall response rates for reading teacher questionnaires, by selected school characteristics,
fifth grade: Spring 2016
Reading teacher questionnaire
(child and classroom level)1
School characteristic2
Number of
respondents
Reading teacher questionnaire
(teacher level)1
Overall response rates Number of
Overall response rates
respondents
Weighted Unweighted Weighted Unweighted
All students 10,460 51.4 52.2 10,445 51.4 52.1
School type
Public 9,505 56.7 55.4 9,486 56.7 55.3
Private 955 58.9 57.9 959 59.3 58.1
Catholic 506 59.0 59.0 508 59.4 59.2
Other private 449 58.7 56.6 451 59.1 56.9
Homeschool/
Unknown
school type 0 0.0 0.0 0 0.0 0.0
Census region3,4
Northeast 1,710 56.2 55.1 1,710 56.2 55.2
Midwest 2,330 59.8 58.4 2,300 59.3 57.8
South 3,680 57.0 56.0 3,680 57.1 56.0
West 2,750 54.9 53.3 2,760 54.9 53.4
Unknown 0 0.0 0.0 0 0.0 0.0
Locale
City 3,040 54.5 52.5 3,056 54.8 52.8
Suburb 4,354 56.2 55.4 4,349 56.2 55.4
Town 778 59.2 58.4 754 58.1 56.6
Rural 1,979 61.4 60.3 1,975 61.3 60.2
Unknown 309 14.4 17.3 311 14.6 17.4
See notes at end of table.
5-20Table 5-10. Overall response rates for reading teacher questionnaires, by selected school characteristics,
fifth grade: Spring 2016—Continued
Reading teacher questionnaire
(child and classroom level)1
School characteristic2
Number of
respondents
Reading teacher questionnaire
(teacher level)1
Overall response rates Number of
respondents
Weighted Unweighted Weighted Unweighted
Overall response rates
School enrollment
1 to 149 students 369 60.1 59.7 352 58.7 56.9
150 to 299 students 1,222 57.9 56.5 1,222 57.9 56.5
300 to 499 students 2,914 58.9 57.3 2,914 58.9 57.3
500 to 749 students 3,803 55.9 54.8 3,809 55.9 54.9
750 or more students 2,149 56.1 54.4 2,143 55.9 54.2
Unknown 3 0.3 0.2 5 0.5 0.4
Percent minority
enrolled
0 to 15 2,459 61.0 59.6 2,442 60.8 59.3
16 to 45 3,005 59.5 58.4 3,003 59.4 58.4
46 to 85 2,634 54.5 53.8 2,619 54.2 53.5
86 to 100 2,352 53.1 51.3 2,368 53.5 51.7
Unknown 10 0.6 0.8 13 0.9 1.0
1 A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child
assessment or parent interview data.
2 School characteristics were taken from the fifth-grade school administrator questionnaire (SAQ) when available. When fifth-grade SAQ data
were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey
(PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User’s Manual and are not
included in the data file.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The weighted response rates were calculated using the fifth-grade student base weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-21Table 5-11. Overall response rates for mathematics teacher questionnaires, by selected school
characteristics, fifth grade: Spring 2016
Mathematics teacher questionnaire
(child and classroom level)1
School characteristic2
Number of
respondents
Mathematics teacher questionnaire
(teacher level)1
Overall response rates Number of
Overall response rates
respondents
Weighted Unweighted Weighted Unweighted
All students 5,234 51.4 52.3 5,213 51.3 52.0
School type
Public 4,772 56.9 55.7 4,750 56.7 55.4
Private 462 58.1 57.4 463 58.4 57.6
Catholic 242 57.9 58.4 242 57.9 58.4
Other private 220 58.3 56.5 221 58.9 56.7
Homeschool/
Unknown
school type 0 0.0 0.0 0 0.0 0.0
Census region3,4
Northeast 860 56.2 55.1 860 56.2 55.1
Midwest 1,150 59.9 58.6 1,140 59.6 58.0
South 1,860 57.2 56.4 1,860 57.2 56.5
West 1,370 54.9 53.3 1,360 54.5 52.9
Unknown 0 0.0 0.0 0 0.0 0.0
Locale
City 1,522 54.4 52.8 1,517 54.5 52.7
Suburb 2,178 56.6 55.7 2,172 56.3 55.5
Town 393 59.6 58.6 384 59.0 57.3
Rural 976 60.8 59.9 974 60.6 59.8
Unknown 165 14.9 17.9 166 15.0 18.0
See notes at end of table.
5-22Table 5-11. Overall response rates for mathematics teacher questionnaires, by selected school
characteristics, fifth grade: Spring 2016—Continued
Mathematics teacher questionnaire
(child and classroom level)1
School characteristic2
Number of
respondents
Mathematics teacher questionnaire
(teacher level)1
Overall response rates Number of
respondents
Weighted Unweighted Weighted Unweighted
Overall response rates
School enrollment
1 to 149 students 178 60.1 59.6 170 58.7 56.9
150 to 299 students 612 56.9 55.7 612 56.9 55.7
300 to 499 students 1,418 58.9 57.4 1,418 59.0 57.4
500 to 749 students 1,946 56.1 55.3 1,934 55.9 55.0
750 or more students 1,078 56.4 54.7 1,077 56.2 54.6
Unknown 2 0.2 0.3 2 0.2 0.3
Percent minority
enrolled
0 to 15 1,215 61.4 59.9 1,206 61.1 59.5
16 to 45 1,503 59.0 58.3 1,502 59.1 58.2
46 to 85 1,335 54.5 54.0 1,323 54.2 53.5
86 to 100 1,176 53.7 51.7 1,176 53.7 51.7
Unknown 5 0.5 0.7 6 0.7 0.9
1 A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child
assessment or parent interview data.
2 School characteristics were taken from the fifth-grade school administrator questionnaire (SAQ) when available. When fifth-grade SAQ data
were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey
(PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User’s Manual and are not
included in the data file.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The weighted response rates were calculated using the fifth-grade student base weight for the sample of students selected for the
mathematics teacher questionnaires.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-23Table 5-12. Overall response rates for science teacher questionnaires, by selected school characteristics,
fifth grade: Spring 2016
Science teacher questionnaire
(child and classroom level)1
School characteristic2
Number of
respondents
Science teacher questionnaire
(teacher level)1
Overall response rates Number of
Overall response rates
respondents
Weighted Unweighted Weighted Unweighted
All students 5,247 51.5 52.4 5,234 51.4 52.2
School type
Public 4,758 56.7 55.5 4,743 56.6 55.3
Private 489 59.3 57.7 491 59.6 58.0
Catholic 262 59.8 59.0 263 60.3 59.3
Other private 227 58.6 56.3 228 58.9 56.6
Homeschool/
Unknown
school type 0 0.0 0.0 0 0.0 0.0
Census region3,4
Northeast 850 56.3 55.3 850 56.0 55.1
Midwest 1,180 59.8 58.4 1,170 59.6 58.1
South 1,840 57.0 56.2 1,840 56.8 56.1
West 1,380 54.9 53.1 1,380 54.9 53.1
Unknown 0 0.0 0.0 0 0.0 0.0
Locale
City 1,521 54.6 52.4 1,515 54.4 52.2
Suburb 2,208 56.7 56.0 2,203 56.6 55.8
Town 383 58.0 57.8 381 57.5 57.5
Rural 996 61.6 60.3 992 61.1 60.1
Unknown 139 13.2 16.1 143 13.9 16.6
See notes at end of table.
5-24Table 5-12. Overall response rates for science teacher questionnaires, by selected school characteristics,
fifth grade: Spring 2016—Continued
Science teacher questionnaire
(child and classroom level)1
School characteristic2
Number of
respondents
Science teacher questionnaire
(teacher level)1
Overall response rates Number of
respondents
Weighted Unweighted Weighted Unweighted
Overall response rates
School enrollment
1 to 149 students 187 59.2 58.5 188 59.8 58.8
150 to 299 students 607 58.6 57.0 605 58.3 56.8
300 to 499 students 1,480 57.9 56.6 1,481 57.8 56.7
500 to 749 students 1,889 56.2 55.2 1,881 56.0 55.0
750 or more students 1,083 56.9 54.7 1,076 56.6 54.3
Unknown 1 0.3 0.2 3 0.8 0.5
Percent minority
enrolled
0 to 15 1,243 60.5 59.4 1,247 60.7 59.6
16 to 45 1,499 59.4 58.4 1,493 59.0 58.2
46 to 85 1,305 55.0 53.9 1,297 54.7 53.5
86 to 100 1,196 53.4 51.8 1,191 53.1 51.6
Unknown 4 0.3 0.6 6 0.9 0.9
1 A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child
assessment or parent interview data.
2 School characteristics were taken from the fifth-grade school administrator questionnaire (SAQ) when available. When fifth-grade SAQ data
were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey
(PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User’s Manual and are not
included in the data file.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The weighted response rates were calculated using the fifth-grade student base weight for the sample of students selected for the science
teacher questionnaires.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-25Table 5-13 presents the response rates for the two special education teacher questionnaires.
Response rates are not presented by subgroup for the special education teacher questionnaires because of
the relatively small number of students eligible for this component. The denominator for the special
education teacher rates is 1,299. This denominator excludes children who did not have either a complete
child assessment score or parent interview for the fifth-grade collection, even if they had special
education teacher data. The two special education teacher questionnaires, teacher- and child-level, had
almost the same response rates for the fifth-grade data collection (93.6 and 93.1 percent, respectively) and
overall (58.7 and 58.4 percent, respectively).
Table 5-13. Response rates for special education teacher questionnaires, fifth grade: Spring 2016
Questionnaire
Number of
respondents
Response rates Overall response rates
Weighted Unweighted Weighted Unweighted
Special education teacher
Teacher-level
Child-level
questionnaire 1,210 93.6 93.1 58.7 57.1
questionnaire 1,205 93.1 92.8 58.4 56.9
NOTE: A child was eligible for the special education questionnaire if he or she had an Individualized Education Plan (IEP) on file with the
school. A respondent is defined as a child for whom a special education teacher questionnaire was returned with at least one response, and who
had either child assessment or parent interview data. The weighted response rates were calculated using the fifth-grade student base weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
Tables 5-14 and 5-15 present response rates for the SAQ included in the fifth-grade data
collection. In the base year, the school sample was representative of schools educating kindergartners and
kindergarten-aged children, so the base-year User’s Manual presented response rates at the school level.
After the base year, the school sample is the set of schools attended by children in the ECLS-K:2011 and
is no longer a nationally representative sample of schools. For this reason, response rates for the SAQ are
presented only at the student level.
Table 5-14 presents the weighted and unweighted response rates for the school administrator
questionnaire by selected school characteristics. They are rates for students who were not homeschooled
and were respondents in the fifth-grade data collection.1 The denominator for the school administrator
rates is 12,285. The weighted response rate for the school administrator questionnaire was 81.6 percent,
ranging from 90.2 percent for students in public schools to 93.5 percent for students in non-Catholic
private schools. By other school characteristics, the response rates by school characteristics for this
1 A fifth-grade respondent has child data (scoreable reading or mathematics or science data, or at least one executive function score, or a height or
weight measurement, or child questionnaire data, or was excluded from assessment due to lack of accommodation for a disability) or parent
interview data from the fifth-grade round of data collection.
5-26questionnaire were between 84.6 for students in schools with more than 85 percent of students who were
racial/ethnic minorities and 97.6 percent for students in rural areas.
Table 5-15 presents the weighted and unweighted response rates for the SAQ by selected
student characteristics. Excluding subgroups with small numbers of sampled students, the highest
weighted response rate was for White students (83.5 percent) and the lowest weighted response rate was
for Black students (73.7 percent).
Table 5-14. Response rates for school administrator questionnaire, by selected school characteristics,
fifth grade: Spring 2016
Student-level school administrator questionnaire
Response rates
School characteristic1
Number of respondents
Weighted Unweighted
All students 10,407 81.6 84.7
School type
Public 9,451 90.2 89.9
Private 956 93.4 94.5
Catholic 498 93.2 94.7
Other private 458 93.5 94.2
Homeschool/Unknown
school type 0 0.0 0.0
Census region2,3
Northeast 1,640 85.8 86.6
Midwest 2,360 96.8 96.8
South 3,600 90.0 89.5
West 2,800 88.4 88.5
Unknown 0 0.0 0.0
Locale
City 3,063 86.8 86.4
Suburb 4,288 88.9 89.0
Town 792 96.4 96.9
Rural 1,969 97.6 97.9
Unknown 295 22.0 27.0
See notes at end of table.
5-27Table 5-14. Response rates for school administrator questionnaire, by selected school characteristics,
fifth grade: Spring 2016—Continued
Student-level school administrator questionnaire
Response rates
School characteristic1
Number of respondents
Weighted Unweighted
School enrollment
1 to 149 students 368 95.4 97.1
150 to 299 students 1,245 93.5 93.8
300 to 499 students 2,917 94.2 93.6
500 to 749 students 3,774 88.9 88.7
750 or more students 2,102 87.4 86.8
Unknown 1 0.2 0.1
Percent minority enrolled
0 to 15 2,494 98.2 98.7
16 to 45 2,946 93.5 93.4
46 to 85 2,592 86.6 86.4
86 to 100 2,373 84.6 84.4
Unknown 2 0.2 0.3
1 School characteristics were taken from the fifth-grade school administrator questionnaire (SAQ) when available. When fifth-grade SAQ data
were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey
(PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User’s Manual and are not
included in the data file.
2 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the
questionnaire was returned with at least one response, and the student had either child assessment or parent interview data. The weighted
response rates were calculated using the fifth-grade student base weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-28Table 5-15. Response rates for school administrator questionnaire, by selected student characteristics,
fifth grade: Spring 2016
Student-level school administrator questionnaire
Student characteristic
Number of
respondents
Response rates
Weighted Unweighted
All students 10,407 81.6 84.7
Sex
Male 5,325 81.8 84.7
Female 5,070 81.5 84.7
Unknown 12 94.3 92.3
Race/ethnicity
White, non-Hispanic 5,172 83.5 88.6
Black, non-Hispanic 952 73.7 76.9
Hispanic 2,858 82.5 83.1
Asian, non-Hispanic 860 78.3 80.4
Native Hawaiian/Other Pacific Islander,
non-Hispanic 54 79.1 83.1
American Indian or Alaska Native,
non-Hispanic 92 80.4 82.9
Two or more races, non-Hispanic 409 76.8 79.4
Unknown 10 100.0 100.0
Year of birth1
2003 50 83.4 88.1
2004 3,250 82.8 86.0
2005 7,100 81.1 84.1
2006 10 71.0 66.7
Unknown 0 0.0 0.0
1 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the
questionnaire was returned with at least one response, and the student had either child assessment or parent interview data. The weighted
response rates were calculated using the fifth-grade student base weight.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
Table 5-16 shows the overall response rates for the SAQ. The overall weighted response rate
was 51.2 percent. As with other overall response rates, the overall rates by subgroups have the same
patterns as the fifth-grade response rates because the base-year school response rate is for all schools and,
thus, the same for all subgroups.
5-29Table 5-16. Overall response rates for school administrator questionnaire, by selected school
characteristics, fifth grade: Spring 2016
Student-level school administrator questionnaire
School characteristic1
Number of
respondents
Overall response rates
Weighted Unweighted
All students 10,407 51.2 51.9
School type
Public 9,451 56.6 55.1
Private 956 58.6 57.9
Catholic 498 58.4 58.1
Other private 458 58.6 57.7
Homeschool/Unknown school type 0 0.0 0.0
Census region2,3
Northeast 1,640 53.8 53.1
Midwest 2,360 60.7 59.3
South 3,600 56.4 54.9
West 2,800 55.4 54.3
Unknown 0 0.0 0.0
Locale
City 3,063 54.4 53.0
Suburb 4,288 55.7 54.6
Town 792 60.4 59.4
Rural 1,969 61.2 60.0
Unknown 295 13.8 16.6
School enrollment
1 to 149 students 368 59.8 59.5
150 to 299 students 1,245 58.6 57.5
300 to 499 students 2,917 59.1 57.4
500 to 749 students 3,774 55.7 54.4
750 or more students 2,102 54.8 53.2
Unknown 1 0.1 0.1
Percent minority enrolled
0 to 15 2,494 61.6 60.5
16 to 45 2,946 58.6 57.3
46 to 85 2,592 54.3 53.0
86 to 100 2,373 53.0 51.7
Unknown 2 0.1 0.2
1 School characteristics were taken from the fifth-grade school administrator questionnaire (SAQ) when available. When fifth-grade SAQ data
were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey
(PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User’s Manual and are not
included in the data file.
2 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma,
South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the
questionnaire was returned with at least one response, and the student had either child assessment or parent interview data. The weighted overall
response rates were calculated using the school base weight for the school response rate component and the fifth-grade student base weight for
the student response rate component. The counts of students by subgroups do not sum to the total because students with unknown school
characteristics are not included in this table.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-305.3 Nonresponse Bias Analysis
NCES statistical standards require that any survey instrument with a weighted unit response
rate less than 85 percent be evaluated for potential nonresponse bias. For the fifth-grade data collection,
almost all components had weighted response rates lower than 85 percent. Table 5-17 shows response
rates for all instruments.
Table 5-17. Weighted and unweighted response rates for all instruments, fifth grade: Spring 2016
Survey instrument
Number of
Weighted
Unweighted
eligible students
response rate
response rate
Child assessment 14,531 72.4 78.8
Parent interview 14,608 67.6 70.0
Teacher questionnaire A
Reading 12,285 82.0 85.1
Teacher questionnaire A
Mathematics 6,139 82.0 85.3
Teacher questionnaire A
Science 6,146 82.1 85.4
Child- and classroom-level
teacher questionnaire
Reading 12,285 81.9 85.0
Child- and classroom-level
teacher questionnaire
Mathematics 6,139 81.8 84.9
Child- and classroom-level
teacher questionnaire
Science 6,146 81.9 85.2
Teacher-level special
education teacher
questionnaire 1,299 93.6 93.1
Child- and classroom-level
special education teacher
questionnaire 1,299 93.1 92.8
School administrator
questionnaire 12,285 81.6 84.7
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2016.
The effect of nonresponse is examined in two ways. Sections 5.3.1 to 5.3.4 discuss the effect
of nonresponse on estimates produced from each instrument with weighted response rate lower than 85
percent. Section 5.3.5 compares estimates of selected base-year characteristics between base-year
5-31respondents and fifth-grade respondents.2 A comparison of the study estimates to frame estimates, which
pertain to schools with fifth grade and to fifth graders in the United States, cannot be done because the
sample of study schools is not a representative sample and the sample of study students is not
representative of all fifth graders. After the base year, students in the ECLS-K:2011 can only represent the
cohort of children who attended kindergarten or were of kindergarten age in ungraded classrooms in the
2010–11 school year. For a comparison to frame estimates that was conducted in the base year of the
study, see chapter 5 of the base-year User’s Manual.
5.3.1 Effect of Nonresponse on Child Assessment Data
Estimates weighted by the nonresponse-adjusted weights are compared with estimates
weighted by the base weights (which are referred to as unadjusted estimates). Large differences between
the estimates weighted by the nonresponse-adjusted weights and the unadjusted weights may indicate the
potential for bias in the unadjusted estimates. If the differences are small, then either there is very small
bias in the estimates or the characteristics used in the adjustment process are not related to the survey
estimates and, therefore, the adjustments do not introduce changes in the estimates.
The unadjusted base weight only takes into account the selection probabilities of the
sampling units and the subsampling of movers to be followed. The nonresponse-adjusted weights are the
weights used to analyze ECLS-K:2011 data. The nonresponse-adjusted weight used in this analysis of the
effect of nonresponse on child assessment data is W9C9P_20, which is adjusted for nonresponse to the
child assessment. For a discussion of how the weights were constructed, see chapter 4.
In the ECLS-K:2011, chi-square analyses were used to identify characteristics that are most
related to nonresponse, and these characteristics were used in the adjustment. Therefore, the likelihood
that the weighted estimates are biased as a result of nonresponse would be lower than if nonresponse
adjustment was not implemented. This method of examining nonresponse bias, combined with the
comparison of estimates of selected base-year characteristics between base-year respondents and fifth-
grade respondents, provides an indication of the degree to which nonresponse adjustments are needed and
how effective the adjustments are.
2 A base-year respondent has child data (scoreable assessment data, or height or weight measurements, or was excluded from assessment due to
lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. A fifth-grade
respondent has child data (scoreable assessment data, or executive function data, or child questionnaire data, or height or weight measurements or
was excluded from assessment due to lack of accommodation for a disability) or parent interview data from the fifth-grade round of data
collection.
5-32Table 5-18 shows estimates of selected items from the child assessment. Table 5-19 shows
the differences between unweighted and weighted estimates, and between estimates produced using base
weights (unadjusted estimates) and estimates produced using nonresponse-adjusted weights. The
differences are shown in absolute value and as a percent (relative difference), together with their p value
(α = 0.05). For example, for the differences between unweighted and unadjusted estimates, the difference
is the absolute value of the unweighted estimate minus the unadjusted estimate, and the percent is the
difference divided by the unweighted estimate. A p value of less than .05 means that there is a statistically
significant difference between the two estimates.
The differences between the unadjusted and adjusted estimates are indications of potential
nonresponse bias. As can be seen in table 5-18 and 5-19, many of the differences in the estimates are not
statistically significant as shown by the p value. For the child assessment, half of the items included in the
analysis show statistical differences between unadjusted and adjusted estimates, compared with 18
percent in fourth grade. The increased number of significant differences in fifth grade is due to the
reduction in sample size that occurred due to sample attrition between the two rounds. Where there is no
statistical difference, it means that the effect of the nonresponse adjustment is neutral (i.e., it does not
result in changes between unadjusted and adjusted estimates). The range of absolute differences is 0 to
1.76, with an average of 0.41. The average difference in the range of absolute differences is greater than
in fourth grade (0.41 in fifth grade and 0.25 in fourth grade). With every subsequent round of data
collection, the sample gets smaller because of attrition and the likelihood of significant differences
between unadjusted and adjusted estimates gets larger. For this reason, the nonresponse adjustment is
essential to reduce the potential bias.
In terms of interpreting percent difference (relative difference), the percent difference is
sensitive not only to sample size but also to the prevalence of a particular characteristic. Large relative
differences can be a function of small sample sizes. For example, as seen in table 5-19 for students who
attended school in a town, there is an absolute difference between the nonresponse-adjusted and
unadjusted estimates of 0.68 and a relative difference of 6.69. For students who attended school in the
South, there is an absolute difference between the nonresponse-adjusted and unadjusted estimates of 1.51
and a relative difference of 4.14. Proportionately there are fewer students who attended school in a town
than students who attended school in the South; therefore, the relative difference is higher for students
who went to school in a town even though the absolute difference is smaller for students in this group
compared to students who attended school in the South. The differences found in the analyses show that
there is some potential for nonresponse bias in the unweighted assessment data, but the weights used to
produce estimates were adjusted for nonresponse and, thus, reduce that potential for bias.
5-33Table 5-18. Estimates using unadjusted and nonresponse-adjusted weights, child assessment, spring
fifth grade: Spring 2016
Characteristic
Sample
size
Unweighted
estimate
Unadjusted1 Adjusted2
Estimate SE Estimate SE
Proportion of students by school type
Public 11,526 91.22 92.28 0.435 91.71 0.390
Private 11,526 8.78 7.72 0.435 8.29 0.390
Proportion of students by census region3,4
Northeast Midwest South West 11,530 16.45 15.74 0.778 15.90 0.173
11,530 21.17 22.03 1.053 22.12 0.253
11,530 34.93 36.45 0.970 37.96 0.335
11,530 27.45 25.78 0.453 24.02 0.269
Proportion of students by locale
City Suburb Town Rural 11,191 31.70 30.91 1.265 30.88 1.169
11,191 43.03 41.10 1.295 39.75 1.177
11,191 7.30 10.17 1.075 10.85 1.136
11,191 17.97 17.83 1.120 18.51 1.025
Proportion of students by race/ethnicity
White, non-Hispanic 18,131 46.81 51.23 1.710 51.79 1.691
Black, non-Hispanic 18,131 13.21 13.04 1.132 13.30 1.225
Hispanic 18,131 25.33 25.52 1.337 24.78 1.250
Asian, non-Hispanic 18,131 8.51 4.47 0.587 4.45 0.660
Native Hawaiian/Pacific Islander,
18,131 0.65 0.45 0.076 0.41 0.082
non-Hispanic
American Indian/Alaska Native,
18,131 0.93 1.25 0.585 1.18 0.548
non-Hispanic
Two or more races, non-Hispanic 18,131 4.56 4.03 0.243 4.10 0.219
Mean estimate of the following student
scores and characteristics
Mathematics scale score 11,426 119.66 119.06 0.370 119.45 0.380
Reading scale score 11,427 136.08 135.76 0.319 136.26 0.293
Science scale score 11,419 73.17 73.02 0.280 73.38 0.284
Mathematics theta score 11,426 1.84 1.82 0.010 1.83 0.010
Reading theta score 11,427 1.45 1.45 0.007 1.45 0.007
Science theta score 11,419 1.86 1.85 0.014 1.87 0.015
Number reversed age percentile 11,429 44.74 43.97 0.417 44.34 0.420
Age (in months) 11,444 132.99 133.04 0.104 133.07 0.101
Height (in inches) 11,106 58.00 58.04 0.045 58.11 0.048
Weight (in pounds) 11,006 98.51 98.83 0.351 99.40 0.371
Body mass index (BMI) 10,983 20.38 20.43 0.064 20.50 0.063
1 Unadjusted estimates are produced using the fifth-grade student base weight. The sample size is the count of cases with nonzero fifth-grade
student base weight.
2 Adjusted estimates are produced using weight W9C9P_20.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: SE = standard error. The sample sizes are the number of cases with a nonzero fifth-grade base weight and a nonmissing value for the
characteristic or group of characteristics.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-345-35
Table 5-19. Differences between unweighted and weighted estimates, and between unadjusted and adjusted estimates, child assessment, spring
fifth grade: Spring 2016
Between unweighted
and unadjusted1
Between unweighted
and adjusted2
Between unadjusted1
and adjusted2
Characteristic
Sample
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Proportion of students by school type
Public 11,526 1.06 1.16 0.017 0.49 0.54 0.212 0.57 0.62 0.041
Private 11,526 1.06 12.07 0.017 0.49 5.58 0.212 0.57 7.38 0.041
Proportion of students by census
region3,4
West Northeast 11,530 0.71 4.32 0.367 0.55 3.34 0.002 0.16 1.02 0.839
Midwest 11,530 0.86 4.06 0.419 0.95 4.49 0.000 0.09 0.41 0.926
South 11,530 1.52 4.35 0.121 3.03 8.67 0.000 1.51 4.14 0.118
11,530 1.67 6.08 0.000 3.43 12.50 0.000 1.76 6.83 0.000
Proportion of students by locale
City 11,191 0.79 2.49 0.534 0.82 2.59 0.490 0.03 0.10 0.975
Suburb 11,191 1.93 4.49 0.139 3.28 7.62 0.007 1.35 3.28 0.050
Town 11,191 2.87 39.32 0.009 3.55 48.63 0.002 0.68 6.69 0.024
Rural 11,191 0.14 0.78 0.899 0.54 3.01 0.597 0.68 3.81 0.171
Proportion of students by race/ethnicity
White, non-Hispanic 18,131 4.42 9.44 0.012 4.98 10.64 0.004 0.56 1.09 0.163
Black, non-Hispanic 18,131 0.17 1.29 0.878 0.09 0.68 0.948 0.26 1.99 0.613
Hispanic 18,131 0.19 0.75 0.884 0.55 2.17 0.662 0.74 2.90 0.040
Asian, non-Hispanic 18,131 4.04 47.47 0.000 4.06 47.71 0.000 0.02 0.45 0.904
Native Hawaiian/Pacific Islander,
non-Hispanic 18,131 0.20 30.77 0.013 0.24 36.92 0.005 0.04 8.89 0.349
American Indian/Alaska Native,
non-Hispanic 18,131 0.32 34.41 0.577 0.25 26.88 0.649 0.07 5.60 0.312
Two or more races, non-Hispanic 18,131 0.53 11.62 0.031 0.46 10.09 0.038 0.07 1.74 0.592
See notes at end of table.5-36
Table 5-19. Differences between unweighted and weighted estimates, and between unadjusted and adjusted estimates, child assessment, spring
fifth grade: Spring 2016—Continued
Between unweighted
and unadjusted1
Between unweighted
and adjusted2
Between unadjusted1
and adjusted2
Characteristic
Sample
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Mean estimate of the following student
scores and characteristics
Mathematics scale score 11,426 0.60 0.50 0.108 0.21 0.18 0.572 0.39 0.33 0.015
Reading scale score 11,427 0.32 0.24 0.314 0.18 0.13 0.537 0.50 0.37 0.000
Science scale score 11,419 0.15 0.21 0.600 0.21 0.29 0.449 0.36 0.49 0.002
Mathematics theta score 11,426 0.02 1.09 0.103 0.01 0.54 0.533 0.01 0.55 0.017
Reading theta score 11,427 # # 0.336 # # 0.705 # # 0.000
Science theta score 11,419 0.01 0.54 0.598 0.01 0.54 0.436 0.02 1.08 0.001
Number reversed age percentile 11,429 0.77 1.72 0.070 0.40 0.89 0.352 0.37 0.84 0.017
Age (in months) 11,444 0.05 0.04 0.652 0.08 0.06 0.422 0.03 0.02 0.289
Height (in inches) 11,106 0.04 0.07 0.478 0.11 0.19 0.034 0.07 0.12 0.000
Weight (in pounds) 11,006 0.32 0.32 0.372 0.89 0.90 0.020 0.57 0.58 0.001
Body mass index (BMI) 10,983 0.05 0.25 0.391 0.12 0.59 0.065 0.07 0.34 0.051
# Rounds to zero.
1 Unadjusted estimates are produced using the fifth-grade student base weight. The sample size is the count of cases with nonzero fifth-grade student base weight.
2 Adjusted estimates are produced using weight W9C9P_20.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the
District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The sample sizes are the number of cases with a nonzero fifth-grade base weight and a nonmissing value for the characteristic or group of characteristics.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), spring 2016.5.3.2 Effect of Nonresponse on Parent Interview Data
The adjusted weight used in the analysis of the effect of nonresponse on parent interview
data is W9C29P_9A0. For a discussion of how the weights were constructed, see chapter 4. The group of
children with this weight can be referred to as fifth-grade longitudinal respondents as it includes base-year
respondents who also had student data from spring kindergarten, first, second, third, fourth, and fifth
grades, and parent data from fall or spring kindergarten and fifth grade. Table 5-20 shows estimates of
selected items from the parent interview. Table 5-21 shows the differences between unweighted and
weighted estimates, and between estimates produced using base weights (unadjusted estimates) and
estimates produced using nonresponse-adjusted weights. The range of absolute differences is 0.01 to 2.90,
and the average is 0.68; this is similar to fourth grade where the average difference was 0.69.
The discussion of how to interpret the relative difference provided above in the section on
the child assessment applies to the parent interview data as well. As noted above, the percent difference is
sensitive not only to sample size but also to the prevalence of a particular characteristic. For example, as
shown in table 5-20, the percent of students who participated in organized athletic activities is 61.51; the
absolute difference between the nonresponse-adjusted estimate and unadjusted estimate is 1.22, and the
relative difference between these two estimates is 1.94, as shown in table 5-21. The percent of students
whose parents used computer to get information from school is 83.31, with an absolute difference of 0.92
and a relative difference of 1.09 between the nonresponse-adjusted estimate and the unadjusted estimate.
The relative difference is smaller for the groups of students with higher prevalence in the characteristic
examined.
As with the child assessment data, the differences found in the analyses show that there is
some potential for nonresponse bias in the unweighted parent interview data, but the weights used to
produce estimates were adjusted for nonresponse and, thus, reduce that potential for bias.
5-37Table 5-20. Estimates using unadjusted and nonresponse-adjusted weights, parent interview, spring fifth
grade: Spring 2016
Characteristic
Sample
size
Unweighted
estimate
Unadjusted1 Adjusted2
Estimate SE Estimate SE
Proportion of students by school type
Public 11,526 91.22 92.28 0.435 91.71 0.388
Private 11,526 8.78 7.72 0.435 8.29 0.388
Proportion of students by census region3,4
Northeast 11,530 16.45 15.74 0.778 15.84 0.181
Midwest 11,530 21.17 22.03 1.053 22.08 0.257
South 11,530 34.93 36.45 0.970 38.08 0.331
West 11,530 27.45 25.78 0.453 24.01 0.270
Proportion of students by locale
City Town Rural 11,191 31.70 30.91 1.265 30.70 1.163
Suburb 11,191 43.03 41.10 1.295 39.65 1.136
11,191 7.30 10.17 1.075 11.01 1.126
11,191 17.97 17.83 1.120 18.64 1.058
Proportion of students by race/ethnicity
White, non-Hispanic 18,131 46.81 51.23 1.710 51.79 1.690
Black, non-Hispanic 18,131 13.21 13.04 1.132 13.27 1.224
Hispanic 18,131 25.33 25.52 1.337 24.81 1.253
Asian, non-Hispanic 18,131 8.51 4.47 0.587 4.44 0.662
Native Hawaiian/Pacific Islander, non-
Hispanic 18,131 0.65 0.45 0.076 0.41 0.094
American Indian/Alaska Native, non-
Hispanic 18,131 0.93 1.25 0.585 1.17 0.548
Two or more races, non-Hispanic 18,131 4.56 4.03 0.243 4.11 0.222
See notes at end of table.
5-38Table 5-20. Estimates using unadjusted and nonresponse-adjusted weights, parent interview, spring fifth
grade: Spring 2016—Continued
Characteristic
Sample
size
Unweighted
estimate
Unadjusted1 Adjusted2
Estimate SE Estimate SE
Proportion of students with the following
characteristics from the parent interview
Parent is currently married/in civil
union/in domestic partnership 10,196 73.75 72.11 0.865 70.90 0.921
At least one parent has a high school
diploma or equivalent 10,220 90.56 91.58 0.602 91.79 0.480
Child cares for self 9,549 9.78 10.07 0.587 9.78 0.583
Child participated in organized athletic
activities 9,665 62.05 62.73 0.868 61.51 0.844
Child participated in performing arts
programs 9,648 23.02 22.96 0.504 23.04 0.525
Child has art classes or lessons 9,651 12.67 12.81 0.449 11.55 0.417
Parent volunteered at school 10,154 46.88 46.47 1.413 44.01 1.325
Parent used computer to get information
from school 10,158 83.00 84.23 0.915 83.31 0.915
Often or sometimes true that parent could
not afford balanced meals in last 12
months 9,306 7.60 8.09 0.399 8.03 0.444
Household poverty index
Below poverty threshold 10,220 21.38 21.46 1.008 22.29 0.905
At or above poverty threshold but below
200 percent poverty threshold 10,220 21.78 21.44 0.575 23.51 0.670
At or above 200 percent poverty threshold 10,220 56.84 57.10 1.226 54.20 1.053
Mean estimate of the following student
characteristics
Total number of persons in household 10,220 4.69 4.66 0.025 4.65 0.027
Total number of siblings in household 10,190 2.53 2.55 0.022 2.53 0.023
Total number of persons in household less
than 18 years of age 10,220 1.63 1.64 0.023 1.63 0.023
1 Unadjusted estimates are produced using the fifth-grade student base weight. The sample size is the count of cases with nonzero fifth-grade
student base weight.
2 Adjusted estimates are produced using weight W9C29P_9A0. This weight applies to base-year respondents who also had student data from
spring kindergarten, first, second, third, fourth, and fifth grades, and parent interview data from fall or spring kindergarten and fifth grade.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: SE = standard error. The sample sizes are the number of cases with a nonzero fifth-grade base weight and a nonmissing value for the
characteristic or group of characteristics.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-395-40
Table 5-21. Differences between unweighted and weighted estimates, and between unadjusted and adjusted estimates, parent interview, spring
fifth grade: Spring 2016
Between unweighted
and unadjusted1
Between unweighted
and adjusted2
Between unadjusted1
and adjusted2
Characteristic
Sample
size
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Proportion of students by school type
Public 11,526 1.06 1.16 0.017 0.49 0.54 0.214 0.57 0.62 0.065
Private 11,526 1.06 12.07 0.017 0.49 5.58 0.214 0.57 7.38 0.065
Proportion of students by census
region3,4
South West Northeast 11,530 0.71 4.32 0.367 0.61 3.71 0.001 0.10 0.64 0.906
Midwest 11,530 0.86 4.06 0.419 0.91 4.30 0.001 0.05 0.23 0.958
11,530 1.52 4.35 0.121 3.15 9.02 0.000 1.63 4.47 0.089
11,530 1.67 6.08 0.000 3.44 12.53 0.000 1.77 6.87 0.000
Proportion of students by locale
City Town Rural 11,191 0.79 2.49 0.534 1.00 3.15 0.396 0.21 0.68 0.768
Suburb 11,191 1.93 4.49 0.139 3.38 7.85 0.004 1.45 3.53 0.040
11,191 2.87 39.32 0.009 3.71 50.82 0.001 0.84 8.26 0.023
11,191 0.14 0.78 0.899 0.67 3.73 0.528 0.81 4.54 0.090
Proportion of students by race/ethnicity
White, non-Hispanic 18,131 4.42 9.44 0.012 4.98 10.64 0.004 0.56 1.09 0.159
Black, non-Hispanic 18,131 0.17 1.29 0.878 0.06 0.45 0.962 0.23 1.76 0.644
Hispanic 18,131 0.19 0.75 0.884 0.52 2.05 0.679 0.71 2.78 0.045
Asian, non-Hispanic 18,131 4.04 47.47 0.000 4.07 47.83 0.000 0.03 0.67 0.844
Native Hawaiian/Pacific Islander,
non-Hispanic 18,131 0.20 30.77 0.013 0.24 36.92 0.014 0.04 8.89 0.458
American Indian/Alaska Native,
non-Hispanic 18,131 0.32 34.41 0.577 0.24 25.81 0.656 0.08 6.40 0.328
Two or more races, non-Hispanic 18,131 0.53 11.62 0.031 0.45 9.87 0.046 0.08 1.99 0.540
See notes at end of table.5-41
Table 5-21. Differences between unweighted and weighted estimates, and between unadjusted and adjusted estimates, parent interview, spring
fifth grade: Spring 2016—Continued
Between unweighted
and unadjusted1
Between unweighted
and adjusted2
Between unadjusted1
and adjusted2
Characteristic
Sample
size
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Proportion of students with the following
characteristics from the parent
interview
Parent is currently married/in civil
union/in domestic partnership 10,196 1.64 2.22 0.060 2.85 3.86 0.003 1.21 1.68 0.019
At least one parent has a high school
diploma or equivalent 10,220 1.02 1.13 0.094 1.23 1.36 0.012 0.21 0.23 0.472
Child cares for self 9,549 0.29 2.97 0.625 # # 0.997 0.29 2.88 0.235
Child participated in organized
athletic activities 9,665 0.68 1.10 0.434 0.54 0.87 0.527 1.22 1.94 0.012
Child participated in performing arts
programs 9,648 0.06 0.26 0.912 0.02 0.09 0.976 0.08 0.35 0.794
Child has art classes or lessons 9,651 0.14 1.10 0.760 1.12 8.84 0.009 1.26 9.84 0.000
Parent volunteered at school 10,154 0.41 0.87 0.773 2.87 6.12 0.034 2.46 5.29 0.000
Parent used computer to get
information from school 10,158 1.23 1.48 0.182 0.31 0.37 0.737 0.92 1.09 0.015
Often or sometimes true that parent
could not afford balanced meals in
last 12 months 9,306 0.49 6.45 0.217 0.43 5.66 0.331 0.06 0.74 0.781
Household poverty index
Below poverty threshold 10,220 0.08 0.37 0.936 0.91 4.26 0.319 0.83 3.87 0.066
At or above poverty threshold but
below 200 percent poverty
threshold 10,220 0.34 1.56 0.555 1.73 7.94 0.012 2.07 9.65 0.000
At or above 200 percent poverty
threshold 10,220 0.26 0.46 0.833 2.64 4.64 0.014 2.90 5.08 0.000
See notes at end of table.Characteristic
Sample
size
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Mean estimate of the following student
characteristics
Total number of persons in household 10,220 0.03 0.64 0.403 0.04 0.85 0.182 0.01 0.21 0.310
Total number of siblings in household 10,190 0.02 0.79 0.321 # # 0.776 0.02 0.78 0.021
Total number of persons in household
less than 18 years of age 10,220 0.01 0.61 0.537 # # 0.913 0.01 0.61 0.348
5-42
Table 5-21. Differences between unweighted and weighted estimates, and between unadjusted and adjusted estimates, parent interview, spring
fifth grade: Spring 2016—Continued
Between unweighted
and unadjusted1
Between unweighted
and adjusted2
Between unadjusted1
and adjusted2
# Rounds to zero.
1 Unadjusted estimates are produced using the fifth-grade student base weight. The sample size is the count of cases with nonzero fifth-grade student base weight.
2 Adjusted estimates are produced using weight W9C29P_9A0. This weight applies to base-year respondents who also had student data from spring kindergarten, first, second, third, fourth, and fifth
grades, and parent interview data from fall or spring kindergarten and fifth grade.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the
District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The sample sizes are the number of cases with a nonzero fifth-grade base weight and a nonmissing value for the characteristic or group of characteristics.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), spring 2016.5.3.3 Effect of Nonresponse on Teacher Questionnaire Data
The adjusted weight used in the analysis of the effect of nonresponse on teacher
questionnaire data is W9C29P_9T90 for data from the reading teacher and W9C29P_9T9Z0 for data from
the mathematics/science teachers. For a discussion of how the weights were constructed, see chapter 4.
The group of children with this weight can be referred to as fifth-grade longitudinal respondents as it
includes base-year respondents who also had student data from spring kindergarten and fifth grade, parent
data from fall or spring kindergarten and fifth grade, and teacher data (reading, mathematics, or science)
from fifth grade. Table 5-22 shows estimates of selected items from the teacher questionnaires. Table 5-
23 shows the differences between unweighted and weighted estimates, and between estimates produced
using base weights (unadjusted estimates) and estimates produced using nonresponse adjusted weights.
The range of absolute differences is 0.00 to 2.20, and the average is 0.58, compared with the average of
0.60 for fourth grade. The range of values and the average are similar to those from the analysis of the
parent interview data. Similarly, the differences found in the analyses show that there is some potential
for nonresponse bias in the unweighted teacher data, but the weights used to produce estimates were
adjusted for nonresponse and, thus, reduce that potential for bias.
Table 5-22. Estimates using unadjusted and nonresponse-adjusted weights, teacher questionnaire data,
spring fifth grade: Spring 2016
Characteristic
Sample
size
Unweighted
estimate
Unadjusted1 Adjusted2
Estimate SE Estimate SE
Proportion of students by school type
Public 11,526 91.22 92.28 0.435 91.74 0.386
Private 11,526 8.78 7.72 0.435 8.26 0.386
Proportion of students by census region3,4
Northeast 11,530 16.45 15.74 0.778 15.91 0.215
Midwest 11,530 21.17 22.03 1.053 22.20 0.264
South 11,530 34.93 36.45 0.970 37.87 0.351
West 11,530 27.45 25.78 0.453 24.02 0.262
Proportion of students by locale
City Town Rural 11,191 31.70 30.91 1.265 30.84 1.196
Suburb 11,191 43.03 41.10 1.295 39.78 1.172
11,191 7.30 10.17 1.075 10.94 1.104
11,191 17.97 17.83 1.120 18.45 1.050
See notes at end of table.
5-43Table 5-22. Estimates using unadjusted and nonresponse-adjusted weights, teacher questionnaire data,
spring fifth grade: Spring 2016—Continued
Characteristic
Sample
size
Unweighted
estimate
Unadjusted1 Adjusted2
Estimate SE Estimate SE
Proportion of students by race/ethnicity
White, non-Hispanic 18,131 46.81 51.23 1.710 51.60 1.699
Black, non-Hispanic 18,131 13.21 13.04 1.132 13.34 1.240
Hispanic 18,131 25.33 25.52 1.337 24.91 1.268
Asian, non-Hispanic 18,131 8.51 4.47 0.587 4.47 0.670
Native Hawaiian/Pacific Islander,
non-Hispanic 18,131 0.65 0.45 0.076 0.34 0.089
American Indian/Alaska Native,
non-Hispanic 18,131 0.93 1.25 0.585 1.25 0.550
Two or more races, non-Hispanic 18,131 4.56 4.03 0.243 4.08 0.216
Proportion of students with the following
characteristics from the teacher data
Teacher took course to address using
assessment data for teaching reading 10,414 67.22 68.56 1.378 68.36 1.266
Teacher has regular or standard state
certificate or advanced professional
certificate 10,429 89.48 89.93 0.678 89.86 0.786
Teacher has bachelor’s degree or higher 10,462 99.89 99.88 0.050 99.91 0.051
Teacher agreed/strongly agreed that
school administrator was encouraging
of staff 10,455 82.29 82.53 0.878 82.60 0.998
Teacher agreed/strongly agreed that child
misbehavior interfered with teaching 10,440 26.31 26.47 1.038 26.44 1.052
More than 50 percent of parents
volunteered regularly 10,372 8.77 8.32 0.560 8.01 0.550
Student reading skills were below grade
level as rated by reading teacher 10,381 25.66 26.54 0.726 24.34 0.807
Student received individual tutoring in
reading/language arts 10,346 24.37 25.08 0.904 23.57 0.898
Parent was very involved at the school 10,357 25.48 25.33 0.786 26.39 0.875
Student was in program to learn English
skills 2,111 37.19 38.78 2.816 37.06 2.847
Student usually worked to best ability in
math 5,180 48.92 49.41 0.965 50.05 1.198
Student math skills were below grade
level as rated by math teacher 5,181 24.63 25.17 0.790 23.25 0.950
Student solved math problems in small
groups almost every day 5,164 60.59 60.87 1.362 60.82 1.397
Student used computer for math almost
every day 5,157 22.26 23.34 1.258 23.31 1.385
Student usually worked to best ability in
science 5,184 51.18 51.68 0.891 51.48 0.930
Student science skills were below grade
level as rated by science teacher 5,183 20.05 20.20 0.683 19.43 0.803
See notes at end of table.
5-44Table 5-22. Estimates using unadjusted and nonresponse-adjusted weights, teacher questionnaire data,
spring fifth grade: Spring 2016—Continued
Characteristic
Sample
size
Unweighted
estimate
Unadjusted1 Adjusted2
Estimate SE Estimate SE
Proportion of students with the following
characteristics from the teacher data—
Continued
Student worked with others on science
project almost every day 5,168 15.42 15.41 1.280 15.20 1.390
Student used equipment for science
almost every day 5,178 7.36 7.27 0.841 6.96 0.820
1 Unadjusted estimates are produced using the fifth-grade student base weight. The sample size is the count of cases with nonzero fifth-grade
student base weight.
2 Adjusted estimates are produced using weight W9C29P_9T90. This weight applies to base-year respondents who also had student data from
spring kindergarten and fifth grade, parent data from fall or spring kindergarten and fifth grade, and reading teacher data from fifth grade. Weight
W9C29P_9T9Z0 is the equivalent weight for the students with mathematics/science teachers.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: SE = standard error. The sample sizes are the number of cases with a nonzero fifth-grade base weight and a nonmissing value for the
characteristic or group of characteristics.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-455-46
Table 5-23. Differences between unweighted and weighted estimates, and between unadjusted and adjusted estimates, teacher questionnaire data,
spring fifth grade: Spring 2016
Between unweighted
and unadjusted1
Between unweighted
and adjusted2
Between unadjusted1
and adjusted2
Characteristic
Sample
size
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Proportion of students by school type
Public 11,526 1.06 1.16 0.017 0.52 0.57 0.184 0.54 0.59 0.104
Private 11,526 1.06 12.07 0.017 0.52 5.92 0.184 0.54 6.99 0.104
Proportion of students by census
region3,4
Northeast 11,530 0.71 4.32 0.367 0.54 3.28 0.013 0.17 1.08 0.841
Midwest 11,530 0.86 4.06 0.419 1.03 4.87 0.000 0.17 0.77 0.860
South 11,530 1.52 4.35 0.121 2.94 8.42 0.000 1.42 3.90 0.146
West 11,530 1.67 6.08 0.000 3.43 12.50 0.000 1.76 6.83 0.000
Proportion of students by locale
City Town Rural 11,191 0.79 2.49 0.534 0.86 2.71 0.478 0.07 0.23 0.931
Suburb 11,191 1.93 4.49 0.139 3.25 7.55 0.007 1.32 3.21 0.057
11,191 2.87 39.32 0.009 3.64 49.86 0.001 0.77 7.57 0.026
11,191 0.14 0.78 0.899 0.48 2.67 0.652 0.62 3.48 0.221
Proportion of students by
race/ethnicity
White, non-Hispanic 18,131 4.42 9.44 0.012 4.79 10.23 0.006 0.37 0.72 0.366
Black, non-Hispanic 18,131 0.17 1.29 0.878 0.13 0.98 0.918 0.30 2.30 0.560
Hispanic 18,131 0.19 0.75 0.884 0.42 1.66 0.743 0.61 2.39 0.088
Asian, non-Hispanic 18,131 4.04 47.47 0.000 4.04 47.47 0.000 # # 0.990
Native Hawaiian/Pacific Islander,
non-Hispanic 18,131 0.20 30.77 0.013 0.31 47.69 0.001 0.11 24.44 0.047
American Indian/Alaska Native,
non-Hispanic 18,131 0.32 34.41 0.577 0.32 34.41 0.561 # # 0.935
Two or more races, non-Hispanic 18,131 0.53 11.62 0.031 0.48 10.53 0.030 0.05 1.24 0.703
See notes at end of table.5-47
Table 5-23. Differences between unweighted and weighted estimates, and between unadjusted and adjusted estimates, teacher questionnaire data,
spring fifth grade: Spring 2016—Continued
Between unweighted
and unadjusted1
Between unweighted
and adjusted2
Between unadjusted1
and adjusted2
Characteristic
Sample
size
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Proportion of students with the
following characteristics from the
teacher data
Teacher took course to address using
assessment data for teaching
Teacher has regular or standard state
certificate or advanced
Teacher has bachelor’s degree or
Teacher agreed/strongly agreed that
school administrator was
Teacher agreed/strongly agreed that
child misbehavior interfered with
More than 50 percent of parents
Student reading skills were below
grade level as rated by reading
Student received individual tutoring
Parent was very involved at the
Student was in program to learn
reading 10,414 1.34 1.99 0.334 1.14 1.70 0.370 0.20 0.29 0.620
professional certificate 10,429 0.45 0.50 0.512 0.38 0.42 0.630 0.07 0.08 0.826
higher 10,462 0.01 0.01 0.994 0.02 0.02 0.641 0.03 0.03 0.317
encouraging of staff 10,455 0.24 0.29 0.782 0.31 0.38 0.753 0.07 0.08 0.869
teaching 10,440 0.16 0.61 0.881 0.13 0.49 0.905 0.03 0.11 0.939
volunteered regularly 10,372 0.45 5.13 0.417 0.76 8.67 0.170 0.31 3.73 0.212
teacher 10,381 0.88 3.43 0.229 1.32 5.14 0.104 2.20 8.29 0.000
in reading/language arts 10,346 0.71 2.91 0.430 0.80 3.28 0.380 1.51 6.02 0.000
school 10,357 0.15 0.59 0.846 0.91 3.57 0.301 1.06 4.18 0.001
English skills 2,111 1.59 4.28 0.574 0.13 0.35 0.964 1.72 4.44 0.144
See notes at end of table.5-48
Table 5-23. Differences between unweighted and weighted estimates, and between unadjusted and adjusted estimates, teacher questionnaire data,
spring fifth grade: Spring 2016—Continued
Between unweighted
and unadjusted1
Between unweighted
and adjusted2
Between unadjusted1
and adjusted2
Characteristic
Sample
size
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Proportion of students with the
following characteristics from the
teacher data—Continued
Student usually worked to best
Student math skills were below
grade level as rated by math
Student solved math problems in
Student used computer for math
Student usually worked to best
ability in math 5,180 0.49 1.00 0.611 1.13 2.31 0.346 0.64 1.30 0.369
teacher 5,181 0.54 2.19 0.495 1.38 5.60 0.151 1.92 7.63 0.001
small groups 5,164 0.28 0.46 0.836 0.23 0.38 0.871 0.05 0.08 0.926
almost every day 5,157 1.08 4.85 0.392 1.05 4.72 0.450 0.03 0.13 0.955
ability in science 5,184 0.50 0.98 0.573 0.30 0.59 0.748 0.20 0.39 0.687
Student science skills were below
grade level as rated by science
Student worked with others on
Student used equipment for science
teacher 5,183 0.15 0.75 0.821 0.62 3.09 0.444 0.77 3.81 0.131
science project 5,168 0.01 0.06 0.993 0.22 1.43 0.874 0.21 1.36 0.661
almost every day 5,178 0.09 1.22 0.921 0.40 5.43 0.626 0.31 4.26 0.330
# Rounds to zero.
1 Unadjusted estimates are produced using the fifth-grade student base weight. The sample size is the count of cases with nonzero fifth-grade student base weight.
2 Adjusted estimates are produced using weight W9C29P_9T90 for the students with reading teachers, and weight W9C29P_9T9Z0 for the students with mathematics/science teachers. This weight applies to
base-year respondents who also had student data from spring kindergarten and fifth grade, parent data from fall or spring kindergarten and fifth grade, and reading teacher data from fifth grade.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the
District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The sample sizes are the number of cases with a nonzero fifth-grade base weight and a nonmissing value for the characteristic or group of characteristics.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), spring 2016.5.3.4 Effect of Nonresponse on School Administrator Questionnaire Data
The adjusted weight used in the analysis of the effect of nonresponse on SAQ data is
W9C29P_9T90. For a discussion of how the weights were constructed, see chapter 4. The group of
children with this weight can be referred to as fifth-grade longitudinal respondents as it includes base-year
respondents who also had student data from spring kindergarten and fifth grade, parent data from fall or
spring kindergarten and fifth grade, and reading teacher data from fifth grade. Table 5-24 shows estimates
of selected items from the SAQ. Table 5-25 shows the differences between unweighted and weighted
estimates, and between estimates produced using base weights (unadjusted estimates) and estimates
produced using nonresponse adjusted weights. The range of absolute differences is 0.00 to 2.09, and the
average is 0.56 (compared with an average of 0.45 for fourth grade), very similar to the data from the
teacher instruments. The differences found in the analyses show that there is some potential for
nonresponse bias in the unweighted SAQ data, but the weights used to produce estimates were adjusted
for nonresponse and, thus, reduce that potential for bias.
Table 5-24. Estimates using unadjusted and nonresponse-adjusted weights, school administrator
questionnaire data, spring fifth grade: Spring 2016
Characteristic
Sample
size
Unweighted
estimate
Unadjusted1 Adjusted2
Estimate SE Estimate SE
Proportion of students by school type
Public 11,526 91.22 92.28 0.435 91.74 0.386
Private 11,526 8.78 7.72 0.435 8.263 0.386
Proportion of students by census region3,4
Northeast 11,530 16.45 15.74 0.778 15.91 0.215
Midwest 11,530 21.17 22.03 1.053 22.20 0.264
South 11,530 34.93 36.45 0.970 37.87 0.351
West 11,530 27.45 25.78 0.453 24.02 0.262
Proportion of students by locale
City Suburb Town Rural 11,191 31.70 30.91 1.265 30.84 1.196
11,191 43.03 41.10 1.295 39.78 1.172
11,191 7.30 10.17 1.075 10.94 1.104
11,191 17.97 17.83 1.120 18.45 1.050
See notes at end of table.
5-49Table 5-24. Estimates using unadjusted and nonresponse-adjusted weights, school administrator
questionnaire data, spring fifth grade: Spring 2016—Continued
Characteristic
Sample
size
Unweighted
estimate
Unadjusted1 Adjusted2
Estimate SE Estimate SE
Proportion of students by race/ethnicity
White, non-Hispanic 18,131 46.81 51.23 1.710 51.60 1.699
Black, non-Hispanic 18,131 13.21 13.04 1.132 13.34 1.240
Hispanic 18,131 25.33 25.52 1.337 24.91 1.268
Asian, non-Hispanic 18,131 8.51 4.47 0.587 4.47 0.670
Native Hawaiian/Pacific Islander, non-
Hispanic 18,131 0.65 0.45 0.076 0.34 0.089
American Indian/Alaska Native, non-
Hispanic 18,131 0.93 1.25 0.585 1.25 0.550
Two or more races, non-Hispanic 18,131 4.56 4.03 0.243 4.08 0.216
Proportion of students with the following
characteristics from the school administrator
questionnaire
Taught classroom programs provided by
school at least once a year 10,586 98.22 97.84 0.567 97.66 0.579
School had staff in computer technology 10,507 75.86 75.42 2.145 76.01 2.001
School used electronic communication
with parents several times a month 10,612 39.60 39.48 2.039 39.11 2.036
School used Response to Intervention 10,459 82.72 83.71 1.838 83.99 1.759
Received Title I funding 9,536 69.36 69.71 2.396 71.80 2.099
Bullying happened on occasion 10,540 72.74 71.64 1.467 72.32 1.467
Crime in the area of the school was
somewhat of a problem or a big
problem 10,517 34.64 34.82 1.680 35.26 1.710
1 Unadjusted estimates are produced using the fifth-grade student base weight. The sample size is the count of cases with nonzero fifth-grade
student base weight.
2 Adjusted estimates are produced using weight W9C29P_9T90. This weight applies to base-year respondents who also had student data from
spring kindergarten and fifth grade, parent data from fall or spring kindergarten and fifth grade, and reading teacher data from fifth grade.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: SE = standard error. The sample sizes are the number of cases with a nonzero fifth-grade base weight and a nonmissing value for the
characteristic or group of characteristics.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
5-505-51
Table 5-25. Differences between unweighted and weighted estimates, and between unadjusted and adjusted estimates, school administrator
questionnaire data, spring fifth grade: Spring 2016
Between unweighted
and unadjusted1
Between unweighted
and adjusted2
Between unadjusted1
and adjusted2
Characteristic
Sample
size
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Proportion of students by school type
Public 11,526 1.06 1.16 0.017 0.52 0.57 0.184 0.54 0.59 0.104
Private 11,526 1.06 12.07 0.017 0.52 5.89 0.184 0.54 7.03 0.104
Proportion of students by census
region3,4
South West Northeast 11,530 0.71 4.32 0.367 0.54 3.28 0.013 0.17 1.08 0.841
Midwest 11,530 0.86 4.06 0.419 1.03 4.87 0.000 0.17 0.77 0.860
11,530 1.52 4.35 0.121 2.94 8.42 0.000 1.42 3.90 0.146
11,530 1.67 6.08 0.000 3.43 12.50 0.000 1.76 6.83 0.000
Proportion of students by locale
City Town Rural 11,191 0.79 2.49 0.534 0.86 2.71 0.478 0.07 0.23 0.931
Suburb 11,191 1.93 4.49 0.139 3.25 7.55 0.007 1.32 3.21 0.057
11,191 2.87 39.32 0.009 3.64 49.86 0.001 0.77 7.57 0.026
11,191 0.14 0.78 0.899 0.48 2.67 0.652 0.62 3.48 0.221
Proportion of students by
race/ethnicity
White, non-Hispanic 18,131 4.42 9.44 0.012 4.79 10.23 0.006 0.37 0.72 0.366
Black, non-Hispanic 18,131 0.17 1.29 0.878 0.13 0.98 0.918 0.30 2.30 0.560
Hispanic 18,131 0.19 0.75 0.884 0.42 1.66 0.743 0.61 2.39 0.088
Asian, non-Hispanic 18,131 4.04 47.47 0.000 4.04 47.47 0.000 # # 0.990
Native Hawaiian/Pacific Islander,
non-Hispanic 18,131 0.20 30.77 0.013 0.31 47.69 0.001 0.11 24.44 0.047
American Indian/Alaska Native,
non-Hispanic 18,131 0.32 34.41 0.577 0.32 34.41 0.561 # # 0.935
Two or more races, non-Hispanic 18,131 0.53 11.62 0.031 0.48 10.53 0.030 0.05 1.24 0.703
See notes at end of table.5-52
Table 5-25. Differences between unweighted and weighted estimates, and between unadjusted and adjusted estimates, school administrator
questionnaire data, spring fifth grade: Spring 2016—Continued
Between unweighted
and unadjusted1
Between unweighted
and adjusted2
Between unadjusted1
and adjusted2
Characteristic
Sample
size
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Absolute
difference
Relative
difference p value
Proportion of students with the
following characteristics from the
school administrator questionnaire
Taught classroom programs
provided by school at least once a
year 10,586 0.38 0.39 0.505 0.56 0.57 0.335 0.18 0.18 0.255
School had staff in computer
technology 10,507 0.44 0.58 0.836 0.15 0.20 0.943 0.59 0.78 0.286
School used electronic
communication with parents
several times a month 10,612 0.12 0.30 0.955 0.49 1.24 0.811 0.37 0.94 0.516
School used Response to
Intervention 10,459 0.99 1.20 0.594 1.27 1.54 0.472 0.28 0.33 0.558
Received Title I funding 9,536 0.35 0.50 0.883 2.44 3.52 0.247 2.09 3.00 0.001
Bullying happened on occasion 10,540 1.10 1.51 0.456 0.42 0.58 0.773 0.68 0.95 0.235
Crime in the area of the school was
somewhat of a problem or a big
problem 10,517 0.18 0.52 0.914 0.62 1.79 0.718 0.44 1.26 0.492
# Rounds to zero.
1 Unadjusted estimates are produced using the fifth-grade student base weight. The sample size is the count of cases with nonzero fifth-grade student base weight.
2 Adjusted estimates are produced using weight W9C29P_9T90. This weight applies to base-year respondents who also had student data from spring kindergarten and fifth grade, parent data from fall or
spring kindergarten and fifth grade, and reading teacher data from fifth grade.
3 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the
District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The sample sizes are the number of cases with a nonzero fifth-grade base weight and a nonmissing value for the characteristic or group of characteristics.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), spring 2016.5.3.5 Effect of Nonresponse on Characteristics from the Base Year
In this section, the effect of nonresponse is explored by comparing estimates of selected
base-year characteristics between kindergarten respondents and fifth-grade respondents.3 The estimates
are unadjusted estimates (i.e., they are weighted by the base weights). Base-year characteristics of the
kindergarten respondents are weighted by the base-year base weight that takes into account only the
selection probabilities of the sampling units. Base-year characteristics of the fifth-grade respondents are
weighted by the fifth-grade base weight that takes into account the selection probabilities and the
subsampling of movers to be followed.
Table 5-26 shows the differences in the unadjusted base-year estimates between the
kindergarten respondents and the fifth-grade respondents. As noted above, the characteristics presented in
this table are from the base year, since the purpose of this analysis is to detect large changes in the same
estimates due to sample attrition between the two data collections. Because of missing values, the
kindergarten sample size is smaller than 18,174, the number of base-year respondents. Similarly, the fifth-
grade sample size is smaller than 12,346, the number of fifth-grade respondents. Each difference is shown
as an absolute value and as a relative difference (i.e., the difference divided by the kindergarten estimate).
The relative differences range from 0.01 percent to 17.70 percent, for an average of 4.13 percent. The
largest relative difference is for the percentage of Black students. As in previous years, response rates for
Black students are the lowest among the different race/ethnicity groups (not counting the Hawaiian
Native/Pacific Islander and the American Indian/Alaska Native groups with very small sample sizes). The
other relative differences that are larger than 5 percent are for students in the West (5.28 percent),
students in towns (8.93 percent), students of two or more races (8.05 percent), students in the Native
Hawaiian/Pacific Islander group (8.11 percent), students in the American Indian/Alaska Native group
(10.48 percent), and students in households with income below the poverty threshold (5.89 percent).
Since locale and race/ethnicity are characteristics used to construct nonresponse cells for nonresponse
adjustments, any potential bias would be reduced in estimates produced using weights adjusted for
nonresponse.
3 A base-year respondent has child data (scoreable assessment data or height or weight measurements or was excluded from assessment due to
lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. A fifth-grade
respondent has child data (scoreable assessment data, or executive function data, or child questionnaire data, or height or weight measurements or
was excluded from assessment due to lack of accommodation for a disability) or parent interview data from the fifth-grade round of data
collection.
5-53Table 5-26. Differences between unadjusted base-year estimates from kindergarten respondents and
unadjusted base-year estimates from fifth-grade respondents: Spring 2011 and spring 2016
Sample size
Unadjusted estimates and difference between
kindergarten and fifth grade1
Characteristic from the base year
Kindergarten
Fifth
grade Kindergarten
Fifth
grade
Absolute
difference
Relative
difference
Proportion of students by school type
Public 17,791 12,252 89.07 89.06 0.01 0.01
Private 17,791 12,252 10.93 10.94 0.01 0.09
Proportion of students by census
region2,3
Northeast 17,790 12,252 15.74 15.48 0.26 1.65
Midwest 17,790 12,252 21.98 22.13 0.15 0.68
South 17,790 12,252 38.23 37.08 1.15 3.01
West 17,790 12,252 24.04 25.31 1.27 5.28
Proportion of students by locale
City 17,525 12,091 32.79 32.68 0.11 0.34
Suburb 17,525 12,091 33.35 34.55 1.20 3.60
Town 17,525 12,091 11.20 10.20 1.00 8.93
Rural 17,525 12,091 22.65 22.57 0.08 0.35
Proportion of students by race/ethnicity
White, non-Hispanic 18,129 12,334 50.67 52.53 1.86 3.67
Black, non-Hispanic 18,129 12,334 13.73 11.30 2.43 17.70
Hispanic 18,129 12,334 25.64 26.34 0.70 2.73
Asian, non-Hispanic 18,129 12,334 4.43 4.50 0.07 1.58
Native Hawaiian/Pacific Islander,
non-Hispanic 18,129 12,334 0.37 0.40 0.03 8.11
American Indian/Alaska Native, non-
Hispanic 18,129 12,334 1.05 1.16 0.11 10.48
Two or more races, non-Hispanic 18,129 12,334 4.10 3.77 0.33 8.05
See notes at end of table.
5-54Table 5-26. Differences between unadjusted base-year estimates from kindergarten respondents and
unadjusted base-year estimates from fifth-grade respondents: Spring 2011 and spring
2016—Continued
Sample size
Unadjusted estimates and difference between
kindergarten and fifth grade1
Characteristic from the base year
Kindergarten
Fifth
grade Kindergarten
Fifth
grade
Absolute
difference
Relative
difference
Proportion of students with the
following characteristics from the
spring kindergarten parent interview
Parent is currently married, in civil
union, or domestic partnership 13,481 9,825 72.89 75.18 2.29 3.14
Non-English language used at
home 13,611 9,898 7.90 7.58 0.32 4.05
At least one parent has a high
school diploma or equivalent 16,005 11,218 90.56 90.99 0.43 0.47
Household poverty index
Below poverty threshold 13,527 9,850 25.96 24.43 1.53 5.89
At or above poverty threshold but
below 200 percent poverty
threshold 13,527 9,850 22.41 22.08 0.33 1.47
At or above 200 percent poverty
threshold 13,481 9,850 51.63 53.49 1.86 3.60
1 Unadjusted estimates are produced using the kindergarten base weight for kindergarten and the fifth-grade base weight for fifth grade.
2 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The sample sizes for kindergarten are the number of base-year respondents with a nonmissing value for the kindergarten characteristic or
group of characteristics. The sample sizes for fifth grade are the number of fifth-grade respondents with a nonmissing value for the kindergarten
characteristic or group of characteristics.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2011 and spring 2016.
For each group in table 5-27, the sample size is the number of records with nonzero final
weights. Generally, a relative difference of more than 5 percent indicates that there may be potential bias
in the fifth-grade adjusted estimate. Relative differences between the adjusted estimates for kindergarten
and fifth grade range from 0.06 percent to 8.13 percent, with an average of 2.37 percent. Relative
differences larger than 5 percent are seen for children who regularly spoke a non-English language at
home during kindergarten (8.13 percent) and students in households with income below the poverty
threshold (6.34 percent) and students in households below 200 percent of the poverty threshold (6.23
percent) during kindergarten. That is, even after adjusting estimates, there are proportionately fewer
children in the fifth-grade round than in the kindergarten round who regularly spoke a non-English
language at home during kindergarten, proportionately fewer children in the fifth-grade round than in the
kindergarten round in households below the poverty threshold during kindergarten, and proportionately
5-55more children in the fifth-grade round than in the kindergarten round in households at or above the
poverty threshold but below 200 percent poverty during kindergarten. However, as mentioned before, the
relative difference is a function not only of the sample size but also of the prevalence of a particular
characteristic. For example, only 8.24 percent of kindergartners and 7.57 percent of students in fifth grade
regularly used a non-English language at home in kindergarten, representing a high relative difference.
Kindergarten estimates of percent below poverty (26.33 percent) and at or above poverty threshold but
below 200 percent poverty (22.47 percent) differed compared to their estimates of 24.66 and 23.87 for
fifth grade, which also resulted in high relative differences. In contrast, 90.37 percent of kindergartners
and 91.84 percent of students in fifth grade had at least one parent who had a high school degree or higher
when the student was in kindergarten, which represents a low relative difference.
Table 5-27. Differences between adjusted base-year estimates from kindergarten respondents and
adjusted base-year estimates from fifth-grade longitudinal respondents: Spring 2011 and
spring 2016
Sample size
Adjusted estimates and difference between
kindergarten and fifth grade1
Characteristic from the base year
Kindergarten
Fifth
grade Kindergarten
Fifth
grade
Absolute
difference
Relative
difference
Proportion of students by school type,
kindergarten year
Public 15,798 8,538 89.00 89.20 0.20 0.22
Private 15,798 8,538 11.00 10.80 0.20 1.82
Proportion of students by census
region, kindergarten year2,3
Northeast 15,800 8,538 16.24 15.88 0.36 2.22
Midwest 15,800 8,538 21.77 22.09 0.32 1.47
South 15,800 8,538 37.47 37.88 0.41 1.09
West 15,800 8,538 24.52 24.16 0.36 1.47
Proportion of students by locale,
kindergarten year
City 15,559 8,422 32.82 32.06 0.76 2.32
Suburb 15,559 8,422 33.81 33.79 0.02 0.06
Town 15,559 8,422 10.85 11.05 0.20 1.84
Rural 15,559 8,422 22.52 23.10 0.58 2.58
See notes at end of table.
5-56Table 5-27. Differences between adjusted base-year estimates from kindergarten respondents and
adjusted base-year estimates from fifth-grade longitudinal respondents: Spring 2011 and
spring 2016—Continued
Sample size
Adjusted estimates and difference between
kindergarten and fifth grade1
Characteristic from the base year Kindergarten
Fifth
grade Kindergarten
Fifth
grade
Absolute
difference
Relative
difference
Proportion of students by
race/ethnicity, kindergarten year
White, non-Hispanic 16,083 8,541 51.34 51.80 0.46 0.90
Black, non-Hispanic 16,083 8,541 13.50 13.27 0.23 1.70
Hispanic 16,083 8,541 24.75 24.80 0.05 0.20
Asian, non-Hispanic 16,083 8,541 4.60 4.44 0.16 3.48
Native Hawaiian/Pacific Islander,
non-Hispanic 16,083 8,541 0.42 0.41 0.01 2.38
American Indian/Alaska Native,
non-Hispanic 16,083 8,541 1.21 1.17 0.04 3.31
Two or more races, non-Hispanic 16,083 8,541 4.18 4.11 0.07 1.67
Proportion of students with the
following characteristics from the
spring kindergarten parent
interview
Parent is currently married, in
civil union, or domestic
partnership 13,481 7,728 72.65 74.78 2.13 2.93
Non-English language used at
home 13,611 7,772 8.24 7.57 0.67 8.13
At least one parent has a high
school diploma or equivalent 16,005 8,512 90.37 91.84 1.47 1.63
Household poverty index
Below poverty threshold 13,527 7,741 26.33 24.66 1.67 6.34
At or above poverty threshold but
below 200 percent poverty
threshold 13,527 7,741 22.47 23.87 1.40 6.23
At or above 200 percent poverty
threshold 13,527 7,741 51.20 51.46 0.26 0.51
1 Adjusted estimates are produced using weight W1_2P0 for kindergarten and weight W9C29P_9A0 for fifth grade. W1_2P0 applies to base-year
respondents. W9C29P_9A0 applies to base-year respondents who also had student data for spring kindergarten, first, second, third, fourth grade
and fifth grade, and parent interview data from fall or spring kindergarten and fifth grade.
2 States in each region:
Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont.
Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin.
South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina,
Tennessee, Texas, Virginia, West Virginia, and the District of Columbia.
West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total.
NOTE: The sample sizes for kindergarten are the number of cases with a nonzero kindergarten final weight (weight W1_2P0) and a nonmissing
value for the kindergarten characteristic or group of characteristics. The sample sizes for fifth grade are the number of cases with a nonzero fifth-
grade final weight (weight W9C29P_9A0) and a nonmissing value for the kindergarten characteristic or group of characteristics.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2011 and spring 2016.
5-57This page intentionally left blank.6. DATA PREPARATION
In the fifth-grade round (spring 2016), two types of data collection instruments were again
used for the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011):
computer-assisted interviews and assessments (CAI) and self-administered paper forms (hard-copy
questionnaires). As in all earlier rounds of data collection, data collected in the fifth-grade round were
reviewed and prepared for release to analysts. The approaches used to prepare the data differed with the
mode of data collection. The direct child assessments and parent interviews were conducted using CAI.
Editing specifications were built into the CAI programs used by assessors or interviewers collecting these
data. The teacher and school administrator hard-copy questionnaires were self-administered. When these
hard-copy questionnaires were returned to the data collector’s home office, staff recorded the receipt of
these forms into a project-specific forms tracking system. Data from the hard-copy questionnaires were
then captured by scanning the completed forms. Before scanning, coders reviewed the questionnaires to
ensure that responses were legible and had been written in appropriate response fields for transfer into an
electronic format. After the data were scanned and reviewed for range and logical consistency, coding of
open-ended1 “other, specify” text responses into existing or new categories was implemented.
The following sections briefly describe the data preparation activities for both modes of data
collection, focusing on the fifth-grade activities. More detailed information on these data preparation
activities can be found in user’s manuals from earlier rounds, in particular the User’s Manual for the base-
2
year.
1 Open-ended items are those that do not provide a predetermined set of response options from which to choose. Closed-ended items are those
with predetermined response categories.
2 Users should refer to the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), User’s Manual for the ECLS-
K:2011 Kindergarten Data File and Electronic Codebook, Public Version (NCES 2015-074) (Tourangeau et al. 2015a), hereinafter referred to as
the base-year User’s Manual, for information about the general study methodology and the kindergarten rounds of data collection; to the Early
Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), User’s Manual for the ECLS-K:2011 Kindergarten–First Grade
Data File and Electronic Codebook, Public Version (NCES 2015-078) (Tourangeau et al. 2015b); for information about the first-grade rounds of
data collection; to the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11 (ECLS-K:2011), User’s Manual for the
ECLS-K:2011 Kindergarten–Second Grade Data File and Electronic Codebook, Public Version (NCES 2017-285) (Tourangeau et al. 2017) for
information about the second-grade rounds of data collection; to the Early Childhood Longitudinal Study, Kindergarten Class of 2010–11
(ECLS-K:2011), User’s Manual for the ECLS-K:2011 Kindergarten–Third Grade Data File and Electronic Codebook, Public Version (NCES
2018-034) (Tourangeau et al. 2018a) for information about the third-grade round of data collection; and to the Early Childhood Longitudinal
Study, Kindergarten Class of 2010–11 (ECLS-K:2011), User’s Manual for the ECLS-K:2011 Kindergarten–Fourth Grade Data File and
Electronic Codebook, Public Version (NCES 2018-032) (Tourangeau et al. 2018b) for information about the fourth-grade round of data
collection.
6-16.1 Coding Text Responses
Additional coding was required for some of the items asked in the CAI parent interview once
the data had been collected. These items included “other, specify” text responses and responses to
questions asking about parent or guardian occupation, which interviewers had entered into the CAI
system verbatim.
Review of “other, specify” items. As in previous rounds, for fifth grade, trained data
preparation staff reviewed respondents’ verbatim “other, specify” text responses and coded responses into
existing response categories as appropriate. These staff also reviewed the “other, specify” text to identify
any responses that occurred with sufficient frequency to warrant the addition of a new response category.
For the fifth-grade round, only one question in the parent interview, HEQ290 What is {CHILD} tutored
in?, required an additional response category. The variable P9TUTSST was added to capture responses
that indicated “social studies” or “history.” Text responses that did not fit into any preexisting category
and were not common enough to be coded into a new category were left coded as “other” in the data.
There were no “other, specify” items in the child assessments.
Parent occupation coding. Similar to procedures used in earlier rounds, data preparation
staff also reviewed respondents’ verbatim responses to questions about their occupation. These staff were
trained to code responses into categories using the coding scheme detailed in the Manual for Coding
Industries and Occupations (NCES 2000-077) (U.S. Department of Education, National Center for
Education Statistics 1999), which was created for the Adult Education Survey of the 1999 National
Household Education Surveys Program (NHES). This coding scheme includes a set of 22 two-digit
occupation codes, which is a condensed version of the set of more detailed codes described in the
Standard Occupational Classification Manual—1980 (U.S. Department of Commerce, Office of Federal
Statistical Policy and Planning 1980). All reported parent occupations were coded according to the NHES
coding scheme; the more detailed scheme from the 1980 manual was used to determine final codes for
occupations requiring more detailed consideration to identify the most appropriate code. (See chapter 7
for further description of the occupation codes.)
Occupation coding began by using a computer string match program developed for the
NHES and updated periodically for use in the Early Childhood Longitudinal Study, Kindergarten Class of
1998–99 (ECLS-K) and the ECLS-K:2011 data collections to autocode the reported occupation into one
of 22 categories. The autocoding procedure automatically assigned occupation codes by identifying key
words and information in each text string response with information on occupation, matching those key
words and information to wording for a particular occupation included in the string match program, and
6-2assigning the code associated with that occupation. For fifth grade, almost half of the reported
occupations were autocoded in this manner (6,839 occupations or 48.5 percent). As a quality control
measure, a human coder, blind to the computer-assigned codes, reviewed all the string text responses and
independently assigned occupation codes using the manuals discussed above. When the autocode and the
manual code differed from one another, a coding supervisor adjudicated the record and determined the
appropriate code.
Text responses that could not be coded using the autocoding system were coded manually
using a customized computer program designed for coding occupations. The customized coding computer
program provided a text string with occupation information to coders, who then determined and assigned the
most appropriate occupation code by reviewing occupation text descriptions in the coding manuals. In
addition to the occupation text strings, the coders used other information collected from respondents, such as
main duties at work, industry, and name of the employer, to ensure that the occupation code assigned to
each case was appropriate. Over half the occupations (51.5 percent) were manually coded for fifth grade.
Every manually coded occupation text response was coded at least twice. Two coders
assigned codes independently, without knowledge of each other’s codes (i.e., using a double-blind coding
process). A coding supervisor adjudicated all reported occupations for which the codes assigned
independently by each coder differed.
Of all the occupations that were assigned a code, 24.3 percent (3,426) required adjudication,
either because the autocode and manually assigned code differed (for the autocoded occupations) or because
the two manually assigned codes differed (for the manually coded occupations). Of the 6,839 reported
occupations that were autocoded, 1,010 occupations (14.8 percent) required adjudication because the coder
disagreed with the autocoding. Of the 7,254 reported occupations that were manually coded, 2,416 (33.3
percent) required adjudication because the two human coders disagreed. Following the adjudication process,
the coding supervisor conducted a review of all occupation codes that were assigned manually.
For fifth grade, the assigned coding staff were experienced, having coded occupations for the
ECLS-K:2011 in one or more of the previous rounds. The occupation coding supervisor was also
experienced with the NCES coding scheme and had been involved in the project’s occupation coding
activity since the base year. In instances in which the two coders manually assigned the same code, but
the supervisor disagreed with the code, the case would have been subject to additional examination, and
together the supervisor and coders would have considered the merits of the proposed code before a final
code was assigned. However, there were no instances in the fifth-grade round in which the supervisor
disagreed with the code assigned when the two coders agreed, having assigned the same occupation code.
6-36.1.1 Household Roster Review
The fifth-grade parent interview included a household roster in which information on
household composition was collected. Following protocols established during the previous rounds, three
general types of checks were run on the household roster information to identify missing or inaccurate
information that would require editing.
 First, the relationship of an individual living in the household to the study child was
compared to the individual’s listed age and sex. Inconsistencies, such as a male
mother, and unusual combinations of characteristics, such as a biological mother over
age 65, were examined further. Information was corrected when the interview
contained sufficient information to support a change.
 Second, while it is possible to have more than one mother or more than one father in a
household, households with more than one mother or more than one father were
reviewed to ensure they were not cases of data entry error. Corrections were made
whenever clear errors were identified and a clear resolution existed.
 Third, the relationships of an individual in the household to both the study child and
the respondent were examined, as there were cases in which the relationship of an
individual to the study child conflicted with his or her status as the spouse/partner of
the respondent. For example, in a household containing a child’s grandparents but not
the child’s parents, the grandmother might be designated the “mother” figure, and the
grandfather thus became the “father” figure for the purposes of some questions in the
interview by virtue of his marriage to the grandmother. In this example, these cases
would have been examined but left unchanged. Both the original—and correct
(grandfather)—relationship data and the new “parent-figure” designation (father) that
had been constructed were retained. In other situations, discrepancies in the reported
relationships indicated an error, and the data were edited. For example, in a household
containing two mothers, if a review of the audio recording from the interview
indicated the relationship of the second mother was documented incorrectly by the
interviewer—that the second female identified as a mother was not actually a mother
to the focal child—the relationship of the second female would have been edited
(corrected) to something other than mother.
A flag on the data file (X9EDIT) identifies cases that were reviewed or edited for any of the
reasons described above; the flag was set to 1 if the case was identified for review for any of these
household roster checks. Note that a code of 1 does not necessarily indicate that the data were changed; if
the data were reviewed and found to be as reported by the respondent or there was no clear error to be
fixed, the reviewed data were left as is. There were 553 cases (5.4 percent) identified for review of the
household roster from the spring of fifth grade.
6-46.1.2 Partially Complete Parent Interviews
Parents did not have to complete an entire interview for the data collected from them to be
included in the data file. However, parent interviews did have to be completed through a specified section
of the interview for those data to be included. For the fifth-grade round, the respondent had to answer all
applicable questions through the majority of the section on family structure (FSQ). There were 931
partially completed spring parent interviews for which the respondent answered applicable questions in
the FSQ section but did not complete the entire interview.3 All data derived from questions asked after the
interview termination point for these partially completed interviews are set to -9 for “not ascertained.”
6.2 Receipt, Coding, and Editing of Hard-Copy Questionnaires
6.2.1 Receipt Control
Receipt control was managed in the same manner for fifth grade as it had been in the earlier
rounds of the ECLS-K:2011. Please refer to the base-year User’s Manual for details.
6.2.2 Scanning of Hard-Copy Questionnaires
Scanning of hard-copy questionnaires was managed in the same manner for fifth grade as it had
been in the earlier rounds of the ECLS-K:2011. Please refer to the base-year User’s Manual for details.
6.2.3 Coding for Hard-Copy Questionnaires
Similar to the process described for the parent interview and identical to procedures used in
earlier rounds, “other, specify” text responses at the instrument level were reviewed by the data preparation
staff and coded into existing response categories as appropriate. No “other, specify” text responses collected
in the fifth-grade hard-copy questionnaires occurred with sufficient frequency to warrant the addition of a
new response category. Text responses that did not fit into any preexisting category and were not common
enough to be coded into new categories were left coded as “other” in the data.
3 Note that due to skip patterns applicable to individual cases, parents did not have to answer every question up to the end of the specified section
for the parent interview data to be included in the file. The last question in the FSQ section that applied to all cases was FSQ200 (marital status).
6-56.2.4 Data Editing
The data editing process for hard-copy questionnaires was managed in the same manner for
fifth grade as it had been in the earlier rounds of the ECLS-K:2011. The base-year User’s Manual has
more detail related to editing.
As part of the editing process in fifth grade as well as in earlier rounds of the ECLS-K:2011,
skip patterns were enforced. In cases in which respondents did not follow the skip instructions and
proceeded to answer the questions that were supposed to be skipped, responses for the inapplicable
dependent questions generally were deleted and the data were set to -1, the inapplicable code. There was
one check box on the school administrator questionnaire (SAQ) that was part of a skip pattern that, in
certain circumstances, was not enforced:
 School administrator questionnaire (SAQ): S9SCHPMC
If your school is a private, magnet, or charter school, please check here and SKIP TO Q A13.
When respondents marked this check box, they were directed to skip ahead in the
questionnaire because a subset of subsequent, dependent questions were not applicable to them. In some
cases, it was clear to the data editors that the check box was marked in error by the respondent and the
responses to the dependent questions were valid, usable data. In such cases, the check box was edited
(corrected) in order to retain responses to dependent questions in the data. Consequently, data for this
check box may not reflect the actual responses provided by the person completing the questionnaire.
6-67. DATA FILE CONTENT AND COMPOSITE VARIABLES
This chapter describes the contents of the Early Childhood Longitudinal Study, Kindergarten
Class of 2010–11 (ECLS-K:2011) kindergarten through fifth-grade (K-5) restricted-use data file. The data
are accessible through software called the electronic codebook (ECB). The ECB allows data users to view
variable frequencies, tag variables for extraction, and create the SAS, SPSS for Windows, or Stata code
needed to create an extract file for analysis. The child data file on the ECB is referred to as a “child
catalog.” Instructions for installing the ECB are provided in chapter 8. A help file with further
information about using the ECB is included in the ECB (see menu bar option “Help” and drop-down
option “Contents”).
The K-5 file provides data at the child level and contains one record for each of the
18,174 children who participated, or whose parent participated, in at least one of the two kindergarten
data collections (fall 2010 or spring 2011). References to “parents” in this chapter include both parents
and guardians. Each child record contains data from the various respondents associated with the child
(i.e., the child herself or himself, a parent, one or more teachers, a school administrator and, if applicable,
a nonparental care provider), weights and imputation flags, and administrative variables from the Field
Management System (FMS),1 for example, “F9SCHZIP” for the ZIP code of the school the child attended
in the spring of 2016 (round 9). Among the 18,174 participants from kindergarten, the file includes fall
2011 data for those with a child assessment or parent interview in fall 2011, spring 2012 data for those
with a child assessment or parent interview in spring 2012, fall 2012 data for those with a child
assessment or parent interview in fall 2012, spring 2013 data for those with a child assessment or parent
interview in spring 2013, spring 2014 data for those with a child assessment or parent interview in spring
2014, spring 2015 data for those with a child assessment or parent interview in spring 2015, and spring
2016 data for those with a child assessment or parent interview in spring 2016.
The raw data are provided in an ASCII data file named childK5.dat. To develop data files for
statistical analyses, analysts should use the ECB software or the file record layout located in appendix B
of the DVD. The ECB writes syntax files that must be run within a statistical software package to
generate customized data files. Users should not access the ASCII data file directly, as any changes made
to that file will alter the raw data obtained during data collection.
1 The Field Management System (FMS) includes information collected about the study schools, school staff, and children from available
administrative records or existing data sources (such as the Common Core of Data) or from conversations between data collection staff and
school staff.
7-1This chapter focuses primarily on the composite variables that were created from information
obtained during the fifth-grade data collection. Most of the variables have been computed in the same way
as those that were created using information collected in the base year (i.e., kindergarten), first grade, second
grade, third grade, and fourth grade. To the extent feasible, the composite variables have also been
computed in the same way as those created for the Early Childhood Longitudinal Study, Kindergarten Class
of 1998-99 (ECLS-K). This results in consistency between the two studies and facilitates comparisons
between the two cohorts. However, some composites were created differently in the ECLS-K:2011 than in
the ECLS-K. Documentation for both studies should be consulted before conducting cross-cohort analyses
using composites. For reference, this ECLS-K:2011 kindergarten–fifth grade User’s Manual (NCES 2019-
101), the User’s Manual for the ECLS-K:2011 kindergarten–fourth grade data file (NCES 2018-032), the
User’s Manual for the ECLS-K:2011 kindergarten–third grade data file (NCES 2018-034), the User’s
Manual for the ECLS-K:2011 kindergarten–second grade data file (NCES 2017-285), the User’s Manual for
the ECLS-K:2011 kindergarten–first grade data file (NCES 2015-078), and the User’s Manual for the
ECLS-K:2011 kindergarten data file (NCES 2015-074) are included in the appendix C folder of the DVD.
The user’s manuals for earlier rounds of data collection should be consulted for detailed descriptions of the
composite variables computed for rounds 1 through 8. For information on the ECLS-K, the Combined
User’s Manual for the ECLS-K Eighth-Grade and K-8 Full Sample Data Files and Electronic Codebooks
(NCES 2009-004) (Tourangeau et al 2009) is available on the National Center for Education Statistics
website (https://nces.ed.gov/pubsearch/pubsinfo.asp?pubid=2009004), as are the round-specific manuals for
each round of ECLS-K data collection (https://nces.ed.gov/pubsearch/getpubcats.asp?sid=024).
As discussed in Appendix B, the public-use file is derived from the restricted-use file and is
identical in format. However, masking techniques such as re-categorization and top- and bottom-coding
have been applied to some data to make them suitable for public release. As a result of masking, some
variables in the public-use file may not contain the exact same categories and values described in this
chapter. Please see Appendix B for information on which variables are modified in the public-use file and
see the public-use codebook for the exact categories and values provided in the public data.
This chapter is divided into several sections. Sections 7.1 through 7.4 describe variable
naming conventions, identification variables, missing values, and data flags. Section 7.5 provides details
about the creation of composite variables, and section 7.6 focuses on the methodological variables.
7-27.1 Variable Naming Conventions
Variables are named according to the data source (e.g., parent interview, teacher
questionnaires about the teacher and child) and the data collection round to which they pertain. With the
exception of the identification variables described in section 7.2, the first two or three characters of each
variable (referred to as the variable prefix) include (1) a letter designating the source and (2) a number
indicating the data collection round. For example, the number 9 is used for the data collection that took
place in the spring of 2016. For the spring 2016 teacher child-level questionnaires, as in the spring 2015
teacher questionnaires, there are prefixes for reading (G9), mathematics (M9), and science (N9). These
variable naming conventions are used consistently in the data file. The prefixes used for fifth-grade
variables in the kindergarten–fifth grade data file are listed in exhibit 7-1.
Exhibit 7-1. Prefixes for fifth-grade variables
Variable prefix Source of data
A9
A9…Z1
Data collected from the spring 2016 teacher-level reading questionnaire
Data collected from the spring 2016 teacher-level mathematics or science
questionnaire
C9 Data/scores from the spring 2016 direct child assessment
D9 Data collected from the spring 2016 special education teacher-level questionnaire
E9 Data collected from the spring 2016 special education child-level questionnaire
F9 Data from the spring 2016 Field Management System (FMS)
IF Imputation flags
G9 Data collected from the spring 2016 reading teacher child-level questionnaire
M9 Data collected from the spring 2016 mathematics teacher child-level questionnaire
N9 Data collected from the spring 2016 science teacher child-level questionnaire
P9 Data collected from the spring 2016 parent interview
S9 Data collected from the spring 2016 school administrator questionnaire
X_ Composite/derived variables not specific to a particular round
X9 Spring 2016 composite/derived variables
W Analytic weights and stratum/cluster identifiers
1 The variable names for teacher-level data from the child’s mathematics or science teacher are the same as the variable names for teacher-level
data from the child’s reading teacher, but have the letter Z at the end of the variable name.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), kindergarten-fifth grade (K-5) restricted-use data file.
Some variable names end with a suffix denoting a particular feature of the variable of which
users should be aware. The suffix “_R” indicates that the variable has been updated or revised since its
release in a prior data file. The suffix “2” is used for composites that are based on data from different
items or have new categories added relative to a prior round. The suffix “_I” indicates that missing data
for the variable have been imputed or, in the case of a composite variable, that it is computed from
imputed source variables. Imputation is discussed in sections 7.5.2.6 and 7.5.4.8.
7-37.2 Identification Variables
The kindergarten through fifth-grade data file contains a child identification (ID) variable
(CHILDID) that uniquely identifies each record. For children who have a twin who also participated in
the study, TWIN_ID is the child ID number of the focal child’s twin. The file also contains an ID for the
parent (PARENTID). The parent ID number (PARENTID) is the same number as the child ID.
Unlike in the ECLS-K, CHILDID is randomly generated, so it cannot be used to group
children into classrooms or schools (that is, there is no commonality among IDs for children within the
same school or classroom). The K-5 restricted-use data file does contain IDs for the child’s general
classroom teacher in each round, special education teacher (if applicable) in each round, school in each
round, and before- and after-school care provider in the kindergarten year (if the child was in before- or
after-school care with one provider at least 5 hours per week). Users who wish to conduct hierarchical-
level analyses with the school or classroom as additional levels can use these ID variables to group
children within schools and classrooms. However, it should be noted that children change schools and
classrooms over time, and this should be taken into account in any analysis of school or classroom effects.
Additionally, as children change schools and classrooms over time, cluster sizes may become too small to
support hierarchical analyses. The IDs available on the restricted-use file are listed in exhibit 7-2.
For each study child in the spring 2016 data collection, teacher- and child-level
questionnaires were given to the child’s reading teacher and either his/her mathematics or science teacher.
The variable X9MSFLAG indicates whether the child was sampled for the mathematics or science
domain. Children’s classroom teachers in spring 2016 are identified in the restricted-use file with the ID
variables T9R_ID for reading teachers, T9M_ID for mathematics teachers, and T9S_ID for science
teachers.
There are also class link ID variables (T9RCLASS for reading, T9MCLASS for math, and
T9SCLASS for science) to identify for which class(es) a teacher answered questions. These class link
variables indicate subject and time of day information for a specific class taught by a teacher. They have a
three-character code that begins with a letter followed by a two-digit number (e.g., R01). The letter
indicates the subject taught: R for reading, M for math, S for science, and P for special education. To
identify which teacher completed information for which class for a specific study child, researchers need
to consider both the teacher ID variable(s) and the class link ID variable(s). The teacher ID will be the
same for children taught by the same teacher. However, one teacher could teach multiple classes of the
same subject. The information in the class link variables distinguishes which class the child was in for
7-4children taught by the same teacher. For example, if T9_RID is the same across child-level cases,
T9RCLASS could equal R01 for one child, R02 for another child, and even R03 for another child. The
T9RCLASS variable indicates that these three children are in three different classes with the same
teacher. Children who have the same value for a teacher ID in one of the subjects (e.g., the same value for
the reading teacher ID, T9R_ID) and the same class link ID for that subject (e.g., R01 for reading) were in
the same class.
Exhibit 7-2. Identification variables included in the ECLS-K:2011 kindergarten–fifth grade restricted-
use data file
Order on file Variable Label
1 CHILDID CHILD IDENTIFICATION NUMBER
2 PARENTID PARENT IDENTIFICATION NUMBER
3 S1_ID FALL 2010 SCHOOL IDENTIFICATION NUMBER
4 S2_ID SPRING 2011 SCHOOL IDENTIFICATION NUMBER
5 S3_ID FALL 2011 SCHOOL IDENTIFICATION NUMBER
6 S4_ID SPRING 2012 SCHOOL IDENTIFICATION NUMBER
7 S5_ID FALL 2012 SCHOOL IDENTIFICATION NUMBER
8 S6_ID SPRING 2013 SCHOOL IDENTIFICATION NUMBER
9 S7_ID SPRING 2014 SCHOOL IDENTIFICATION NUMBER
10 S8_ID SPRING 2015 SCHOOL IDENTIFICATION NUMBER
11 S9_ID SPRING 2016 SCHOOL IDENTIFICATION NUMBER
12 T1_ID FALL 2010 TEACHER IDENTIFICATION NUMBER
13 T2_ID SPRING 2011 TEACHER IDENTIFICATION NUMBER
14 T3_ID FALL 2011 TEACHER IDENTIFICATION NUMBER
15 T4_ID SPRING 2012 TEACHER IDENTIFICATION NUMBER
16 T5_ID FALL 2012 TEACHER IDENTIFICATION NUMBER
17 T6_ID SPRING 2013 TEACHER IDENTIFICATION NUMBER
18 T7_ID SPRING 2014 TEACHER IDENTIFICATION NUMBER
19 T8R_ID SPRING 2015 READING TEACHER IDENTIFICATION NUMBER
20 T8M_ID SPRING 2015 MATH TEACHER IDENTIFICATION NUMBER
21 T8S_ID SPRING 2015 SCIENCE TEACHER IDENTIFICATION NUMBER
22 T9R_ID SPRING 2016 READING TEACHER IDENTIFICATION NUMBER
23 T9M_ID SPRING 2016 MATH TEACHER IDENTIFICATION NUMBER
24 T9S_ID SPRING 2016 SCIENCE TEACHER IDENTIFICATION NUMBER
25 D2T_ID SPRING 2011 SPECIAL ED TEACHER ID NUMBER
26 D4T_ID SPRING 2012 SPECIAL ED TEACHER ID NUMBER
27 D6T_ID SPRING 2013 SPECIAL ED TEACHER ID NUMBER
28 D7T_ID SPRING 2014 SPECIAL ED TEACHER ID NUMBER
29 D8T_ID SPRING 2015 SPECIAL ED TEACHER ID NUMBER
30 D9T_ID SPRING 2016 SPECIAL ED TEACHER ID NUMBER
31 CC_ID1 CHILD CARE PROVIDER IDENTIFICATION NUMBER
32 TWIN_ID CHILDID FOR FOCAL CHILD’S TWIN
1 Kindergarten only.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010-11 (ECLS-K:2011), kindergarten-fifth grade (K-5) restricted-use data file.
7-5A single teacher may also have taught two subjects, such as reading and math. If this is the
case, for example, then T9R_ID would equal T9M_ID. Similar to when a teacher teaches multiple classes
in the same subject, to identify which teacher completed information for which class for a specific study
child, researchers need to consider both the teacher ID variable(s) and the class link ID variable(s).
For children who had an Individualized Education Program (IEP) on record with the school
that was identified as part of the process for determining accommodations for the child assessment,
D9T_ID provides the ID number for their special education teacher or related service provider. For some
students, a general classroom teacher was also the student’s special education teacher. However, D9T_ID
does not match T9R_ID, T9M_ID, or T9S_ID for these students. The ID variable S9_ID indicates the
school the child attended at the time of the spring 2016 data collection.
Each child has a school ID number for the two kindergarten data collections, the spring first-
grade data collection, the spring second-grade data collection, the spring third-grade data collection, the
spring fourth-grade data collection, and the spring fifth-grade data collection. Children selected for the
fall subsamples also have school ID numbers for the fall 2011 and fall 2012 data collections. Not all ID
numbers represent specific schools. Instead, certain ID numbers have been designated to identify children
who were homeschoolers (9100), moved to a nonsampled county (9997), were unlocatable (9995), moved
outside the United States (9993), were movers who were not subsampled to be followed into their new
schools (9998), were deceased (9994), or whose parents asked for them to be removed from the study
(9999).
If a child did not have an IEP on record with the school that was identified as part of the
process for determining accommodations for the child assessment, there is no special education teacher or
related services provider associated with that child, and D9T_ID is missing. The D9T_ID would also be
missing if the school records indicated that a child had an Individualized Family Service Plan (IFSP)
when he or she was younger, but did not have an IEP at the time of data collection. If a child had an IEP
identified as part of the process for determining accommodations for the child assessment and, therefore,
a special education teacher associated with him or her, there is an ID provided in D9T_ID whether or not
the special education teacher responded to the spring 2016 special education teacher questionnaires.
For reading, mathematics, or science and special education teachers, there could be missing
data for the child’s teacher-level or child-level questionnaire even though there is an assigned teacher ID
(for example, if the reading, math, science, or special education teacher replied to only one of the two
teacher questionnaires (i.e., child-level or teacher-level) or did not fully complete the questionnaires, an
ID would be present, but there would be missing data). It is left to users to determine how they would like
7-6to set “not applicable” versus “not ascertained” codes when data for T9R_ID, T9M_ID, T9S_ID, or
D9T_ID are missing. Note that if a teacher did not complete a teacher-level questionnaire, completed a
child-level questionnaire for one child, and did not complete another child-level questionnaire for a
different child to whom the teacher was also linked, both children would have the same teacher ID
number (e.g., T9R_ID, T9M_ID, T9S_ID, for the reading, math, or science teacher, respectively, or
D9T_ID for the special education teacher), but only the child for whom the teacher completed the child-
level questionnaire would have data for those variables. It should also be noted that as either a
mathematics questionnaire or science questionnaire, but not both, was fielded for each study child, the
teacher ID will be missing for each child for the subject that was not selected for a questionnaire. For
example, a study child for whom a mathematics questionnaire was fielded and not a science questionnaire
will have system missing for T9S_ID and T9SCLASS.
7.3 Missing Values
Variables on the ECLS-K:2011 data file use a standard scheme for identifying missing data.
Missing value codes are used to indicate item nonresponse (when a question is not answered within an
otherwise completed interview or questionnaire), legitimate skips (when a question was not asked or
skipped because it did not pertain to the respondent), and unit nonresponse (when a respondent did not
complete any portion of an interview or questionnaire) (see exhibit 7-3).
Exhibit 7-3. Missing value codes used in the ECLS-K:2011 data file
Value Description
-1 -2 -4 -5 -7 -8 -9 Not applicable, including legitimate skips
Data suppressed (public-use data file only)
Data suppressed due to administration error
Item not asked in School Administrator Questionnaire form B
Refused (a type of item nonresponse)
Don’t know (a type of item nonresponse)
Not ascertained (a type of item nonresponse)
(blank) System missing (unit nonresponse)
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K: 2011), kindergarten-fifth grade (K-5) restricted-use data file.
The -1 (not applicable) code is used to indicate that a respondent did not answer a question
due to skip instructions within the instrument. In the parent interview, “not applicable” is coded for
questions that were not asked of the respondent because a previous answer made the question inapplicable
to the particular respondent. For example, a question about a child’s sibling’s age is not asked when the
7-7respondent has indicated that the study child has no siblings. For the teacher and school administrator
self-administered instruments, “not applicable” is coded for questions that the respondent left blank
because the written directions instructed him or her to skip the question due to a certain response on a
previous question that made the question inapplicable to the particular respondent. One example of the
use of “not applicable” is found in the spring 2016 school administrator questionnaire question E2.
Question E1 asks whether the school received Title I funds for this school year. If the answer to question
E1 is “yes,” the respondent is directed to continue to question E2 asking if the school was operating a
Title I targeted assistance or schoolwide program. If the answer to question E1 is “no,” the respondent is
supposed to skip to question E3 and question E2 would be coded as -1 (not applicable). If questions E1,
E2, and E3 are left blank by the respondent, and the respondent did not indicate that it is a private school
(S9PRVSCH = 0), data for these questions are coded as -9 (not ascertained), meaning the questions
should have been answered but were not. If the respondent indicated that the school is private
(S9PRVSCH = 1) and questions E1, E2, and E3 are left blank, data for these questions are coded as -1
(not applicable) because they were supposed to be left blank given the school’s designation as private.
There are some exceptions to the standard use of -1 to indicate data are inapplicable for
specific cases. For questions about the hours and minutes that the child spends playing video games, the
response about the number of minutes (P9VIDMIN) could be either “0” or -1 (not applicable) if parents
did not provide a response that included minutes, depending on how interviewers coded this during the
interview. Some interviewers entered a 0 for the minutes field and some skipped the question altogether.
If the question about the number of minutes was skipped, this variable is coded -1. Another exception to
the standard use of -1 is that for several round 9 variables (theta scores from children’s cognitive tests in
reading, X9RTHETK5, math, X9MTHETK5, and science, X9STHETK5), -1 is a valid value and should
not be identified as missing data.
In order to protect the confidentiality of study participants, some data are suppressed in the
public-use data file. The code -2 indicates the suppression of data for confidentiality. The suppression
code -4 is used in rare instances in which there was a problem in the administration of an item that led to a
high proportion of cases having missing or flawed data on the affected item, such that the data that were
collected for the item were not useful and, therefore, are suppressed on the file. Although the
administration error typically did not affect all cases, the -4 missing data code is assigned to all cases,
including those not specifically affected by the error.
The -7 (refused) code indicates that the respondent specifically told the interviewer that he or
she would not answer the question. This, along with the -8 (don’t know) code and the -9 (not ascertained)
code, indicate item nonresponse. The -7 (refused) code is not used in the school or teacher data.
7-8The -8 (don’t know) code indicates that the respondent specifically told the interviewer that
he or she did not know the answer to the question. The -8 (don’t know) code is not used in the school or
teacher data. For questions where “don’t know” is one of the options explicitly provided, a -8 is not coded
for those who choose this option; instead the “don’t know” response is coded as indicated in the value
label information for the variable associated with that question.
The -9 (not ascertained) code indicates that the respondent left a question blank that he or she
should have answered (or for which it is uncertain whether the item should have been answered or
legitimately skipped because the respondent also left a preceding item blank). However, if a gate question2
was left blank, but valid responses are provided to follow-up questions, the valid responses are included in
the data file. For example, in the spring 2016 school administrator questionnaire, question D1 asks, “Do any
of the children in this school come from a home where a language other than English is spoken?” If the
school administrator left D1 blank (i.e., unanswered), but then provided a valid response for question D2
which asks, “What percentage of children in this school are English language learners (ELL)?,” D1 is coded
as -9 and the information from D2 is included in the data file as reported. If a gate question and its follow-up
questions were left blank, all of the questions (gate and follow-up) are coded as -9 (not ascertained).
For data that are not collected using the teacher and school administrator self-administered
questionnaires (e.g., direct assessment scores, the child questionnaire), a -9 means that a value was not
ascertained or could not be calculated due to nonresponse. The -9 (not ascertained) code is also used in
the parent interview data when the interview ended before all applicable questions were asked. In these
cases, the code of -9 is used for all variables associated with interview questions that came after the point
at which the parent ended the interview. One exception to this coding scheme is the pointer variables.3
Pointer variables are not set to -9 when the interview ended before all applicable questions were asked;
instead they are set to the value corresponding to the household’s parent figure(s). Another exception to
this coding scheme should be noted for imputed variables (i.e., parent education, employment, and
income). If a respondent completed a parent interview through the family structure section (a partially
completed interview), but ended the interview before answering questions about education, employment,
or income, these questions (e.g., parent education questions, P9HIG_1_I and P9HIS_1_I) have imputed
values and imputation flags (e.g., IFP9HIG_1 and IFP9HIS_1) greater than zero. The -9 code is also used
in the parent interview for questions that were edited4 or inadvertently skipped in computer-assisted
2 A gate question is the first question in a series with skips to one or more follow-up questions.
3 Pointer variables indicate the household roster number of a person in the household who was the subject of questions about a parent figure.
4 Edits to household composition data that result in the addition or deletion of a parent or parent figure in the child’s household sometimes result
in -9 (not ascertained) codes being used for variables in multiple sections of the parent interview that have questions that are asked depending on
the presence of specific parents or parent figures. The affected sections in the spring 2016 parent interview are FSQ (Family Structure), DWQ
7-9interviewing (CAI) programming. After editing, for complete interviews, the data for all questions that
should have been asked but were not are coded as -9 (not ascertained), while the data for other skipped
questions are coded as -1 (not applicable); codes -7 and -8 are used only when respondents stated a
response of “refused” or “don’t know,” and not as a result of editing or inadvertently skipping a question
as a result of CAI programming.
Missing values (-1, -7, -8, or -9) in questions that allow for more than one response are
coded the same for all coding categories used for the question. For example, in the spring 2016 parent
interview, if the question about subjects in which the child was tutored (HEQ290) has the answer of -8
(don’t know), then all the subject variables associated with that question (e.g., reading, math, science,
foreign language, and any categories that were added based on “other, specify” upcoding) are also coded
as -8 (don’t know).
The “system missing” code appears as a blank when viewing codebook frequencies and in
the ASCII data file. System missing codes (blanks) indicate that data for an entire instrument or
assessment are missing due to unit nonresponse. For example, when a child’s parent does not participate
in the parent interview, all of the data associated with questions from the parent interview are coded as
“system missing” (blank) for that child. These blanks may be converted to another value when the data
are extracted into specific processing packages. For instance, SAS converts these blanks into periods
(“.”) for numeric variables.
Codes used to identify missing values (-1, -7, -8, -9, or system missing) are not all identified
as missing values by default in data analysis software. Users will need to define these as missing values in
the software they are using to analyze the data. Depending on the research question being addressed, in
some instances users may want to assign a valid value to cases with missing values. For example, a
teacher who reported that he or she did not have any English language learners in his or her classroom in
the spring of 2016 (question F10 in the reading teacher questionnaire; question C10 in the mathematics
and science teacher questionnaires) skipped the next question (question F11 in the reading teacher
questionnaire; question C11 in the mathematics and science teacher questionnaires) asking how many
English language learners were in his or her classroom. An analyst interested in knowing the average
number of English language learners in the classrooms of children in the ECLS-K:2011 may want to
recode a value of -1 (not applicable) on the variable associated with question F11 or question C11 to a
value of 0 (thereby indicating no English language learners in the classroom) in those instances where a
(Discipline, Warmth, and Emotional Supportiveness), NRQ (Nonresident Parents), COQ (Country of Origin for Nonresident Biological Parents),
PPQ (Parent’s Psychological Well-Being and Health), PEQ (Parent Education and Human Capital), and EMQ (Parent Employment). The -9 (not
ascertained) code is used for both questions that are asked about specific parent/parent figures as well as those that are based on skips from those
questions.
7-10teacher indicated in question F10/question C10 that there were no English language learners in the
classroom. It is advised that users crosstabulate all gate questions and follow-up questions before
proceeding with any recodes or use of the data. Additionally, data users are encouraged to closely
examine the distribution of their data and value labels to determine if values that appear to be missing
value codes are valid data prior to any recoding.
Composite variables may be derived using data from one or more instrument(s) in one round
of data collection, from instrument data across multiple rounds, or from both instrument data and data
from administrative records in one or more rounds. If a particular composite is inapplicable for a certain
case, for example, as school composite variables are for children who are homeschooled, the variable is
given a value of -1 (not applicable) for that case. In instances where a variable is applicable but complete
information required to construct the composite is not available, the composite is given a value of -9 (not
ascertained). The -7 (refused) code is not used for any of the composites except for the height and weight
composites. The -8 (don’t know) code is not used for any of the composites.
There is variation in the use of system missing for composite variables. Some child
demographic variables (date of birth, sex, and race/ethnicity) are considered applicable to all 18,174
children who participated in the base year and are not assigned a value of system missing for any case.
For composite variables using data from both a survey instrument and other administrative or school data
sources, only nonparticipants in a given round of data collection are assigned values of system missing.
For composite variables using data from only one instrument, (e.g., X9PAR1AGE, parent 1’s age, is
derived from the spring 2016 parent interview), a value of system missing is assigned if the instrument on
which they are based was not completed; if the instrument was completed and an item used in the
composite derivation was missing, the composite is assigned a value of -9 as described above.
7.4 Data Flags
7.4.1 Child Assessment Flags (X9RDGFLG, X9MTHFLG, X9SCIFLG, X9NRFLG,
X9NRGEST, X9DCCSFLG, X9FLNKFLG, X9HGTFLG, X9WGTFLG, X9ASMTST,
X9EXDIS, X9CQFLG)
There are many flags on the data file that indicate the presence or absence of child
assessment data. X9RDGFLG denotes whether a child had scoreable reading assessment data in spring
2016, X9MTHFLG denotes whether a child had scoreable mathematics assessment data in spring 2016,
7-11and X9SCIFLG denotes whether a child had scoreable science assessment data in spring 2016.5 If a child
answered fewer than 10 questions in any direct cognitive assessment domain (reading, mathematics, or
science), the assessment was not considered scoreable. Only items actually attempted by the child counted
toward the scoreability threshold.6 A flag value of 1 indicates that the child responded to 10 or more
questions in the assessment for that domain, and thus has the associated scores. A flag value of 0 indicates
the child had fewer than 10 responses and does not have a score.
X9NRFLG indicates the presence of Numbers Reversed scores and X9DCCSFLG indicates
the presence of Dimensional Change Card Sort (DCCS) scores. X9FLNKFLG indicates the presence of
Flanker scores. X9HGTFLG and X9WGTFLG indicate the presence of data for height and weight in
spring 2016, respectively.
For the Numbers Reversed and DCCS assessments, as long as the child started the
assessment task and answered at least one test question beyond the practice items, a W-ability score (for
Numbers Reversed) or a computed overall score (for DCCS) was computed. Flags for each of the scores
are coded as 1 if the child has a W-ability score (for Numbers Reversed) or a computed overall score (for
DCCS), coded as 0 if the child participated in the child assessment but does not have a score, and set to
system missing if the child did not participate in the child assessment. The Numbers Reversed grade-
normed scores are calculated using information about how far into the school year the assessment
occurred. For some children the school year start and end dates are unavailable, so an estimate based on
the mean of available data is used instead. (Information about the calculation of these grade-normed
scores can be found in section 3.2.2.) The data file includes a flag that indicates whether the assessment
point was estimated for the Numbers Reversed grade-normed scores (X9NRGEST). This flag is set to 0
when actual school start and end dates are known, and set to 1 when the assessment point was estimated.
The child’s assessment status for the spring of 2016 is indicated by the composite
X9ASMTST. The valid values include 1 for children who have any assessment data in the data file,7 2 for
those children who were excluded due to disability (and, therefore, do not have assessment data in the
data file), and 3 for children who do not have assessment data in the data file and were not excluded due
to disability. Note that those excluded due to disability (code 2) are considered to be participants in the
data collection round even if they do not have any parent interview data either.
5 For earlier rounds of data collection, these reading and mathematics flags took into account both the English and Spanish administrations of the
assessments. (The science assessment was administered only in English.) In the fall 2012 and then in every round thereafter, all children received
the reading, mathematics, and science assessments in English so no language of administration is specified here. For more information on the
language of administration, see section 2.1.1.
6 See chapter 3 for a complete discussion of assessment scoreability.
7 Having child assessment data includes (1) having reading and/or mathematics and/or science scores, (2) having at least one executive function
score, or (3) having a height or weight measurement.
7-12In addition, there is a composite variable that uses FMS data to indicate whether the child
was excluded from the assessment due to a disability: X9EXDIS. Study team leaders obtained
information from school staff in the fall of 2015 and spring of 2016 about whether a child had an IEP on
file and if any information in a child’s IEP indicated that he or she would need Braille, large print, or sign
language, accommodations that were not available for the ECLS-K:2011. It was also determined whether
the IEP specifically prohibited the child from participating in standardized assessments such as those
conducted in the ECLS-K:2011. If so, the child was not assessed, and X9EXDIS was coded as 1 (child
was excluded from the assessment due to a disability). Otherwise, X9EXDIS was coded as 0 (child was
not excluded from the assessment due to a disability). Students could have been excluded from taking the
assessment for other reasons (e.g., lack of parental consent); these children are also coded as 0 on
X9EXDIS. The number of cases with system missing values varies across the nine XnEXDIS variables
(that is, one per round), due to the sample for each round. The cases that are system missing on X1EXDIS
are cases that were added to the sample in the spring of the base year and thus were not members of the
sample in round 1. The cases that are system missing on X3EXDIS and X5EXDIS are those that were not
selected for the fall subsample. There are no cases coded as system missing on these variables in rounds
2, 4, 6, 7, 8, and 9.
There is also a flag on the data file, X9CQFLG, that indicates whether there are child
questionnaire data. A flag value of 1 indicates that there are child questionnaire data and a flag value of 0
indicates that there are no child questionnaire data.
7.4.1.1 Child Theta Score Outlier (X_RTOFLG, X_MTOFLG, X_STOFLG)
Child theta score outliers are defined for each round in each subject. Using unweighted data,
an outlier score is defined as one that falls either four standard deviations below the score mean (for that
particular round and subject), or four standard deviations above the score mean (for that particular round
and subject). Four standard deviations of the mean theta was selected as the outlier threshold to identify
extreme outliers, which were expected to comprise less than 1/100th of a percent of the population. Each
of the flags X_RTOFLG, X_MTOFLG, and X_STOFLG is defined as 1 if the theta score is an outlier as
defined above; or as 0 if the theta score is between the values of four standard deviations below and four
standard deviations above the mean.
7-137.4.2 Parent Data Flags (X9PARDAT, X9EDIT, X9BRKFNL)
There is one flag that indicates the presence of parent interview data in spring 2016.
X9PARDAT is coded as 1 if there was a fully completed or partially completed interview in spring 2016.
A partially completed interview in spring 2016 was one that ended before all applicable questions were
answered, but that had answers to questions through FSQ200 (variable P9CURMAR) in the Family
Structure Questions (FSQ) section.
The flag X9EDIT indicates whether, for a given case, household matrix data were reviewed
or edited. It is coded as 1 if a parent interview household matrix was edited (e.g., if the age of a household
member was reported incorrectly and had to be updated, or a person who was added to the household in
error needed to be deleted from the household) or reviewed for editing even if no data were changed (e.g.,
if there were data that suggested a possible problem, but after examining the case the data were left as
they were reported). This flag is included to make users aware that data cleaning or review of household
matrix data was necessary for a particular case. If something about the household composition or
characteristics of the household members seems unusual (e.g., the child is identified as having a 34-year-
old brother in the household) and this flag is set to 1, this is an indication that the unusual data were
reviewed and either edited to appear as they do in the data file or left as is because it was confirmed the
data were accurate or there was no additional information indicating how the data could be edited
accurately. When the flag is set to 1 and data (e.g., for the ages or relationships of household members)
are corrected, the data are only changed in the variables for the round of the study to which the data flag
pertains; no corrections are made to the data for the prior rounds to reflect the later corrections.
Researchers who are using data about household composition from the parent interview household roster
in their analyses should examine all rounds of household roster data closely, recognizing that for a limited
number of cases corrected information from later rounds may need to be applied to earlier rounds. Before
applying changes to earlier-round data, researchers should ensure that they are making changes for the
correct household member(s). It should also be ensured that any changes noted in the relationship
variables are related to the correction of errors and not to real changes in the relationship of household
members to the study child.
The composite variable X9BRKFNL indicates a final breakoff from the round 9 parent
interview. A final breakoff occurs when a respondent stops in the middle of the interview before
answering all applicable questions. These composites identify the variable associated with the last
question answered by the parent. The breakoff point is provided only for those parent interviews with a
status of partially complete. Cases for which a parent completed the interview have a value of -1,
indicating that the case was not a breakoff.
7-147.4.3 Teacher Flags (X9TQTDAT, X9TQTZDAT, X9TQRDGDAT, X9TQMTHDAT,
X9TQSCIDAT, X9MSFLAG, X9SETQA, X9SETQC)
In the spring fifth-grade collection, as in the spring fourth-grade collection, a reading teacher
for each child was identified. In addition, half of the sampled children were randomly assigned to have
their mathematics teacher complete questionnaires, while the other half of the sampled children were
randomly assigned to have their science teacher complete questionnaires. Thus, every child has a reading
teacher and either a mathematics or a science teacher identified for him or her.
These reading, mathematics, and science teachers were asked to complete two types of self-
administered questionnaires, as follows:
1. The teacher-level questionnaire included questions about the teachers, such as their
views on the school climate, their evaluation methods used for reporting to parents,
and their background and education.
2. The child- and classroom-level questionnaire had two parts. Part 1 contained child-
level questions that asked the teacher to rate the study child identified on the cover of
the questionnaire on academic and social skills, school engagement, and classroom
behaviors. Part 2 contained subject matter-specific, class-level questions pertaining to
the reading, mathematics, or science class of the study child. For example, teachers
were asked how much time the study child’s class spends on specific skills and
activities—skills aligned with the Common Core State Standards. This second section
also contained questions on instruction and grading practices, classroom behavioral
issues, and homework assignments.
Since one teacher could instruct multiple study children in the same class, data collection
procedures were implemented to minimize teacher burden by not asking teachers to answer questions
about the same class for multiple children (see Chapter 2). One “key child” was identified for each class,
and the teacher only completed Part 2 (the classroom information) of the child- and classroom-level
questionnaire for this key child. Information collected for the key child was then applied to all study
children in the same reading, math, or science class as the key child. If a teacher taught different classes
of a single subject (e.g., multiple reading classes), a key child was identified for each class, and the
teacher was asked to complete the class-level questions for each section of that subject that he or she
taught. Teachers linked to at least one ECLS-K:2011 child were also asked to complete the teacher-level
questionnaire. Data from the teacher-level questionnaire were linked to every study child in the teacher’s
class(es). The data file contains flag variables that can be used to determine whether data were obtained
7-15from a teacher.8 There are separate subject-matter flag variables corresponding to each type of teacher
questionnaire (teacher-level and child-level). Two flags indicate the type of teacher that completed the
teacher-level questionnaire. X9TQTDAT indicates it was a reading teacher who completed the teacher-
level questionnaire. X9TQTZDAT indicates it was a mathematics or science teacher who completed the
teacher-level questionnaire. X9TQRDGDAT, X9TQMTHDAT, and X9TQSCIDAT are flags to indicate
the subject matter for the child-level questionnaires for reading, mathematics, and science, respectively.
The variable X9MSFLAG indicates whether the child was sampled for the mathematics or science teacher
questionnaire.
Two flags indicate the presence of data from each of the two special education teacher
questionnaires for spring 2016 (X9SETQA for the teacher-level questionnaire and X9SETQC for the
child-level questionnaire). Cases linked to a special education teacher who did not complete a
questionnaire and cases that were not linked to a special education teacher have a value of 0 on these
flags.
Users interested in information about whether special education teacher questionnaires were
requested for a case, regardless of whether special education questionnaires were completed in the spring
of 2016, can use the composite variable X9SPECS (discussed in section 7.5.1.11), which is based on
information from the FMS rather than the special education questionnaires. X9SPECS can be used with
the flags for the presence of data for special education teacher questionnaires, X9SETQA and X9SETQC,
to indicate whether special education questionnaires were requested and received. For example, if
X9SETQA = 0 and X9SPECS = 1, this indicates that the case was linked to a special education teacher
who did not complete a teacher-level special education questionnaire, but special education
questionnaires were requested. If X9SETQA = 0 and X9SPECS = 2, this indicates that the case was not
linked to a special education teacher and special education questionnaires were not requested. X9SPECS
is described further below in section 7.5.1.11.
7.4.4 School Administrator Data Flag (X9INSAQ)
There is a flag for the school administrator questionnaire (X9INSAQ) that is coded as 1 if
there are data from the spring 2016 school administrator questionnaire (SAQ) and 0 if there are no data
from the SAQ.
8 An identification number is provided in the teacher ID variable T9R_ID, T9M_ID, and T9S_ID as long as a child was linked to a reading, math,
or science teacher, even if the teacher did not complete any questionnaires.
7-167.4.5 Child Destination School Flag (X9DEST)
The X*DEST composites identify schools that became a destination school in the given
round. Destination schools are schools for which it was determined that at least four ECLS-K:2011
children moved into them during the same round of the study and from the same original school at which
they were sampled for the study. This typically happened when children attended a school that ended with
a particular grade (e.g., a school that only provided education through first grade) or a school that closed.
Destination schools may be new to the ECLS-K:2011 or may have participated in a past round. A school
already participating in the study could be designated a destination school if four children from the same
original school move into that school. Once a school has been identified as a destination school, it is not
identified as a destination school again in a later round if it subsequently satisfies the conditions for being
labeled a destination school again from the same original school. However, a school may be identified as
a destination school in more than one round of the study if it satisfies the conditions for being labeled a
destination school based on students moving there from another school. Users can identify schools that
were ever designated as destination schools by looking at whether any of the X*DEST composites = 1.
The composite, X9DEST, identifies schools that became destination schools in the current round, round
9. The variable X9DEST is nonmissing for respondents in the spring 2016 round and is coded as 1 if the
child attended a school that became a destination school in the spring of 2016, and 0 otherwise.
7.5 Composite Variables
To facilitate analysis of the survey data, composite variables were derived and included in
the data file. This section identifies the source variables and provides other details for the composite
variables. Most composite variables were created using two or more variables that are also available in
the data file, each of which is named in the text that explains the composite variable. Other composites,
for example, X_CHSEX_R, were created using data from the FMS and the sampling frame, which are not
available in the data file. Note that some of these variables have been updated or revised since their
release on previous data files. Such variables have an “_R” suffix in their name.
7-177.5.1 Child Composite Variables
There are many child-level composite variables in the child catalog. The nonassessment
variables are described in further detail here. The child-level composites for the direct and indirect child
assessment are described in chapter 3.
7.5.1.1 Child’s Date of Birth (X_DOBYY_R and X_DOBMM_R)
The composite variables for the child’s date of birth are based on data from previous rounds
of the study and are the same as the date of birth variables released in the K–4 longitudinal data file
(X_DOBMM_R, X_DOBDD_R,9 and X_DOBYY_R). The child’s date of birth was not collected in the
spring 2016 interview. Information about child’s date of birth was collected from schools at the time of
sampling and stored in the FMS, collected from parents in the fall kindergarten parent interview, and then
collected or confirmed by parents in the spring kindergarten parent interview. (Parents confirmed the
parent report from the fall or FMS data if the fall parent report was not obtained.) Questions to collect
date of birth information were only asked in the fall 2011, spring 2012, fall 2012, or spring 2013
interviews if data from the parent interview about the child’s date of birth were missing due to unit or
item nonresponse. In these rounds of the study, the parent was only asked child’s date of birth if the
parent had not confirmed FMS-reported data (or had not reported date of birth if there were no FMS data)
in a prior interview. In creating the composite, data from the most recent parent interview were given
priority over data from other rounds because they were collected most recently and any data that were
missing from the parent interview due to unit or item nonresponse had the potential to be updated in a
subsequent data collection.
7.5.1.2 Child’s Age at Assessment and the Date of Assessment (X9AGE, X9ASMTDD,
X9ASMTMM, X9ASMTYY)
The child’s age at assessment in months (X9AGE) was calculated by comparing the exact
date the child completed the ECLS-K:2011 direct child assessment according to administrative data that
are not included in the data file to the child’s date of birth (X_DOBDD_R [day of birth],10
X_DOBMM_R [month of birth], X_DOBYY_R [year of birth]). The calculation of age in months uses
9 X_DOBDD_R indicates the child’s exact day of birth. This is an administrative variable that is not included in the K-5 longitudinal data file for
issues related to confidentiality.
10 X_DOBDD_R indicates the child’s exact day of birth. This is an administrative variable that is not included in the K-5 longitudinal data file for
issues related to confidentiality.
7-18the number of days in each month and is adjusted for leap years. The child assessment date was examined
to ensure it was within the field period. If the assessment date fell outside the field period, the modal
assessment date for the child’s school was used to set the composite and was retained for the data file.11
Variables indicating the date of assessment (day, month, and year) in round 9 are also
included in the kindergarten through fifth grade data file. The variable for the day of assessment
(X9ASMTDD) provides a range of days in a month that the child was assessed and is coded as 1 (days 1
through 7); 2 (days 8 through 15); 3 (days 16 through 22); 4 (day 23 or later); or -9 (not ascertained). The
exact day of the month is not provided for reasons related to confidentiality. The variable for the month of
assessment (X9ASMTMM) indicates the month that the child was assessed, and the variable for the year
of assessment (X9ASMTYY) indicates the year that the child was assessed.
7.5.1.3 Child’s Sex (X_CHSEX_R)
The composite variable for the child’s sex is based on data from previous rounds of the study
and is the same as the variable released in the K–4 longitudinal data file (X_CHSEX_R). The child’s sex
was not collected in the spring 2016 interview. Information about child’s sex was collected from schools
at the time of sampling and stored in the FMS, collected from parents in the fall kindergarten parent
interview, and then collected or confirmed by parents in the spring kindergarten parent interview (parents
confirmed the parent report from the fall or FMS data if the fall parent report was not obtained).
Questions to collect information on the child’s sex were only asked in the fall 2011, spring 2012, fall
2012, or spring 2013 interviews if data from the parent interview about the child’s sex were missing due
to unit or item nonresponse. In these rounds of the study, the parent was only asked the child’s sex if the
parent had not confirmed FMS reported data (or had not reported the child’s sex if there were no FMS
data) in a prior interview. In creating the composite, data from the most recent parent interview were
given priority over data from other rounds because they were collected in the most recent interview and
any data that were missing from the parent interview due to unit or item nonresponse had the potential to
be updated in a subsequent data collection.
11 Some assessments that were partially but not entirely completed during the field period were assigned a final status after the end of the data
collection round. Thus, assessment dates after the end of the field period reflect the timing of the assignment of the final disposition, not the
actual date of assessment. These cases were adjusted so that the assessment date reflects the modal date for the school.
7-197.5.1.4 Race/Ethnicity (X_AMINAN_R, X_ASIAN_R, X_HAWPI_R, X_BLACK_R,
X_WHITE_R, X_HISP_R, X_MULTR_R, X_RACETHP_R, X_RACETH_R)
There are three types of composite variables indicating child’s race/ethnicity in the ECLS-
K:2011 file: (1) dichotomous variables for each race/ethnicity category (X_AMINAN_R, X_ASIAN_R,
X_HAWPI_R, X_BLACK_R, X_WHITE_R, X_HISP_R, X_MULTR_R) derived from data collected in
the parent interview; (2) a single race/ethnicity composite derived from data collected in the parent
interview (X_RACETHP_R); and (3) a race/ethnicity composite that draws from either the parent-
reported data about the child’s race or the FMS (X_RACETH_R), with FMS data used only if parent
responses about the child’s race were missing. Parent interview responses about the races of the child’s
biological parents were not used in the creation of child race composite variables. Race/ethnicity
information was updated in these composite variables for about 25 to about 30 cases, depending on the
specific composite, based on information collected from parents in the spring 2016 parent interviews.
Parents were asked about the child’s ethnicity in the spring of 2016 if ethnicity information
for the child from the parent interview items was missing due to unit or item nonresponse. Specifically,
parents were asked whether or not their child was Hispanic or Latino. Parents were also asked about the
child’s race in spring 2016 only if parent interview race data for the child were missing. Parents were
asked to indicate to which of five race categories (White, Black or African American, Asian, Native
Hawaiian or other Pacific Islander, American Indian or Alaska Native) their child belonged, and they
were allowed to indicate more than one. From these responses, a series of five dichotomous race variables
were created that indicate separately whether the child belonged to each of the five specified race groups.
In addition, one additional dichotomous variable was created to identify those who had indicated that their
child belonged to more than one race category.12
The seven dichotomous ethnicity and race variables (X_HISP_R, X_AMINAN_R,
X_ASIAN_R, X_HAWPI_R, X_BLACK_R, X_WHITE_R, X_MULTR_R) were created using parent
data from spring 2016, or if those data were not asked in spring 2016 because they were asked in a
previous round of the study, the dichotomous composites were set to the values of the spring 2015
dichotomous race composites that used parent data from the fourth grade, third grade, second grade, first
grade, and base year collections (X_HISP_R, X_AMINAN_R, X_ASIAN_R, X_HAWPI_R,
X_BLACK_R, X_WHITE_R, X_MULTR_R). Otherwise, the dichotomous ethnicity and race composites
were set to -9 (not ascertained).
12 Unlike the ECLS-K, in the ECLS-K:2011 “other” was not a permitted response for the race question.
7-20Using the six dichotomous race variables and the Hispanic ethnicity variable, the
race/ethnicity composite variables for the child (X_RACETHP_R, X_RACETH_R) were created. The
categories for these variables are: White, non-Hispanic; Black or African American, non-Hispanic;
Hispanic, race specified; Hispanic, no race specified; Asian, non-Hispanic; Native Hawaiian or other
Pacific Islander, non-Hispanic; American Indian or Alaska Native, non-Hispanic; and more than one race
specified, non-Hispanic. A child is classified as Hispanic if a parent indicated the child’s ethnicity was
Hispanic or Latino regardless of whether a race was identified and what that race was. If a child is not
reported to be Hispanic or Latino, the race/ethnicity categories (White, non-Hispanic; Black or African-
American, non-Hispanic; Asian, non-Hispanic; Native Hawaiian or Other Pacific Islander, non-Hispanic;
and American Indian or Alaska Native, non-Hispanic; More than one Race, non-Hispanic) are coded
according to the child’s reported race. If the report about whether the child was Hispanic or Latino was -7
(refused) or -8 (don’t know), or if the child is not Hispanic or Latino and parent reported race is missing,
X_RACETHP_R is coded as -9 (not ascertained); if the report about whether the child was Hispanic or
Latino is also missing from the FMS, or if the child is not Hispanic or Latino and race is also missing
from the FMS, X_RACETH_R is coded as -9 (not ascertained). The difference between X_RACETHP_R
and X_RACETH_R is that if race or ethnicity data are missing from the spring 2016 parent interview,
X_RACETH_R is set to the value used for the spring 2015 composite, also called X_RACETH_R, which
uses both parent data and FMS data, while only parent-report data were used for the variable
X_RACETHP_R. Thus, there are more missing data for X_RACETHP_R than for X_RACETH_R.
About 29 cases have a value for X_RACETHP_R that is different in the K-5 longitudinal file
than in the K-4 longitudinal file due to the collection of child race/ethnicity data in the spring 2016 parent
interview. About 9 of these cases changed value from -9 (not ascertained) to a valid value and about 20
cases changed from code 4, Hispanic-no race reported, to code 3, Hispanic-race reported. About 25 cases
have a changed value for X_RACETH_R due to the collection of child race/ethnicity data in the spring
2016 parent interview. Nearly all of these cases, about 22, changed from code 4, Hispanic-no race
reported, to code 3, Hispanic-race reported.
The categories for X_RACETHP_R and X_RACETH_R are mutually exclusive, meaning
that a child is coded as just one race/ethnicity. Users interested in the specific races of children who are
identified as multiracial, or who are interested in identifying the race(s) of children who are identified as
Hispanic, should use the dichotomous race variables discussed above.
7-217.5.1.5 Child’s Height (X9HEIGHT)
To obtain accurate measurements, each child’s height was measured twice in each data
collection round. The height measurements were entered into the computer program used for the
assessment, with a lower limit set at 35 inches and an upper limit set at 80 inches.
For the height composites, if the two height measurements (C9HGT1 and C9HGT2 for
spring 2016) were less than 2 inches apart, the average of the two height values was computed and used
as the composite value. If the two spring measurements were 2 inches or more apart, for X9HEIGHT (the
child’s height in spring 2016), the measurement that was closest to 56.75 inches for boys and 56.80
inches for girls was used as the composite value. This is the 50th percentile height for children who were
11 years old (133.29 months for boys and 132.68 months for girls: the average age at assessment in spring
2016 using the composite X9AGE). The height averages come from the 2000 Centers for Disease Control
and Prevention (CDC) Growth Charts (www.cdc.gov/growthcharts/html_charts/statage.htm).13 The two
height measurements were 2 or more inches apart for 11 cases for X9HEIGHT.
If one value for height was missing, the other value was used for the composite. If both the
first and second measurements of height were coded as -8 (don’t know), then the height composite was
coded as -9 (not ascertained). Children who did not have their height measured due to a physical disability
were coded as -8 (don’t know) for both height measurements and, therefore, have a code of -9 on the
composite. If both the first and second measurements of height were coded as -7 (refused), then the height
composite was coded as -7 (refused). If both the first and second measurements of height were coded as
-9 (not ascertained) because height data were missing as the result of a breakoff in the child assessment or
the measurements had different missing values (e.g., one was -8 and the other was -9), then the height
composite was coded as -9 (not ascertained).
For about 130 cases, the child’s height in the spring of 2016 (X9HEIGHT) was shorter than
in the spring of 2015 (X8HEIGHT). A difference of 1 inch or less (about 30 children) could be a function
of factors such as slouching versus standing upright; differences in shoes, hairstyle, thickness of socks; or
a combination of these factors. However, about 100 children were recorded as being more than 1 inch
shorter in the spring of 2016 than in the spring of 2015, and about 80 of those were recorded as being
more than 2 inches shorter. These discrepancies may result from measurement error or recording error.
Analysts should use their own judgment in how to use these cases in their analysis.
13 For calculating the median height, the composite X9AGE was used to determine children’s average age at assessment. The average age at
assessment in spring 2016 was 133.29 months for boys and 132.68 months for girls using the composite X9AGE. The closest value on the CDC
Growth Chart was 133.5 for boys and 132.5 for girls.
7-227.5.1.6 Child’s Weight (X9WEIGHT)
To obtain accurate measurements, each child’s weight was measured twice in each data
collection round. The weight measurements were entered into the computer program used for the
assessment, with a lower limit set at 30 pounds and an upper limit set at 300 pounds. Values outside the
range that were documented in assessor comments as being valid measurements were included in the data
file.
For the weight composites, if the two weight measurements obtained within a round (i.e.,
C9WGT1 and C9WGT2 for spring 2016) were less than 5 pounds apart, the average of the two weight
values was computed and used as the composite value. If the two measurements were 5 or more pounds
apart, for X9WEIGHT the measurement that was closest to 80.32 pounds for boys or 82.43 pounds for
girls was used as the composite value. These are the median weights for children who were 11 years old
(133.29 months for boys and 132.68 months for girls: the average age at assessment in spring 2016 using
the composite X9AGE). The weight averages come from the 2000 CDC Growth Charts
(see www.cdc.gov/growthcharts/html_charts/wtage.htm).14 The two weight measurements were 5 or more
pounds apart in 5 cases for X9WEIGHT.
If one value for weight was missing, the other value was used for the composite. If both the
first and second measurements of weight were coded as -8 (don’t know), the weight composite was coded
as -9 (not ascertained). Children who did not have their weight measured due to a physical disability were
coded as -8 (don’t know) for both weight measurements and, therefore, have a code of -9 on the
composite. If both the first and second measurements of weight in the child assessment were coded as -7
(refused), then the weight composite was coded as -7 (refused). If both the first and second measurements
of weight in the child assessment were coded as -9 because weight data were missing as the result of a
breakoff in the child assessment or the measurements had different missing values (e.g., one was -8 and
the other was -9), then the weight composite was coded as -9 (not ascertained).
There are approximately 40 children whose round 9 weights are more than 10 pounds lower
than their round 8 weights; of these, about 25 changes are in the range of 20.75 pounds to 114.5 pounds.
It is possible that some of these changes result from measurement error. Analysts may wish to review
such cases and determine how to account for these weight changes in their analysis.
14 For calculating the median weight, the composite X9AGE was used to determine children’s average age at assessment. The average age at
assessment in spring 2016 was 133.29 months for boys and 132.68 months for girls using the composite X9AGE. The closest value on the CDC
Growth Chart was 133.5 for boys and 132.5 for girls.
7-237.5.1.7 Child’s Body Mass Index (X9BMI)
Composite body mass index (BMI) was calculated by multiplying the composite weight in
pounds by 703.0696261393 and dividing by the square of the child’s composite height in inches (Keys et
al. 1972; Mei et al. 2002). Unrounded values of height and weight were used in the calculation of BMI. If
either the height or weight composite was coded as -9 (not ascertained) or -7 (refused), the BMI
composite was coded as not ascertained (-9). Values of “don’t know” for height and weight were coded as
-9 (not ascertained) in the height and weight composites and also coded as -9 (not ascertained) in the BMI
composite.
7.5.1.8 Child’s Disability Status (X9DISABL2, X9DISABL)
Two composite variables based on information obtained in the parent interview were created
to indicate whether a child had a disability diagnosed by a professional. Note that these variables indicate
either diagnosed disabilities that were identified for the first time in the round 9 parent interview or
diagnoses reported in a previous interview for which the child also had a diagnosis reported in round 9.
The variables must be used in conjunction with the disability composites from earlier rounds to identify
the entire group of children who have ever had a disability diagnosed by a professional. Also, these two
variables differ in how missing data were treated during their creation, as described below.
Questions in the spring 2016 parent interview asked about the child’s ability to be
independent and take care of himself or herself, ability to pay attention and learn, coordination in moving
arms and legs, overall activity level, overall behavior and ability to relate to adults and children,
emotional or psychological difficulties, ability to communicate, difficulty in hearing and understanding
speech, and eyesight. If parents indicated that their child had any issues or difficulties in response to these
questions, follow-up questions asked whether the child had been evaluated by a professional for that
particular issue and whether a diagnosis of a problem was obtained by a professional (CHQ120, CHQ125,
CHQ215, CHQ245, CHQ246, CHQ300, CHQ301). A question was also asked about current receipt of
therapy services or participation in a program for children with disabilities (CHQ340).
The composite variable X9DISABL is coded as 1 (yes) if the parent answered “yes” to at
least one of the questions about diagnosis (indicating a diagnosis of a problem was obtained) or therapy
services (indicating the child received services) (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) and
the questions about the specific diagnoses (CHQ125, CHQ246, CHQ301) were not coded as -7 (refused),
7-24-8 (don’t know), or -9 (not ascertained); or in the case of the vision diagnosis (CHQ301), the question was
not coded as only nearsightedness (myopia), farsightedness (hyperopia), color blindness or deficiency,
astigmatism, or awaiting evaluation; or in the case of a hearing diagnosis (CHQ246), the question was not
coded as only external ear canal ear wax or awaiting evaluation.
Using these criteria to calculate X9DISABL, a child could be coded as having a disability
even if data for some of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245,
CHQ300, CHQ340) were missing. This is because a child is coded as not having a disability if there are
data for at least one of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245,
CHQ300, CHQ340), and the response was either 2 (no) or the item was -1 (inapplicable) (because the
child did not have issues that indicated a question should be asked), even if data for some of these
questions were missing. In addition to having “no” answers or “inapplicable” codes for the diagnoses or
therapy services questions, if the child had a diagnosis, but the specific diagnosis was not reported (was
refused, don’t know, not ascertained) or the diagnosis has not been received (CHQ246 or CHQ301 = 1 for
awaiting evaluation), X9DISABL was also coded as 2 (no) because there was no reported disability. The
composite was coded as -9 (not ascertained) only if all of the data for the questions about diagnoses or
therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) were -7 (refused), -8 (don’t know),
or -9 (not ascertained), or if the items that skipped to these items were -7 (refused), -8 (don’t know), or -9
(not ascertained).
A more conservative approach when coding cases that had incomplete data for the diagnoses
and services variables was used to derive the variable X9DISABL2. Whereas X9DISABL codes cases
with missing data as “no” as long as all the information that was collected indicates the child does not
have a diagnosed disability or receive services for a diagnosed disability, X9DISABL2 is coded as -9 (not
ascertained) when any of the questions about diagnoses or therapy services (CHQ120, CHQ215,
CHQ245, CHQ300, CHQ340) are -7 (refused), -8 (don’t know), or -9 (not ascertained), or the items that
skipped to these items are -7 (refused), -8 (don’t know), or -9 (not ascertained). For X9DISABL2, if there
are no “yes” answers for a disability, but any of the evaluation (CHQ115, CHQ210, CHQ235, CHQ290),
diagnoses (CHQ120, CHQ215, CHQ245, CHQ300), or therapy questions (CHQ340) are -7 (refused), -8
(don’t know), or -9 (not ascertained),15 or if any of the evaluation, diagnosis, or therapy questions were
not asked (were -1 for inapplicable) because of missing data for questions that skipped to those questions
(and thus it is not known if they should have been asked), X9DISABL2 is coded as -9 (not ascertained).
In addition, if the parents indicated that a diagnosis had been obtained, but the specific diagnosis was
15 If CHQ340 was -9 (not ascertained) because the interview broke off after CHQ330, but all answers in CHQ330 and questions prior to CHQ330
indicated that CHQ340 would not have been applicable, X9DISABL and X9DISABL2 were coded 2 (no disability) because that question would
not have been asked for those children.
7-25coded as refused, don’t know, or not ascertained, X9DISABL2 is coded as -9 (not ascertained). This
approach is more conservative because it does not assume that the response for unanswered questions was
“no.” Due to these differences in coding, the number of cases identified as not having a diagnosed
disability is higher for X9DISABL than it is for X9DISABL2.
7.5.1.9 Primary Language in the Child’s Home (X9LANGST)
A composite variable was created to indicate whether English was a primary language
spoken in the home or whether a non-English language was the primary language spoken in the spring of
2016. Parents were asked if any language other than English was regularly spoken in their home
(P9ANYLNG). If a language other than English was not spoken in the home (P9ANYLNG = 2, or if a
language other than English was spoken in the home but the primary language of the household
(P9PRMLNG) was English (P6PRMLNG = 0), the composite is coded as 2 (English language).
If both English and another language were spoken in the home, and the respondent reported
that two or more languages were spoken equally or they could not choose a primary language, the
composite is coded as 3 (cannot choose primary language or two languages equally). Otherwise, if a
language other than English was spoken (P9ANYLNG = 1), either solely (P9ENGTOO) or primarily in
the home (P9PRMLNG has a nonmissing value other than 0 for English), the composite is coded as
1 (non-English language). Otherwise, if there were missing data, X9LANGST is set to the value for the
most recent language composite from a previous round of the study (X6LANGST, X4LANGST, or
X12LANGST).
7.5.1.10 Student Grade Level (X9GRDLVL)
The X9GRDLVL composite indicates the child’s grade level in the spring of 2016 as
reported by the teacher or recorded in the FMS. This composite has valid values for the 12,346 cases that
are respondents for round 9, that is the cases that have either child assessment or parent interview data. It
is constructed using F9CLASS2 (child’s grade in spring 2016 from the FMS). The values include 2 for
second grade, 3 for third grade, 4 for fifth grade, 5 for fifth grade, and 6 for sixth grade.
Note that grade level (F9CLASS2) is included for homeschooled children. For all children,
their grade was known at their initial sampling in school. Based on the assumption that most children
progress a grade level each year, for homeschooled children and other assess-in-home children, the grade
7-26was incremented by 1 year each year. In spring 2011, fall 2011, spring 2012, and fall 2012, the child’s
grade was confirmed with the parent for these cases. In the spring of 2013, 2014, 2015, and 2016, parents
were not asked for this information. The grade level from the spring of 2015 was increased by one grade
for the spring of 2016. This change was also made for cases that started homeschooling in the 2015–16
school year. If a parent volunteered new information about grade level, field team leaders updated the
information in the FMS and that information is reflected in the composite variable.
7.5.1.11 Child Linked to a Special Education Teacher (X9SPECS)
The composite variable X9SPECS indicates whether or not children were linked to a special
education teacher and special education questionnaires were requested from teachers in the spring of
2016, based on the presence or absence of a link to a special education teacher or related service provider
in the FMS. The value is 1 if special education questionnaires were requested and 2 if special education
questionnaires were not requested. Study team leaders asked school staff if any accommodations were
required for the study children to be assessed. During that discussion about assessment accommodations,
team leaders were also supposed to record whether the child had an IEP on file with the school but did not
require any accommodations for the study assessments. The link to a special education teacher was
established automatically when information indicating a child needed an accommodation or had an IEP
but did not require an accommodation was entered in the FMS by study team leaders.
There are cases with a mismatch between X9SPECS and special education teacher reports
about an IEP. In about 130 cases in spring 2016, there were FMS data indicating the child had an IEP on
record at the school (and thus a special education teacher questionnaire was requested from the teacher
and X9SPECS = 1), but the special education teacher indicated in the child-level questionnaire that the
child did not have an IEP (E9RECSPE = 2).
7.5.2 Family and Household Composite Variables
Many composite variables are created to provide information about the sampled children’s
family and household characteristics. It must be noted that household composition composite variables
consider only those people who were household members at the time of the parent interview. If
information on household composition was collected in the spring 2015, spring 2014, spring 2013, spring
2012, spring 2011, or fall 2010 parent interview, the parent respondent was asked to indicate whether the
people living in the household in the most recent interview in which information about household
7-27composition was collected were still in the household at the time of the spring 2016 parent interview, as
well as whether there were any new members of the household. Household members were accounted for
in the derivation of the spring 2016 composite variables if they were still living in the household or had
joined the household since the time of the last interview, as indicated in the variables P9CUR_1–
P9CUR_25.
During the spring 2016 parent interview, information on age, sex, and relationship to the
study child was collected for all new household members. For certain existing household members,
information was collected about whether their relationship to the study child had changed since the
previous interview in which relationship data was collected. Change in relationship was asked for
respondents and their spouses or partners who were identified in a prior round interview as being a step-
or foster mother or father, other male or female parent or guardian, boyfriend or girlfriend of the child’s
parent, relative, or nonrelative. Information about race and ethnicity was collected for specific household
members (parents/guardians and their spouses/partners, or boyfriends/girlfriends if the
boyfriend/girlfriend was a respondent) who were new to the household or had missing race or ethnicity
data from a previous round of the study.
The composite variables for parents (e.g., parent age, parent education) are for the parents
who were members of the household at the time of the spring 2016 interview. The identities of household
parent figures can change over time, meaning that data in a composite may not pertain to the parent figure
in the household in an earlier round. For example, parent education information collected in the spring
2016 parent interview would pertain to a father figure who was in the home during that round but not
necessarily to a father figure who was in the household during the kindergarten, first-, second-, third-, or
fourth-grade years. Users should look at the X9IDP1 and X9IDP2 variables described in section 7.5.2.3 to
determine if the household roster numbers associated with parent 1 and parent 2 in the spring of 2016
match the household roster numbers for parent 1 and parent 2 from an earlier round (e.g., X8IDP1 or
X8IDP2) in order to determine if the parent figures changed.
7.5.2.1 Household Counts (X9HTOTAL, X9NUMSIB, X9LESS18, X9OVER18)
The composite variable X9HTOTAL provides a count of the total number of household
members in the spring of 2016. For households for which household roster information had been
collected in a prior round, this count is the number of household members who were previously rostered
and reported to still be in the household plus any new persons added after the last interview in which
roster information was collected.
7-28Two composite variables take the ages of the household members into account to indicate
the total numbers of (1) adults and (2) children in the household in the spring of 2016. Information about
household members’ ages was collected in the household matrix, or roster, section of the parent interview
(see below for details in section 7.5.2.2). X9LESS18 indicates the total number of people in the
household under age 18, including the study child, siblings, and other children, and X9OVER18 indicates
the total number of people in the household age 18 or older. All household members who were 18 years
old or older, as well as anyone identified as a parent or grandparent of the focal child whose age is
missing, are counted in the total for X9OVER18. Households with members with missing age
information who are not identified as a parent or grandparent are coded as -9 (not ascertained) on
X9OVER18 and X9LESS18. X9LESS18 is created by subtracting X9OVER18 from X9HTOTAL.
The composite X9NUMSIB indicates the total number of siblings (biological, step-,
adoptive, or foster) living in the household with the study child. Siblings were identified by questions in
the FSQ section of the parent interview that asked about the relationship of each household member to the
study child. X9NUMSIB does not count children of the parent’s boyfriend or girlfriend (identified by the
code 5 in the variables associated with question FSQ180) as siblings.
7.5.2.2 Household Rosters
The ECLS-K:2011 data file includes rosters of the household members as collected in the
parent interviews. The roster information appears as part of the block of Family Structure Questions
(FSQ) for each round in which the FSQ section was included in the parent interview. Variable names
begin with P1 for round 1 (fall kindergarten); P2 for round 2 (spring kindergarten); P4 for round 4 (spring
2012, when most children were in first grade); P6 for round 6 (spring 2013, when most children were in
second grade); P7 for round 7 (spring 2014, when most children were in third grade); P8 for round 8
(spring 2015, when most children were in fourth grade); and P9 for round 9 (spring 2016, when most
children were in the fifth grade). No FSQ section was included in the brief round 3 or round 5 parent
interviews.
For each household member in each round, roster variables include the following, where * is
the round number (1, 2, 4, 6, 7, 8, or 9) and # is the household roster number (1 through 25):
 P*PER_#, person type, whether the person is the focal child, respondent, or
spouse/partner of the respondent;
7-29 P*AGE_#, the person’s age;
 P*SEX_#, the person’s sex;
 P*REL_#, how the person is related to the focal child;
 P*MOM_#, if the person is the child’s mother, the type of mother;
 P*DAD_#, if the person is the child’s father, the type of father;
 P*SIS_#, if the person is the child’s sister, the type of sister;
 P*BRO_#, if the person is the child’s brother, the type of brother;
 P*UNR_#, if the person is not a relative, the type of relationship to the study child;
 P*HSP_#, whether the child or parent/guardian is of Hispanic or Latino origin;
 P*AIA_#, whether the child or parent/guardian is American Indian or Alaska Native;
 P*ASN_#, whether the child or parent/guardian is Asian;
 P*BLK_#, whether the child or parent/guardian is Black or African American;
 P*HPI_#, whether the child or parent/guardian is Native Hawaiian or other Pacific
Islander; and
 P*WHT_#, whether the child or parent/guardian is White.
For rounds 2, 4, 6, 7, 8, and 9 there are two additional variables:
 P*CUR_#, whether the person was currently a household member at the time of the
interview; and
 P*REASL#, if the person left the household, the reason for doing so.
For round 2, there are two additional variables.16
 P2JOI_#, the round in which the person was first enumerated as a household member;
and
 P2RDP_#, the round in which the person left the household.
16 In round 2, variables identifying in which round a person was first enumerated as a household member and in which round a person was
identified as having left the household were set in the CAPI parent interview and included in the base-year data file. For later rounds, analysts can
compare the P*CUR_# variables (person is currently a household member) from different rounds, where * is the round number and # is the
person number, to determine in which round a person was first enumerated as a household member and in which round a person was identified as
having left the household.
7-30Once a person is assigned a household roster number, he or she retains that number
permanently. For example, if there are four persons in the household and person 3 leaves the household,
person 4 remains in position 4 in the roster for all rounds. Similarly, if the last person on the roster leaves
the household and a new person subsequently joins the household, that new household member is
assigned to the position below that of the person who was last in the roster and left (for example, if person
6 is the last person on the roster and leaves the household, a new person joining the household would be
assigned to position 7).
If there is no parent interview completed in a given round, then the roster items for that
round are assigned a value of system missing. Beginning in round 4, if a person has left the household
(e.g., P4CUR_# = 2, not a current household member), the roster variables for that position are assigned a
value of -1 for that round and subsequent rounds in which a parent interview is completed.
In rare cases, only in rounds 4 and 6, there are roster positions for which all values are
system missing or -1 across all rounds but P4CUR_# = 2 or P6CUR_# = 2 (not a current household
member). This may occur because a new household member was the respondent for round 3 or 5, when
there was no roster completion or confirmation in the parent interview, and that person had left the
household before the next parent interview in which complete household composition information was
collected.17
Determining household membership in a given round. In round 1, respondents were not
asked if persons were currently household members, because that was the first household enumeration for
the study and all enumerated persons were household members at that time. For rounds 2, 4, 6, 7, 8, and 9
analysts can determine the current household membership at the time of the parent interview for the round
by examining the variables P2CUR_#, P4CUR_#, P6CUR_#, P7CUR_#, P8CUR_#, and P9CUR_#,
respectively. Analysts should not look for the first “empty” position in the roster series to determine the
last person with roster data in the household, since, as noted above, all persons retain their household
positions permanently; if person 3 leaves the household, then person 4 still remains in position 4.
17 Because there was not a household roster in the fall 2011 or fall 2012 parent interviews, there are potentially other household members who
were present in the fall of 2011 or the fall of 2012 and had left the household by the time of the subsequent parent interviews. There would be no
record of these household members in the study.
7-317.5.2.3 Parent Identifiers and Type in the Household (X9IDP1, X9IDP2, X9HPAR1,
X9HPAR2, X9HPARNT)
X9IDP1 and X9IDP2 indicate the positions in the household roster of the sampled child’s
residential parent/parent figure(s) in the spring of 2016.18 The construction of parent identifiers and the
household composition variables from the parent interview data was a multistep process. First, it was
determined from household roster variables whether there was a mother (biological, adoptive, step-, or
foster) and/or a father (biological, adoptive, step-, or foster) in the household. Using this information, the
method described below was used to create X9IDP1 and X9IDP2 for the spring.
1. 2. 3. 4. 5. 6. If there was only one mother (of any type, including unknown type) and only one
father (of any type, including unknown type) in the household, the mother was
identified as parent 1 (X9IDP1) and the father was identified as parent 2 (X9IDP2).
If there was only one mother (of any type, including unknown type) in the household
and no other parent figure (of any type), the mother was identified as parent 1 and
parent 2 is coded as -1 (not applicable). If there was a mother and she had a male
spouse/partner in the household who was not identified as a father (of any type,
including unknown type), the spouse/partner was identified as parent 2.
If there was only one father (of any type, including unknown type) in the household
and no other parent figure (of any type), the father was identified as parent 1 and
parent 2 is coded as -1 (not applicable). If there was a father and he had a female
spouse/partner in the household who was not identified as a mother (of any type), the
spouse/partner was identified as parent 1 and the father was identified as parent 2.
If there were two mothers (or a mother and female spouse/partner) in the household,
an order of preference was used to identify one mother to be parent 1, with the order
specified as biological, adoptive, step-, foster mother or female guardian, then other
female parent or guardian.19 The other mother was identified as parent 2. If there were
two mothers of the same type (e.g., two adoptive mothers) or there were two mothers
and the type for both was -7 (refused) or -8 (don’t know), the mother with the lowest
household roster number was identified as parent 1 and the other mother was
identified as parent 2.
If there were two fathers in the household (or a father and male spouse/partner), an
order of preference was used to identify one father to be parent 1, with the order
specified as biological, adoptive, step-, foster father or male guardian, then other male
parent or guardian. The other father was identified as parent 2. If there were two
fathers of the same type (e.g., two adoptive fathers) or there were two fathers and the
type for both was -7 (refused) or -8 (don’t know), the father with the lowest household
roster number was identified as parent 1 and the other father was identified as
parent 2.
If there was no one in the household identified as a mother or father, then a female
respondent or the female spouse or partner of a male respondent was identified as
18 In the ECLS-K, the parent identifiers were P*MOMID and P*DADID and specifically identified the mother/female guardian and father/male
guardians, respectively, in the household. The format of the parent identifiers was changed in the ECLS-K:2011 to allow for more accurate
identification of households with two mothers/female guardians or two fathers/male guardians.
19 There were new categories in the ECLS-K:2011 parent interview for “Other female parent or guardian” in FSQ140 and “Other male parent or
guardian” in FSQ150 that were not included in the ECLS-K.
7-32parent 1. If the female parent figure had a male spouse or partner, the spouse/partner
was identified as parent 2. If the respondent was male and had a female spouse or
partner, she was designated as parent 1 and he was designated as parent 2. For
example, if a child lived with his grandmother (the respondent) and grandfather, and
neither his mother nor father lived in the household, then the grandmother was
identified as parent 1 and the grandfather was identified as parent 2. If the grandfather
lived in the household, but no grandmother or parents lived there, the grandfather
respondent would be parent 1 and parent 2 would be coded as -1. Demographic
information such as age, race, and education was collected for these “parent figures.”
Once parents/parent figures were identified, X9HPAR1 and X9HPAR2 were created to
identify the specific relationship of parent 1 and parent 2 to the study child.20 It should be noted, however,
that for households in which the child lived with parent figures other than his or her mother and/or father,
the parent figures identified in X9IDP1 and X9IDP2 were not defined as parents (meaning biological,
step-, adoptive, or foster) for the construction of X9HPAR1 and X9HPAR2. For example, if there are a
grandmother and grandfather and there are no parents listed in the household, X9HPAR1 and X9HPAR2
would be coded as category 15 (no resident parent).
X9HPARNT indicates the type(s) of parents living in the household with the study child.
The values for the X9HPARNT composite are as follows:
 1 = two biological/adoptive parents;
 2 = one biological/adoptive parent and one other parent/partner;
 3 = one biological/adoptive parent only; and
 4 = other guardian(s).
When study children are living with parent figures (e.g., grandmother and grandfather),
rather than biological, adoptive, step-, or foster parents, X9HPARNT is coded as 4.
The composite parent identifier variables X9IDP1 and X9IDP2 are used to determine which
composite variables correspond to parent 1 and parent 2, respectively. These “pointer” variables indicate
the household roster number of the person who was the subject of the questions being asked. All parent
composite variables that include “PAR” and the number 1 in the variable name are associated with the
person designated in X9IDP1, who is parent 1. All parent composite variables that include “PAR” and the
number 2 in the variable name are associated with the person designated in X9IDP2, who is parent 2. In
the spring 2016 parent interview, there are two sets of questions that were first asked about parent 1 and
then asked about parent 2 if the household contained two parents.
20 These variables are a combination of P*HMOM and P*HDAD from the ECLS-K.
7-33 The first set of questions about parent 1 and parent 2 were about parent education. For
parent education, there is also a second set of “pointer” variables that hold the
household roster number of the person who was the subject of the education questions
(P9PEQHH1 and P9PEQHH2). For the education questions, the pointer variables are
applicable to up to two parents in the household. If there are two parents in the
household, P9PEQHH1 and P9PEQHH2 are the roster numbers of the first and second
parent, respectively. If there is only one parent in the household, P9PEQHH1 is the
roster number of the first parent and P9PEQHH2 = -1 (not applicable). Since the
parent education questions were asked only of parent(s) or parent figure(s) in the
household, the value of parent education pointer variables is the same as the value for
the composite parent identifier variables.
 The second set of questions about parent 1 and parent 2 asks about parent
employment. There is also a set of “pointer” variables that hold the household roster
number of the person who was the subject of the employment questions (P9EMPP1
and P9EMPP2). For the employment questions, the pointer variables are applicable to
up to two parents in the household. If there are two parents in the household,
P9EMPP1 and P9EMPP2 are the roster numbers of the first and second parent,
respectively. If there is one parent in the household, P9EMPP1 is the roster number of
the first parent and P9EMPP2 = -1 (not applicable). The value of employment pointer
variables is the same as the value for the composite parent identifier variables.
To illustrate how the pointer variables work, suppose there is a household with both a
mother and a father who were listed as the third and fifth individuals in the household roster. According
to the rules outlined above, household member #3, the mother, becomes parent 1 and X9IDP1 equals 3.
All applicable pointer variables for parent 1 will subsequently take on the value 3. Similarly, household
member #4, the father, becomes parent 2 and X9IDP2 equals 4. All applicable pointer variables for parent
2 will subsequently take on the value 4.
Table 7-1 identifies the PEQ and EMQ section pointer variables included in the data file
along with the interview items and variables associated with those pointer variables. The pointer variables
are necessary to determine which parent should be assigned the answers to items about employment.
Returning to the example above, the answers to the employment questions for the mother are stored in
variables that end with the suffix “1” since the mother was identified as parent 1, and her household roster
number is the value in X9IDP1. For example, P9EMPSIT1_I and P9EVRACTV1 indicate the mother’s
current employment situation and whether the mother has been on active duty in the military since the
child was born, respectively. The answers to the employment questions for the father are stored in
variables that end with the suffix “2” since the father was identified as parent 2, and his household roster
number is the value in X9IDP2. For example, P9EMPSIT2_I and P9EVRACTV2 indicate the father’s
current employment situation and whether the father has been on active duty in the military since the
child was born, respectively.
7-34Table 7-1. Pointers to parent figure questions: School year 2015–16
Person pointer Variable names and labels
P9PLQHH1 P9 PLQ083-090
HH PERSON
POINTER 1
P9RES_1 P9 PLQ083 PERSON 1 LANGUAGE TO CHILD
P9CHL_1 P9 PLQ090 CHILD’S LANGUAGE TO PERSON 1
P9PLQHH2 P9 PLQ083-090
HH PERSON
POINTER 2
P9RES_2 P9 PLQ083 PERSON 2 LANGUAGE TO CHILD
P9CHL_2 P9 PLQ090 CHILD’S LANGUAGE TO PERSON 2
P9PEQHH1 P9 PEQ020–021
PERSON 1
ROSTER
NUMBER
P9HIG_1 P9 PEQ020 PERS 1 HIGHEST EDUCATION LEVEL
IFP9HIG_1 P9 IMPUTATION FLAG FOR P9HIG_1_I
P9HIS_1 P9 PEQ021 IF PERS 1 HIGH SCHOOL DIPLOMA/GED
IFP9HIS_1 P9 IMPUTATION FLAG FOR P9HIS_1_I
P9PEQHH2 P9 PEQ020–021
PERSON 2
ROSTER
NUMBER
P9HIG_2 P9 PEQ020 PERS 2 HIGHEST EDUCATION LEVEL
IFP9HIG_2 P9 IMPUTATION FLAG FOR P9HIG_2_I
P9HIS_2 P9 PEQ021 IF PERS 2 HIGH SCHOOL DIPLOMA/GED
IFP9HIS_2 P9 IMPUTATION FLAG FOR P9HIS_2_I
P9EMPP1 P9 EMQ020-215
PERSON 2
ROSTER
NUMBER
P9EMPCHG_1_I P9 EMQ010 EMPLOYMENT CHANGED
IFP9EMPCHG_1 P9 IMPUTATION FLAG FOR P9EMPCHG_1
P9PAY_1_I P9 EMQ020 PERS 1 HAD PAID JOB LAST WEEK
IFP9PAY_1 P9 IMPUTATION FLAG FOR P9PAY_1
P9VAC_1_1 P9 EMQ030 IF PERS 1 ON LEAVE PAST WEEK
IFP9VAC_1 P9 IMPUTATION FLAG FOR P9VAC_1
P9JOB_1 P9 EMQ040 PERSON 1 NUMBER OF CUR JOBS
P9HRS_1_I P9 EMQ050 PERSON 1 HOURS/WK AT ALL JOBS
IFP9HRS_1 P9 IMPUTATION FLAG FOR P9HRS_1
P9LOK_1_I P9 EMQ060 PERS 1 SOUGHT JOB LAST 4 WEEKS
IFP9LOK_1 P9 IMPUTATION FLAG FOR P9LOK_1
P9DO1_1_I P9 EMQ070 PERS 1 CHKD W/PUB EMPL AGNCY
IFP9DO1_1 P9 IMPUTATION FLAG FOR P9DO1_1
P9DO2_1_I P9 EMQ070 PERS 1 CHKD W/PRIV EMP AGNCY
IFP9DO2_1 P9 IMPUTATION FLAG FOR P9DO2_1
P9DO3_1_I P9 EMQ070 PERS 1 CHKD W/EMPLOYR DIRECTLY
IFP9DO3_1 P9 IMPUTATION FLAG FOR P9DO3_1
P9DO4_1_I P9 EMQ070 PERS 1 CHKD W/FRIENDS AND REL
IFP9DO4_1 P9 IMPUTATION FLAG FOR P9DO4_1
P9DO5_1_I P9 EMQ070 PERS 1 PLACED OR ANSWERED ADS
IFP9DO5_1 P9 IMPUTATION FLAG FOR P9DO5_1
P9DO6_1 P9 EMQ070 PERS 1 CHKD SCHL/UNIV EMPL CTR
IFP9DO6 P9 IMPUTATION FLAG FOR P9DO6_1
P9D07_1 P9 EMQ070 PERS1 CHKD UNION/PROFSSL REG
IFP9DO7 P9 IMPUTATION FLAG FOR P9DO7_1
P9DO8_1 P9 ATTENDED JOB TRAINING
P9DO9_1 P9 EMQ070 PERS 1 RD WANT ADS/INTRNT SRCH
P9DO10_1 P9 EMQ070OS PERS 1 DID SOMETHING ELSE
P9DOW_1 P9 EMQ080 WHAT PERSON 1 DOING LAST WEEK
P9TAK_1 P9 EMQ100 PERS 1 COULD TAKE JOB LAST WK
P9EVRACTV1 P9 EMQ210 PERS 1 SERVED ACTIVE DUTY
P9CURACTV1 P9 EMQ215 PERS 1 CURR ON ACTIVE DUTY
See note at end of table.
7-35Table 7-1. Pointers to parent figure questions: School year 2015–16—Continued
Person pointer Variable names and labels
P9EMPP2 P9 EMQ020-215
PERSON 2
ROSTER
NUMBER
P9EMPCHG 2 I IFP9EMPCHG_2 P9 EMQ010 EMPLOYMENT CHANGED
P9 IMPUTATION FLAG FOR P9EMPCHG_2
P9PAY_2_I P9 EMQ020 PERS 1 HAD PAID JOB LAST WEEK
IFP9PAY_2 P9 IMPUTATION FLAG FOR P9PAY_2
P9VAC_2_2 P9 EMQ030 IF PERS 1 ON LEAVE PAST WEEK
IFP9VAC_2 P9 IMPUTATION FLAG FOR P9VAC_2
P9JOB_2 P9 EMQ040 PERSON 1 NUMBER OF CUR JOBS
P9HRS_2_I P9 EMQ050 PERSON 1 HOURS/WK AT ALL JOBS
IFP9HRS_2 P9 IMPUTATION FLAG FOR P9HRS_2
P9LOK_2_I P9 EMQ060 PERS 1 SOUGHT JOB LAST 4 WEEKS
IFP9LOK_2 P9 IMPUTATION FLAG FOR P9LOK_2
P9DO1_2_I P9 EMQ070 PERS 1 CHKD W/PUB EMPL AGNCY
IFP9DO1_2 P9 IMPUTATION FLAG FOR P9DO1_2
P9DO2_2_I P9 EMQ070 PERS 1 CHKD W/PRIV EMP AGNCY
IFP9DO2_2 P9 IMPUTATION FLAG FOR P9DO2_2
P9DO3_2_I P9 EMQ070 PERS 1 CHKD W/EMPLOYR DIRECTLY
IFP9DO3_2 P9 IMPUTATION FLAG FOR P9DO3_2
P9DO4_2_I P9 EMQ070 PERS 1 CHKD W/FRIENDS AND REL
IFP9DO4_2 P9 IMPUTATION FLAG FOR P9DO4_2
P9DO5_2_I P9 EMQ070 PERS 1 PLACED OR ANSWERED ADS
IFP9DO5_2 P9 IMPUTATION FLAG FOR P9DO5_2
P9DO6_2 P9 EMQ070 PERS 1 CHKD SCHL/UNIV EMPL CTR
IFP9DO6 P9 IMPUTATION FLAG FOR P9DO6_2
P9D07_2 P9 EMQ070 PERS1 CHKD UNION/PROFSSL REG
IFP9DO7 P9 IMPUTATION FLAG FOR P9DO7_2
P9DO8_2 P9 ATTENDED JOB TRAINING
P9DO9_2 P9 EMQ070 PERS 1 RD WANT ADS/INTRNT SRCH
P9DO10_2 P9 EMQ070OS PERS 1 DID SOMETHING ELSE
P9DOW_2 P9 EMQ080 WHAT PERSON 1 DOING LAST WEEK
P9TAK_2 P9 EMQ100 PERS 1 COULD TAKE JOB LAST WK
P9EVRACTV2 P9 EMQ210 PERS 2 SERVED ACTIVE DUTY
P9CURACTV2 P9 EMQ215 PERS 2 CURR ON ACTIVE DUTY
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), kindergarten–fifth grade restricted-use data file.
7-367.5.2.4 Parent Demographic Variables (X9PAR1AGE, X9PAR2AGE, X9PAR1RAC,
X9PAR2RAC)
X9PAR1AGE is a composite variable for the age of parent 1 from the household roster (the
person whose roster number is indicated in X9IDP1) and X9PAR2AGE is the composite variable for the
age of parent 2 from the household roster (the person whose roster number is indicated in X9IDP2).21 The
ages of all household members (other than the child) who had their ages collected in the fall of 2010 or
spring of 2011 were automatically incremented by five years for the spring 2016 parent interview. Age
was incremented by four years for household members who were living in the household in the spring of
2012 and had age information collected in that interview but who were not in the household in the fall of
2010 or the spring of 2011. Age was incremented by three years for household members who were living
in the household in the spring of 2013 and had age information collected in that interview but who were
not in the household in the fall of 2010, the spring of 2011, or the spring of 2012. Age was incremented
by two years for household members who were living in the household in the spring of 2014 and had age
information collected in that interview but who were not in the household in the fall of 2010, the spring of
2011, the spring of 2012, or the spring of 2013. Age was incremented by one year for household members
who were living in the household in the spring of 2015 and had age information collected in that
interview but who were not in the household in the fall of 2010, the spring of 2011, the spring of 2012,
the spring of 2013, or spring of 2014. For information about how the first and second parents were
selected for these and other parent variables, see section 7.5.2.3 above.
The composite variables for race/ethnicity for the parent/guardians were derived in the same
way as those for the child, except that there are no variables that supplement parent-reported
race/ethnicity with FMS data as was done for children. All data on parent race/ethnicity come from the
parent interview. Race/ethnicity information collected for parents in the spring 2016 parent interview is
provided in the data file in categorical race/ethnicity composites (X9PAR1RAC for parent 1 in the
household, the person whose roster number is indicated in X9IDP1, and X9PAR2RAC for parent 2, the
person whose roster number is indicated in X9IDP2). Race and ethnicity information was collected only
once for each parent/guardian. If race and ethnicity information was collected in the fall of 2010, spring
of 2011, spring of 2012, spring of 2013, spring of 2014, or spring of 2015, it was not collected again in
the spring of 2016. The questions about race and ethnicity were only asked in the spring 2016 parent
interview to collect this information for parents/guardians who were new to the household in that round or
when this information was missing for parents/guardians who lived in the household at the time of the
spring 2016 interview.
21 These variables are a combination of P*HDAGE and P*HMAGE in the ECLS-K.
7-37Respondents were allowed to indicate that they, and the other parent figure when applicable,
were Hispanic or Latino, and whether they belonged to one or more of the five race categories (White,
Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific
Islander).22 From these responses, a person’s race/ethnicity was classified into eight mutually exclusive
categories. A person’s race/ethnicity was classified as “more than one race, non-Hispanic” if more than
one race was specified and the answer to the question about being Hispanic or Latino was 2 (no). A
person’s race/ethnicity was classified as “Hispanic, race specified” if the answer to the question about
being Hispanic or Latino was 1 (yes) and at least one race was indicated in the question about race. If a
person was Hispanic or Latino, but a race was not indicated, that person’s race/ethnicity was classified as
“Hispanic, no race specified.” The remaining race/ethnicity categories (White, non-Hispanic; Black or
African-American, non-Hispanic; Asian, non-Hispanic; Native Hawaiian or Other Pacific Islander; non-
Hispanic; and American Indian or Alaska Native, non-Hispanic) were coded according to the person’s
reported race when the person was not Hispanic or Latino. If the answer to the question about being
Hispanic or Latino was -7, -8, or -9 (refused, don’t know, or not ascertained, respectively), or if the
person was not Hispanic/Latino and the answer to the question about race was -7, -8, or -9 (refused, don’t
know, or not ascertained, respectively), race/ethnicity was coded as -9 (not ascertained).
Parent race/ethnicity was obtained for all parents/guardians and spouses of respondent
parents/guardians but may or may not have been collected for a parent’s boyfriend or girlfriend. For
example, in a household with a birth mother and stepfather, the race/ethnicity of both parents was
obtained. However, in a household with a birth mother and her boyfriend, the race/ethnicity of the mother
was obtained but the race/ethnicity of the boyfriend was not unless he was the respondent.23
7.5.2.5 Parent Education Variables (X9PAR1ED_I, X9PAR2ED_I)
There are two parent education composite variables on the file: X9PAR1ED_I (parent 1’s
highest level of education) and X9PAR2ED_I (parent 2’s highest level of education). This composite
variable describes the education level of parents who were in the household at the time of the spring 2016
interview. In spring 2016, questions were asked about the education level of all parents. In previous
22 Unlike the ECLS-K, in the ECLS-K:2011 “other” was not a permitted response for the race question.
23 In the spring of 2016, race/ethnicity information was collected for some persons who did not meet the criteria for having race and ethnicity
questions asked in the spring of 2016 but did meet the criteria for having race and ethnicity collected in an earlier round of the study. Persons who
have race and ethnicity on the file for spring 2016 include the study child, those with a relationship of mother/female guardian or father/male
guardian in any round (P*REL_* = 1 or 2 or P*UNR = 3 or 4), those who were a respondent in any round (P*PER_* = 1), and persons who were
spouse/partners of respondent parents in any round.
7-38rounds of the study, parent education composites were generally based on the first round in which
education questions were asked. The composite variables are based on reports of the parent’s highest
education level (P9HIG_1_I, P9HIG_2_I) and whether the parent had a high school degree or its
equivalent, such as a GED (P9HIS_1_I, P9HIS_2_I). If the highest education level reported for a parent
was in grades 0 through 12 (e.g., P9HIG_1_I = 11) and the parent had a high school degree or its
equivalent (e.g., P9HIS_1_I = 1 or 2), or if the highest education level was 13 (high school
equivalent/GED) or 14 (high school diploma), then the composite variable is coded as 3 (high school
diploma or equivalent). Otherwise, the education composite is coded according to the value of the highest
education level. Some codes on the highest education question were grouped together in the composite
variable categories. The categories “vocational/technical after high school, but no vocational/technical
diploma” and “vocational technical program after high school diploma” (e.g., P9HIG_1_I = 15 or 16)
were coded as 4 (vocational/technical program). The categories “some college, but no degree” and
“associate’s degree” (P9HIG_1_I = 17 or 18) were coded as 5 (some college). The categories “doctorate
degree” and “professional degrees after a bachelor’s degree” (e.g., P9HIG_1_I = 22 or 23) were coded as
9 (doctorate or professional degree). The variables reflect the education level of the household member(s)
corresponding to X9IDP1 and X9IDP2. For example, if X9IDP1 and X9IDP2 pointed to a child’s
grandmother and grandfather, then the highest level of education would be collected about these
nonparent guardians. See section 7.5.2.3 for more detailed discussion of how X9IDP1 and X9IDP2 were
determined. As described in section 7.5.2.8, education data are imputed if they are missing.
7.5.2.6 Parent Occupation Variables (X9PAR1EMP_I, X9PAR2EMP_I, X9PAR1OCC_I,
X9PAR2OCC_I, X9PAR1SCR_I, X9PAR2SCR_I)
Several composites describe parents’ employment status, their occupations, and the prestige
of their occupations. The pointer variables for employment data, P9EMPP1 and P9EMPP2, are set to the
same value as X9IDP1 and X9IDP2, and can be used to identify the household roster number of the
individual(s) to which the data pertain.
X9PAR1EMP_I and X9PAR2EMP_I describe the work status of parent 1 and parent 2,
respectively, and are based on the number of hours parents worked in the past week (e.g., P9HRS_1_I), or
if a parent did not work, are based on activities the parent did to look for work (e.g., P9DO1_1_I). More
specifically, X9PAR1EMP_I (parent 1 employment status) is coded as 1 (35 hours or more per week) if
parent 1 worked 35 or more hours per week, and coded as 2 (less than 35 hours per week) if parent 1
worked 0 to 34 hours per week. X9PAR1EMP_I is coded as 2 (less than 35 hours per week) when
P9HRS_1_I = 0 because the respondent indicated that the parent was employed even if he or she on
7-39average worked less than one hour per week. If parent 1 was actively looking for work (P9LOK_1_I = 1)
and did one of seven activities to look for work (P9DO1_1_I = 1 (checked with a public employment
agency); P9DO2_1_I = 1 (checked with a private employment agency); P9DO3_1_I = 1 (checked with an
employer directly or sent a resume to an employer); P9DO4_1_I = 1 (checked with friends or relatives);
P9DO5_1_I = 1 (placed or answered ads/sent a resume related to an ad); P9DO6_1_I (contacted
school/university employment center); or P9DO7_1_I (checked a union register or professional
register)),24 then X9PAR1EMP_I is coded as 3 (looking for work). If parent 1 was not working for pay,
not on vacation, and not looking for work (P9PAY_1 = 2 and P9VAC_1_I = 2 and P9LOK_1_I = 2), or if
parent 1 was looking for work (P9LOK_1_I = 1) and the variables for the seven activities indicating the
parent was actively looking for work were all coded as 2 (no), X9PAR1EMP_I is coded as 4 (not in the
labor force).25 X9PAR2EMP_I (parent 2 employment status) is created the same way as X9PAR1EMP_I,
but uses the data linked to parent 2.
Imputation was performed on the variables (e.g., P9HRS_1_I, P9DO1_I) that were used to
create the X9PAR1EMP_I and X9PAR2EMP_I composite variables. Variables that were imputed were
those associated with questions about whether the parent had worked for pay in the last week or was on
leave or vacation, hours worked in a typical week, whether the parent was looking for work and, if so,
what the parent was doing to find work. Each variable has a separate imputation flag (e.g., IFP9PAY_1 is
the imputation flag for P9PAY_1_I, the variable for whether parent 1 had paid job last week) indicating
whether data were imputed for each case in the data file.
The composite variables for parent occupation, X9PAR1OCC_I and X9PAR2OCC_I, are
coded based on information collected through questions in the parent interview about the name of the
parent’s employer, the type of business or industry in which the parent worked, the parent’s job title, and
the most important activities or duties the parent did for the job (EMQ120, EMQ130, EMQ140, and
EMQ150). This identifying information is not included in the file due to confidentiality issues. It was
used to code occupations into standard categories using the Manual for Coding Industries and
Occupations (U.S. Department of Education, National Center for Education Statistics 1999). This coding
manual was created for the National Household Education Surveys (NHES) Program and uses an
aggregated version of occupation codes. There are 22 occupation codes in this coding scheme. If it was
24 P9DO6_1_I (contacted school/university employment center) and P9DO7_1_I (checked a union register or professional register) were new
variables that were added as activities indicating the respondent was looking for work and used for coding the employment composite in the
spring of 2013.
25 Because some persons were not looking for work according to the seven categories described above, even though it was reported that a parent
was looking for work (P9LOK_1_I = 1), the parent is coded as not in the labor force (X9PAR1EMP_I = 4) rather than as looking for work
(X9PAR1EMP_I = 3). If a parent was reported as looking for work (P9LOK_1_I =1), the questions about the parent’s last occupation were asked.
There are 95 cases with occupation data that are categorized as X9PAR1EMP = 4 (not in the labor force) because they indicated that all they were
doing to look for work was looking at or reading want ads or some “other” activity that did not qualify them to be classified as looking for work
according to the U.S. Bureau of Labor Statistics (2014); there are 40 cases with occupation data where X9PAR2EMP = 4.
7-40unclear which of the 22 codes should be used for an occupation using this manual, the more detailed
coding system in the Standard Occupational Classification Manual—1980 (U.S. Department of
Commerce, Office of Federal Statistical Policy and Planning, 1980) was used to identify the appropriate
occupation code. The Standard Occupational Classification Manual is the full, detailed coding scheme of
which the NHES coding scheme is a condensed version, and thus provides more detail for making coding
decisions. The occupation codes are shown in exhibit 7-4.
Exhibit 7-4. Industry and occupation codes used in the ECLS-K:2011
1. Executive, Administrative, and Managerial Occupations
This category includes senior-level and middle management occupations and occupations that directly
support management. Senior-level managers are persons concerned with policymaking, planning, staffing,
directing, and/or controlling activities. Middle managers include persons who plan, organize, or direct
and/or control activities at the operational level. Workers in this category are not directly concerned with
the fabrication of products or with the provision of services. Other officials and administrators include
consultants, library directors, custom house builders, and location managers. Legislators are also included
in this category.
2. Engineers, Surveyors, and Architects
This category includes occupations concerned with applying principles of architecture and engineering in
the design and construction of buildings, equipment and processing systems, highways and roads, and
land utilization.
3. Natural Scientists and Mathematicians
This category includes those engaged primarily in the application of scientific principles to research and
development. Natural scientists are those in the physical sciences (e.g., chemistry, physics) and the life
sciences (e.g., biology, agriculture, medicine). In addition, this category includes those in computer
science, mathematics (including statistics), and operations research.
4. Social Scientists, Social Workers, Religious Workers, and Lawyers
This category includes occupations concerned with the social needs of people and with basic and applied
research in the social sciences.
5. Teachers: College, University, and Other Postsecondary Institution; Counselors, Librarians,
and Archivists
This category includes those who teach at higher education institutions and at other postsecondary (after
high school) institutions, such as vocational institutes. In addition, vocational and educational counselors,
librarians, and archivists are included here.
7-41Exhibit 7-4. Industry and occupation codes used in the ECLS-K:2011—Continued
6. Teachers, Except Postsecondary Institution
This category includes prekindergarten and kindergarten teachers, elementary and secondary teachers,
special education teachers, instructional coordinators, and adult education teachers (outside postsecondary
education).
7. Physicians, Dentists, and Veterinarians
This category includes health care professionals who diagnose and treat patients. In addition to
physicians, dentists, and veterinarians, this category includes optometrists, podiatrists, and other
diagnosing and treating professionals, such as chiropractors, hypnotherapists, and acupuncturists.
8. Registered Nurses, Pharmacists, Dieticians, Therapists, and Physician’s Assistants
This category includes occupations concerned with the maintenance of health, the prevention of illness
and the care of the ill through the provision and supervision of nursing care; compounding drugs,
planning food service or nutritional programs; providing assistance to physicians; and the provision of
therapy and treatment as directed by physicians.
9. Writers, Artists, Entertainers, and Athletes
This category includes occupations concerned with creating and executing artistic works in a personally
interpreted manner by painting, sculpturing, drawing, engraving, etching, and other methods; creating
designs for products and interior decorations; designing and illustrating books, magazines, and other
publications; writing; still, motion picture, and television photography/filming; producing, directing,
staging, acting, dancing, singing in entertainment; and participating in sports and athletics as a competitor
or player and administering and directing athletic programs.
10. Health Technologists and Technicians
This category includes occupations concerned with providing technical assistance in the provision of
health care. For example, clinical laboratory technologists and technicians, dental hygienists, radiologic
technicians, licensed practical nurses (LPNs), and other health technologists are included here.
11. Technologists and Technicians, Except Health
This category includes those providing technical assistance in engineering and scientific research,
development, testing, and related activities, as well as operating and programming technical equipment
and systems.
12. Marketing and Sales Occupations
This category includes occupations involving selling goods or services, purchasing commodities and
property for resale, and conducting wholesale or retail business.
7-42Exhibit 7-4. Industry and occupation codes used in the ECLS-K:2011—Continued
13. Administrative Support Occupations, Including Clerks
This category includes occupations involving preparing, transcribing, transferring, systematizing, and
preserving written communications and records; collecting accounts; gathering and distributing
information; operating office machines and data processing equipment; operating switchboards;
distributing mail and messages; and other support and clerical duties such as bank teller, data entry keyer,
etc.
14. Service Occupations
This category includes occupations providing personal and protective services to individuals, and current
maintenance and cleaning for building and residences. Some examples include food service, health
service (e.g., aides or assistants), cleaning services other than household, and personal services.
15. Agricultural, Forestry, and Fishing Occupations
This category is concerned with the production, propagation (breeding/growing), gathering, and catching
of animals, animal products, and plant products (timber, crop, and ornamental); the provision of services
associated with agricultural production; and game farms, fisheries, and wildlife conservation.
16. Mechanics and Repairers
This category includes persons who do adjustment, maintenance, part replacement, and repair of tools,
equipment, and machines. Installation may be included if it is usually done in conjunction with other
duties of the repairers.
17. Construction and Extractive Occupations
This category includes occupations that normally are performed at a specific site, which will change over
time, in contrast to production workers, where the work is usually at a fixed location. Construction
workers include those in overall construction, brick masons, stonemasons, carpenters, electricians,
drywall installers, paperhangers and painters, etc. Extractive occupations include oil well drillers, mining
machine operators, and so on.
18. Precision Production Occupations
This category includes occupations concerned with performing production tasks that require a high degree
of precision or attainment of rigid specification and operating plants or large systems. Included in this
category are tool and die makers, pattern and model makers, machinists, jewelers, engravers, and so on.
Also included are some food-related workers including butchers and bakers. Plant and system operators
include water and sewage, gas, power, chemical, petroleum, and other plant or system operators.
19. Production Working Occupations
This category includes occupations concerned with setting up, operating, and tending of machines and
hand production work, usually in a factory or other fixed place of business.
7-43Exhibit 7-4. Industry and occupation codes used in the ECLS-K:2011—Continued
20. Transportation and Material Moving Occupations
This category includes occupations concerned with operating and controlling equipment used to facilitate
the movement of people or materials and the supervising of those workers.
21. Handlers, Equipment Cleaners, Helpers, and Laborers
This category includes occupations that involve helping other workers and performing routine
nonmachine tasks. A wide variety of helpers, handlers, etc., are included in this category. Examples
include construction laborers, freight, stock, and material movers, garage and service station-related
occupations, parking lot attendants, and vehicle washers and equipment cleaners.
22. Unemployed, Retired, Disabled, or Unclassified Workers
This category includes persons who are unemployed, have retired from the work force, or are disabled. It
also includes unclassified occupations that do not fit into the categories above (e.g., occupations that are
strictly military, such as “tank crew member” and “infantryman”).
Once occupations were classified in X9PAR1OCC_I and X9PAR2OCC_I, they were
assigned the average of the 1989 General Social Survey (GSS) prestige scores for occupations included
within the 21 broad occupation codes, which is reported in variables X9PAR1SCR_I and X9PAR2SCR_I.
If the parent’s occupation was 22 (Unemployed, Retired, Unclassifiable), the prestige score was set to -9
(not ascertained). If the parent’s occupation was -1 (No Occupation) on X9PAR1OCC_I or
X9PAR2OCC_I, the prestige score was also coded as -1. Although the GSS prestige scores are from
1989, they are still being used by the current GSS survey and matched to 1980 census codes.
26 Because
these prestige scores were also used for the ECLS-K 1998–99 cohort, they allow for comparisons to the
ECLS-K. Table 7-2 provides the prestige score values for each occupation category.
Occupations were imputed if such information was not collected in the parent interview.
Missing data for individual items related to parent employment were imputed first, and then those
imputed data were used to compute the occupation composite variables if necessary (i.e., cases missing
employment status that were imputed to be working or on leave from a job also had their occupation
imputed and a prestige score assigned to the imputed occupation; cases missing data for the variables
about looking for work and that were imputed to be actively looking for work (defined by EMQ070
answers 1-7) also had occupation imputed).
26 New technology jobs that came into existence since 1989 were appropriately coded. For example, “website developer” was included in the
“Technologists and Technicians, Except Health” category; “website sales” was included in the “Marketing and Sales Occupations” category; and
“run web printer” was included in the “Production Working Occupations” category.
7-44Table 7-2. Occupation categories and assigned prestige scores
Occupation category Prestige score
1 Executive, Administrative, and Managerial Occupations 2 Engineers, Surveyors, and Architects 3 Natural Scientists and Mathematicians 4 Social Scientists, Social Workers, Religious Workers, and Lawyers 5 Teachers: College/University/Postsecondary; Counselors/
53.50
64.89
62.87
59.00
72.10
Librarians/Archivists
6 Teachers, Except Postsecondary Institution 7 Physicians, Dentists, and Veterinarians 8 Registered Nurses, Pharmacists, Dieticians, Therapists, and Physician’s
63.43
77.50
61.56
Assistants
9 Writers, Artists, Entertainers, and Athletes 52.54
10 Health Technologists and Technicians 57.83
11 Technologists and Technicians, Except Health 48.69
12 Marketing and Sales Occupations 35.78
13 Administrative Support Occupations, Including Clerks 38.18
14 Service Occupations 34.95
15 Agriculture, Forestry, and Fishing Occupations 35.63
16 Mechanics and Repairers 39.18
17 Construction and Extractive Occupations 39.20
18 Precision Production Occupations 37.67
19 Production Working Occupations 33.42
20 Transportation and Material Moving Occupations 35.92
21 Handlers, Equipment Cleaners, Helpers, and Laborers 29.60
22 Unemployed, Retired, Disabled, or Unclassified Workers (if a person was
on leave from a job or unemployed and actively looking for work, he or she
was asked the occupation questions. Category 22 was used only if a
respondent reported an occupation that could not be classified in the coding
scheme, “unemployed “or “retired.”)
-1 (No occupation) Because these
occupations could
not be classified, the
prestige score is
coded -9 (not
ascertained)
When occupation is
-1, the prestige score
is also -1.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016 (K-5) data file.
The first stage of imputation was to use longitudinal imputation, where possible. In
longitudinal imputation, values from a prior interview are carried forward when data from the round of
interest are missing, provided the parent figure for whom data are being imputed was the same in both
years. In the spring 2016 round, the complete employment module was administered, as it was in the base
year through second grade. However in third and fourth grades, only a single employment item was
asked, and it had four possible responses: working part-time, working full-time, a stay-at-home parent or
guardian, and not working. Therefore the employment data from third and fourth grades are of limited
value for imputing missing fifth grade employment data. Longitudinal imputation for missing
7-45employment items used the most recent available information for that parent from base year through
second grade values. However information from the single employment question in fourth grade (and
third grade, if necessary) were used in the following way for a parent with missing employment
information in fifth grade. If the parent provided a response to the employment question in fourth grade,
and if this indicated that the parent changed employment status between the second-grade and fourth-
grade rounds (for example, from employed to not working, or from part-time to full-time employment),
then instead of carrying forward the second-grade employment values to fifth grade, the fourth-grade
employment response was used where possible, and any remaining missing information was imputed by
hot deck imputation. For example, if the fourth-grade response indicated that parent 1 was “not working,”
then P9PAY_1_I and P9VAC_1_I are set to 2 (fifth-grade round variables for not working and not on
vacation, respectively), and P9HRS_1_I to zero (fifth-grade round variable for number of hours worked),
and their imputation flags indicate that longitudinal imputation was used. However the variables related to
“looking for work” (P9LOK_1_I, P9DO*_1_I) were imputed by hot deck. If the employment and
occupation data for a parent figure are missing for the spring of 2016, but the information is available
from the spring 2013, and the parent figure is still employed or actively looking for work in fifth grade,
then the spring 2013 values were used to impute the spring 2016 variables.27
Where longitudinal imputation was not possible, missing values were imputed using a hot
deck method in which similar respondents and nonrespondents are grouped or assigned to “imputation
cells,” and a respondent’s value is randomly “donated” to a nonrespondent within the same cell. Cells are
defined by characteristics such as geographic region, school locale, school type, household type, age,
race, education, and income. After imputation was completed, the average of the GSS prestige scores was
assigned to each occupation code.
The imputation flag variables IFX9PAR1OCC and IFX9PAR1SCR indicate whether the
occupation (X9PAR1OCC_I) and occupational prestige score (X9PAR1SCR_I) for parent 1 were
imputed. These flags match in value because the prestige score (e.g., X9PAR1SCR_I) is coded directly
from occupation (e.g., X9PAR1OCC_I). Similarly, the flags IFX9PAR2OCC and IFX9PAR2SCR
indicate whether the occupation (X9PAR2OCC_I) and occupational prestige score (X9PAR2SCR_I) for
parent 2 were imputed.
27 Analyses were conducted to compare occupations in the spring of 2013 to those in the spring of 2016. Most parents (64 percent) had the same
occupations between these two rounds, thus there was insufficient evidence to suggest that hot deck imputation should be used rather than
longitudinal imputation. In addition, there were very few cases (less than 25) that were longitudinally imputed in spring 2016 from values
obtained through previous round hot deck imputation.
7-467.5.2.7 Household Income and Poverty (X9INCCAT_I, X9POVTY_I)
Household income data were collected in the spring 2016 parent interview. Parents were
asked to report income by broad range ($25,000 or less or more than $25,000) and by detailed range as
shown in table 7-3.28 The composite X9INCCAT_I was created using the detailed income range
information. X9INCCAT_I was set to the value of P9INCLOW_I (detailed income range for those who
reported the broad income range in P9HILOW_I as $25,000 or less) or P9INCHIG (detailed income range
for those who reported the broad income range in P9HILOW_I as more than $25,000). When data for the
broad range variable (P9HILOW_I) or one of the detailed range variables (P9INCLOW_I, P9INCHIG_I)
were missing (i.e., coded as -7 (refused), -8 (don’t know), or -9 (not ascertained)), income information
was imputed.
Table 7-3. Detailed income range categories used in the parent interview: Spring 2016
Detailed income range Total household income
1 ........................................................................................................................... $5,000 or less
2 ........................................................................................................................... $5,001 to $10,000
3 ........................................................................................................................... $10,001 to $15,000
4 ........................................................................................................................... $15,001 to $20,000
5 ........................................................................................................................... $20,001 to $25,000
6 ........................................................................................................................... $25,001 to $30,000
7 ........................................................................................................................... $30,001 to $35,000
8 ........................................................................................................................... $35,001 to $40,000
9 ........................................................................................................................... $40,001 to $45,000
10 ......................................................................................................................... $45,001 to $50,000
11 ......................................................................................................................... $50,001 to $55,000
12 ......................................................................................................................... $55,001 to $60,000
13 ......................................................................................................................... $60,001 to $65,000
14 ......................................................................................................................... $65,001 to $70,000
15 ......................................................................................................................... $70,001 to $75,000
16 ......................................................................................................................... $75,001 to $100,000
17 ......................................................................................................................... $100,001 to $200,000
18 ......................................................................................................................... $200,001 or more
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
If the parent figures in the household were the same at the time of the spring 2016 parent
interview as at the time of the spring 2015 parent interview, income reported in the spring of 2015 was
used for longitudinal imputation. If spring 2015 income was not available, but spring 2014 income was
available and the parent figures were the same in 2016 as in 2014, then income reported in the spring of
28 Starting at category 9 of the detailed income range, the categories for the income variable in the ECLS-K:2011 are different from those used in
the ECLS-K. More narrow ranges of income were used at higher income levels in the ECLS-K:2011 in order to determine whether household
income was near 200 percent of the federal poverty threshold given household size. If so, follow-up questions about exact income were asked.
7-472014 was carried forward to 2016.29 Where longitudinal imputation was not possible, missing values
were imputed using the hot deck method described in section 7.5.2.8. Cells were defined by
characteristics such as geographic region, school locale, school type, household type, age, and race. When
information used to define the imputation cells was missing for any of these variables in fifth grade,
information was used from fourth, third, second, or first grades or the base year, where available.
Imputation flag values for IFP9HILOW, IFP9INCLOW, and IFP9INCHIG identify cases for which
longitudinal or hot deck imputation was conducted.
Reported income was used to determine household poverty status in the spring of 2016,
which is provided in variable X9POVTY_I. For some households, more detailed information about
household income than the ranges described above was collected. Specifically, when parent respondents
reported a detailed household income range suggesting the household income was close to or lower than
200 percent of the U.S. Census Bureau poverty threshold for a household of its size, the respondents were
asked to report household income to the nearest $1,000 (referred to as exact income) in order to determine
household poverty status more accurately. Table 7-4 shows the reported detailed income categories for
households of a given size for which respondents were asked the exact income question. For example, a
respondent in a household with two people would have been asked to provide an exact income if the
respondent had indicated that the household income was less than or equal to $30,000. Table 7-4 also
shows how the income categories compare to the value that is 200 percent of the weighted average 2015
poverty threshold.30 The 2015 weighted poverty thresholds were used for the poverty composite because
respondents in the spring of 2016 were asked about household income in the past year. As noted in Table
7-4, the 2015 poverty thresholds were compared to the 2014 thresholds that were available at the time of
programming.
When information about exact household income was available (P9TINCTH_I), it was used
in conjunction with household size (X9HTOTAL) to calculate the poverty composite. When exact income
was not available because the exact income question was not asked, the midpoint of the detailed income
category (X9INCCAT_I) was used in conjunction with household size (X9HTOTAL).31
29 No adjustment was made for inflation when household income was longitudinally imputed from a prior round.
30 The CAPI program used to conduct the parent interview was programmed to only ask for exact income when parent respondents reported a
detailed household income range suggesting the household income was close to or lower than 200 percent of the U.S. Census Bureau poverty
threshold for a household of its size. Although the parent interview in which this information was collected was conducted in the spring of 2016,
the 2014 poverty thresholds were used for instrument programming because they were the most recent thresholds available when programming
was done. The question about exact income was asked for the following conditions: (NUMBER IN HH = 1 AND PAQ.110 < 6) OR (NUMBER
IN HH = 2 AND PAQ.110 < 8) OR (NUMBER IN HH = 3 AND PAQ.110 < 9) OR (NUMBER IN HH = 4 AND PAQ.110 < 11) OR (NUMBER
IN HH = 5 AND PAQ.110 < 13) OR (NUMBER IN HH = 6 AND PAQ.110 < 14) OR (NUMBER IN HH = 7 AND PAQ.110 < 16) OR
(NUMBER IN HH = 8 AND PAQ.110 < 17) OR (NUMBER IN HH is greater than or equal to 9 AND PAQ.110 < 17).
31 Because exact income information was not collected from all parents, the ECLS-K:2011 provides an approximate but not exact measure of
poverty.
7-48Table 7-4. Criteria for reporting income to the nearest $1,000 in the spring parent interview and 2015
thresholds for 200 percent of poverty: Spring 2016
Household size
ECLS-K:2011 parent interview
income categories
200 percent of weighted average
thresholds for 20151, 2
Two Three Four Five Six Seven Eight Nine or more Less than or equal to $35,000 Less than or equal to $40,000 Less than or equal to $50,000 Less than or equal to $60,000 Less than or equal to $65,000 Less than or equal to $75,000 Less than or equal to $100,000 Less than or equal to $100,000 $30,782 or less
$37,742 or less
$48,514 or less
$57,482 or less
$65,084 or less
$73,996 or less
$82,058 or less
$98,354 or less
1 U.S. Census Bureau, Current Population Survey. Poverty Thresholds for 2015 by Size of Family and Number of Related Children Under 18
Years Old, retrieved 9/28/2016 from http://www.census.gov/data/tables/time-series/demo/income-poverty/historical-poverty-thresholds.html.
2 The 2015 weighted poverty thresholds were used for the poverty composite because respondents in the spring of 2016 were asked about
household income in the past year. At the time that the spring 2016 parent interview was finalized, the most updated poverty thresholds available
were the weighted 2014 poverty thresholds. Poverty thresholds for 2014 were similar to the poverty thresholds for 2015. However, because of
differences in one category, exact income should have been asked for one narrow range of incomes according to the 2015 thresholds, but it was
not asked because the 2014 thresholds were used. Using the 2015 poverty thresholds rather than the 2014 poverty thresholds, cases with six
household members and an income between $65,001 and $65,084 were not asked exact income when they should have been.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
Household poverty status in the spring of 2016 was determined by comparing total
household income reported in the parent interview to the weighted 2015 poverty thresholds from the U.S.
Census Bureau (shown in table 7-5), which vary by household size. Although the parent interview was
conducted in the spring of 2016, the 2015 weighted poverty thresholds were used in the derivation of the
poverty composite because respondents were asked about household income in the past year. Exact
income (P9TINCTH_I) was asked in the parent interview or imputed for all persons in categories 1 and 2
of the poverty composite. Imputation of exact income was conducted according to thresholds in the parent
interview. Households with an exact income that fell below the appropriate threshold were classified as
category 1, “below the poverty threshold,” in the composite variable. Households with an exact income
that was at or above the poverty threshold but below 200 percent of the poverty threshold were classified
as category 2, “at or above the poverty threshold, but below 200 percent of the poverty threshold,” in the
composite variable. Households with a total income (either exact or the income representing the midpoint
of the detailed range reported by the composite) that was at or above 200 percent of the poverty threshold
were classified as category 3, “at or above 200 percent of the poverty threshold,” in the composite
variable.32 For example, if a household contained two members and the household income was lower than
$15,391, the household was considered to be below the poverty threshold and would have a value of 1 for
the composite. If a household with two members had an income of $15,391 or more, but less than
32 In the ECLS-K:2011, there are three categories in the poverty composite rather than two categories for “below poverty threshold” and “at or
above poverty threshold” as there were in the ECLS-K. The ECLS-K:2011 categories 2 and 3 can be combined to create a poverty composite
variable comparable to the ECLS-K poverty composite variable.
7-49$30,782 (200 percent of the poverty threshold for a household of two), the composite would have a value
of 2. If a household with two members had an income of $30,782 or more, the composite would have a
value of 3.
Table 7-5. ECLS-K:2011 poverty composite and 2015 census poverty thresholds: Spring 2016
Household size
poverty
threshold
Census weighted
average poverty
thresholds for 2015
(X9POVTY_I = 1)1
100 percent to less than 200
percent of census weighted
average poverty thresholds for
2015 (X9POVTY_I = 2)1
Census weighted
average thresholds
for poverty 20151
Two Less than $15,391 $15,391 to less than $30,782 $15,391
Three Less than $18,871 $18,871 to less than $37,742 $18,871
Four Less than $24,257 $24,257 to less than $48,514 $24,257
Five Less than $28,741 $28,741 to less than $57,482 $28,741
Six Less than $32,542 $32,542 to less than $65,084 $32,542
Seven Less than $36,998 $36,998 to less than $73,996 $36,998
Eight Less than $41,029 $41,029 to less than $82,058 $41,029
Nine or more Less than $49,177 $49,177 to less than $98,354 $49,177
1 U.S. Census Bureau, Current Population Survey. Poverty Thresholds for 2015 by Size of Family and Number of Related Children Under 18
Years Old, retrieved 9/28/2016 from http://www.census.gov/data/tables/time-series/demo/income-poverty/historical-poverty-thresholds.html.
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
7.5.2.8 Socioeconomic Status (SES) (X9SESL_I)
SES was computed at the household level using data from collected parents who completed
the parent interview in the spring of 2016.33 The SES variable reflects the socioeconomic status of the
household at the time of data collection. The five components used to create the SES are as follows:
 Parent/guardian 1’s education;
 Parent/guardian 2’s education;
 Parent/guardian 1’s occupational prestige score;
 Parent guardian 2’s occupational prestige score; and
 Household income.
33 In the fall of 2011, spring of 2012, and spring 2016, occupation prestige scores were included as individual variables and, along with income
and education, were used to create a composite variable for socioeconomic status (SES). In the spring of 2013, occupation prestige scores were
included as individual variables, but parents were not asked for education information and thus the composite for socioeconomic status was not
created.
7-50Not all parents completed the parent interview in the spring of 2016; among those who did,
not all responded to every question. There are 7,954 children for whom no spring 2016 parent interview
was completed. Table 7-6 shows the numbers of cases with missing data on each of the five component
variables used to compute SES, among the 10,220 children who had an otherwise complete parent
interview.
Table 7-6. Missing data for socioeconomic status (SES) source variables, fifth grade year: School year
2015–16
Variable Number missing Percent
Parent/guardian 1’s education 956 9.35
Parent/guardian 2’s education 741 7.25
Parent/guardian 1’s occupation 1,014 9.92
Parent/guardian 2’s occupation 789 7.72
Detailed income range 1,775 17.37
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010–11 (ECLS-K:2011), spring 2016.
In order to provide SES data for as many children who had an otherwise complete parent
interview as possible, missing values were imputed for each of the individual items used to compute the
composite variables that factor into the derivation of SES, namely parent education, employment,
occupational prestige, and household income. For example, missing values for highest grade completed
(P9HIG_n_I) and diploma status (P9HIS_n_I) were imputed for cases for which these items were asked
in the spring of 2016 but the data were missing (-7 (refused), -8 (don’t know), or -9 (not ascertained)) and
those imputed data were used to compute the parent education composite variables. Missing data for
individual items related to parent employment (whether the parent had worked for pay in the last week or
was on leave or vacation, hours worked in a typical work week, whether the parent was looking for work
and if, so, what the parent was doing to find work) were imputed, and then those imputed data were used
to compute the occupation composite variables if necessary (i.e., cases missing employment status that
were imputed to be working or on leave from a job also had their occupation imputed and a prestige score
assigned to the imputed occupation; cases missing data for the variables about looking for work and that
were imputed to be actively looking for work (defined by EMQ070 answers 1-7) also had occupation
imputed). The different income category variables were also imputed.
Two methods were used to impute missing data: longitudinal imputation and hot deck
imputation. Longitudinal imputation (carrying forward a previous round value) was sometimes used when
previous round data were available for the items for which data were missing in the spring of 2016. For
example, in some cases a parent interview broke off before the questions in the employment section were
asked, but employment and occupation data for the parent(s) in the household are available from previous
7-51rounds. Similarly, for some cases data for the income variables were reported in a previous round but not
in the spring 2016 parent interview. Longitudinal imputation was used to impute data for the various
employment and occupation items only for parent figures who were household members in spring 2016
and a previous round. Longitudinal imputation was used for household income items only if there was no
change in parent figures in the household (that is, the two parents or only parent present in an earlier
round remained in spring 2016). Values imputed in this manner are flagged as being imputed
longitudinally.
When longitudinal imputation was not possible (either because there was a change in parent
figures or previous round data were not available), hot deck imputation was used. In hot deck imputation,
the value reported by a respondent for a particular component variable (e.g., highest grade completed or
occupation) is assigned or “donated” to a “similar” person who failed to respond to that question.
Auxiliary demographic information known for both donors and nonrespondents is used to form
imputation cells that include donors and nonrespondents with similar values for the characteristics that
define the cells. The specific demographic characteristics used to define imputation cells varied by the
component being imputed, as noted below. The imputed value for a case with a missing value is taken
from a randomly selected donor among the respondents within the cell.
For each imputed variable, imputation cells were created using demographic characteristics
that were the best predictors of the variable. Characteristics such as census region, school type
(public/Catholic/non-Catholic religious/private nonsectarian), school locale (city/suburb/town/rural),
household type (female single parent/male single parent/two parents), parents’ race/ethnicity, and parents’
age range were used to form the cells. Chi-square automatic interaction detector (CHAID) analyses were
used to determine these predictors.
The order of imputation is parent 1’s education variables; parent 2’s education variables;
parent 1’s labor force status variables; parent 1’s occupation; parent 2’s labor force status variables;
parent 2’s occupation; detailed income range when the broad income range is known; detailed income
range when the broad income range is not known; and exact income where applicable based on household
income and detailed income range. Imputation cells for each component imputed were created using the
other components, when possible.
7-52The hot deck imputation was implemented as follows:
 For households with both parents present, parent 1’s and parent 2’s variables were
imputed separately.
 Imputed as well as reported values were used to create imputation cells. For any given
component, the imputation cells were created using (1) collected and imputed data for
those variables that were imputed before the given component, and (2) collected data
only for those variables that were imputed after the given component.
 Values imputed by hot deck were not donated.
After imputation was completed, the occupational prestige variables (X9PAR1SCR_I and
X9PAR2SCR_I) were created by assigning the average of the 1989 GSS prestige score associated with
parent occupation, as described above in section 7.5.2.6.
Upon completion of imputation, the composite variables that are used in the computation of
SES were created. These are parent education (X9PAR1ED_I and X9PAR2ED_I), parent occupational
prestige scores (X9PAR1SCR_I and X9PAR2SCR_I), and household income (X9INCCAT_I). Although
imputation was conducted at the level of the component variables used to compute these composites, the
names of the composites themselves also carry the _I designation to indicate that they contain imputed
data. These composite variables do not have their own imputation flags. The imputation flags associated
with the variables used to compute the composites can be reviewed to identify cases for which the
composite is based on imputed data.34
The values of each SES component were then normalized so that the component had a mean
of 0 and a standard deviation of 1. In this normalization step, -1 (not applicable) values are treated as
missing. This is also known as the z-score. For the h-th SES component, a z-score zhi for the i-th
household was computed as
z−
x
whi
=
hi
sd
( )w
x
,
where x w
hi
is the value of the h-th SES component for the i-th household; x
is the weighted mean35 of
x ( )w
hi
; and is the standard deviation of xsd w
. Note that where h is household income, x
x hi
is the
natural log of the midpoint of the detailed income range. The weight used to compute the z score was the
spring 2016 child base weight, W9CI0.
34 The questionnaire items about occupation (job title, job activities, employer, industry) are not included in the data file; the imputation flags for
occupation are associated with the occupation composite variables.
35 The fifth-grade base weight (i.e., sample weight) adjusted for base-year nonresponse and mover subsampling was used.
7-53The SES variable for the i-th household was then computed as
m
∑
h
=1
z
hi
SES
i
=
,
m
where m is the number of components. Note that for households with only one parent present and for
parents who were retired or not currently in the labor force, not all the components were defined. In these
cases, the SES is the average of the z-scores of the available components.
7.5.2.9 Respondent ID and Relationship to Focal Child (X9RESID, X9RESREL2)
The respondent to the parent interview was a person identified as the household member
who knew the most about the child’s care, education, and health. X9RESID indicates the household roster
number of the spring 2016 parent interview respondent. The relationship variables (P9REL_1-P9REL_25,
P9MOM_1-P9MOM_25, P9DAD_1-P9DAD_25, and P9UNR_1-P9UNR_25) associated with the
respondent’s household roster number were used to code X9RESREL2. If the respondent was a biological
mother or father, X9RESREL2 is coded as 1 (biological mother) or 4 (biological father), respectively. If
the respondent was an adoptive, step-, or foster mother or father, or other female or male guardian,
X9RESREL2 is coded as 2 (other mother type) or 5 (other father type), respectively. If the respondent
was a mother or father but the type of mother (P9MOM_#) or father (P9DAD_#) was coded as -7
(refused), -8 (don’t know), or -9 (not ascertained), X9RESREL2 is coded as 3 (mother of unknown type)
or 6 (father of unknown type).36 If the respondent was a grandparent, aunt, uncle, cousin, sibling, or other
relative, X9RESREL2 is coded as 7 (nonparent relative). If the respondent was a girlfriend or boyfriend
of the child’s parent or guardian; a daughter or son of the child’s parent’s partner; other relative of the
child’s parent’s partner; or another nonrelative, X9RESREL2 is coded as 8 (nonrelative). Otherwise,
X9RESREL2 is coded as -9 (not ascertained). Because the interviewer initially asked to speak with the
previous round respondent at the beginning of the spring 2016 parent interview, the respondent for
previous interviews (X*RESID) was the same person for many cases.
36 Categories for mothers and fathers of unknown type were new for the spring 2012 composite. Mothers and fathers of unknown type were
included in the categories “other mother type” and “other father type” for the fall 2010 and spring 2011 composites, X1RESREL and
X2RESREL.
7-547.5.2.10 Food Security Status
The food security status of the children’s household was determined by responses to the 18
food security questions (P9WORRFD through P9NOMONY) asked in section FDQ of the spring 2016
parent interview.37 The questions measured the households’ experiences related to food insecurity and
reduced food intake in the last 12 months. Questions were asked about adults’ experiences separately
from the experiences of the children in the household.38 They were combined into scales using statistical
methods based on the Rasch measurement model. The food security questions were developed by
academic researchers using ethnographic and case-study methods with low-income women and families
to identify natural language used to describe their situations and behaviors when they had difficulty
obtaining enough food. The scales derived from the food security questions were validated using
statistical methods based on item response theory and by comparing measured food security with other
indicators of food adequacy. Composites were created that indicate the food security status of the child’s
household generally (based on all 18 adult and child items), as well as the food security status of the
adults (based on 10 household- and adult-referenced items) and of the children (based on 8 child-
referenced items) in the household separately.
When interpreting food security statistics, users should keep in mind that food security status
is a household-level characteristic. In most households classified as having very low food security, the
children in the household were not food insecure at that level of severity. Young children in U.S.
households are generally protected from disrupted diets and reduced food intake to a greater extent than
are older children or adults in the same households (Nord and Hopwood 2007). The household scale
combines adult and child items and reflects primarily experiences of adults in the household. The child
scale is more likely to reflect the food security of the sampled child, but it may reflect, primarily, the
experiences of elder siblings of the sampled child if any are present. The questions refer to conditions
among any or all of the children in the household. Thus, for many research applications, the adult scale
may be preferred instead of the household scale or children’s scale. In other applications, the household or
children’s scale may be used with controls for the presence and age of older children in the household.
Calculations of the scales indicating household food security and adult food security were
carried out in accordance with the standard methods described in Guide to Measuring Household Food
37 Some of the item numbers for these variables are different from those used in the ECLS-K because the food security section was reordered in
the ECLS-K:2011. Three items (FDQ160, FDQ170, and FDQ180) also had slight wording changes compared to how they were asked in the
ECLS-K. Composites that involve items with wording changes relative to the ECLS-K have a “2” at the end of them.
38 In spring 2011, spring 2012, and spring 2016, questions were asked about adults’ experiences separately from the experiences of the children in
the household. In spring 2014 and spring 2015, to reduce respondent burden, a shorter 10-item version of this measure suggested by the United
States Department of Agriculture (USDA) was used to measure adult food security. The adult food security measure can be used to predict child
food security.
7-55Security, Revised 2000 (U.S. Department of Agriculture 2000). Calculations of the scale indicating
children’s food security were carried out in accordance with the standard methods described in Measuring
Children’s Food Security in U.S. Households, 1995–99 (U.S. Department of Agriculture 2002). Analysis
of the ECLS-K:2011 data using statistical methods based on the Rasch measurement model found that
item severity parameters in the ECLS-K:2011 data were near enough to the standards benchmarked by the
Current Population Survey Food Security Supplement that it was appropriate to use the standard
benchmark household scores, which are based on the latter data source.
7.5.2.10.1 Food Security Status: Raw Scores (X9FSRAW2, X9FSADRA2, and X9FSCHRA)
The household food security raw score, X9FSRAW2, is a count of affirmative responses to
the 18 food security items, and an ordinal-level measure of food insecurity. It can be used in analyses as
an ordinal measure of food insecurity or to identify more severe or less severe categories of food
insecurity than those identified in the categorical food security variables described in section 7.5.2.10.3.
The raw score is ordinal, not interval, so it should not be used when a linear measure is required, such as
for calculation of a mean. Responses to items skipped because of screening are assumed to be negative for
the purpose of creating the score. For cases that have some missing data but at least some valid responses,
missing responses were considered to be negatives. Cases with no valid responses to any of the 18 food
security items are coded as missing -9 (not ascertained). X9FSRAW2 is based on 18 items, with a
potential range of 0 to 18, but in the round 9 data ranges from 0 to 17. X9FSADRA2 is the adult food
security raw score, which is a simple count of the number of household- and adult-referenced food
security items affirmed by the parent, and ranges from 0 to 10. X9FSCHRA is the children’s food security
raw score, which is a simple count of the number of child-referenced food security items affirmed by the
parent. It ranges from 0 to 8.
Responses to items skipped because of screening are assumed to be negative for the purpose
of creating the score. For cases that have some missing data but at least some valid responses, missing
responses were considered to be negatives. Cases with no valid responses to any of the 18 food security
items, or those with all -7 (refused) or -8 (don’t know) answers to P9WORRFD, P9FDLAST,
P9BLMEAL, P9LOWCST, P9NOBAL, and P9CANTAF are coded as missing -9 (not ascertained).
Definitions for negative and affirmed values of food security items are shown in exhibit 7-5.
7-56Exhibit 7-5. Definitions of negative and affirmed values for the food security items in the ECLS-K:2011
kindergarten–fifth grade restricted-use data file
Question number Negative responses (coded 0) Affirmative responses (coded 1)
FDQ130A FDQ130B FDQ130C FDQ140 FDQ150 FDQ160 FDQ170 FDQ180 FDQ190 FDQ191 FDQ192A FDQ192B FDQ192C FDQ210 FDQ240 FDQ242 FDQ243 FDQ250 3 (never true) 3 (never true) 3 (never true) 2 (no); or screened out in previous
questions
3 (only 1 or 2 months; FDQ140 = 2; or
screened out in previous questions
2 (no); or screened out in previous
questions
2 (no); or screened out in previous
questions
2 (no); or screened out in previous
questions
2 (no); or screened out in previous
questions
3 (only 1 or 2 months); FDQ190 = 2; or
screened out in previous questions
3 (never true) 3 (never true) 3 (never true) 2 (no); or screened out in previous
questions
2 (no); or screened out in previous
questions
2 (no); or screened out in previous
questions
3 (only 1 or 2 months); FDQ242 = 2; or
screened out in previous questions
2 (no); or screened out in previous
questions
1 (often true); or 2 (sometimes true)
1 (often true); or 2 (sometimes true)
1 (often true); or 2 (sometimes true)
1 (yes)
1 (almost every month); or 2 (some
months, but not every month)
1 (yes)
1 (yes)
1 (yes)
1 (yes)
1 (almost every month); or 2 (some
months, but not every month)
1 (often true); or 2 (sometimes true)
1 (often true); or 2 (sometimes true)
1 (often true); or 2 (sometimes true)
1 (yes)
1 (yes)
1 (yes)
1 (almost every month); or 2 (some
months, but not every month)
1 (yes)
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of
2010-11 (ECLS-K:2011), kindergarten-fifth grade (K-5) restricted-use data file.
7.5.2.10.2 Food Security Status: Continuous Measures (X9FSSCAL2, X9FSADSC2, and
X9FSCHSC)
X9FSSCAL2 is the scale score presentation of the household food security items. It is a
continuous, interval-level measure of food insecurity and is appropriate for linear models, such as
correlation, regression, or analysis of variance. This scale score is a Rasch transformation of the raw score
(X9FSRAW2). Valid values range from 1.4 to 12.2, with higher values indicating more severe food
deprivation. Under Rasch-model assumptions, the scale score for households that affirm no items (raw
7-57score = 0) is undefined. It is less than the lowest measured value (1.4), but its precise value is unknown
and may vary substantially among households. X9FSSCAL2 for such cases is assigned a value of -6.
These households are food secure, but the appropriate size of the interval between their score and the
score of households that affirmed one item is not known and varies from household to household. If these
cases (a substantial majority of all cases) are included in linear models, appropriate methods must be
used. If the food security scale score is a predictor variable, a value of 0 may be assigned to cases with a
raw score of 0 and a dummy variable added to identify households with a raw score of 0.
X9FSADSC2 is the adult food security scale score. This is a measure of the severity of food
insecurity experienced by adults in the household in the previous 12 months. It is a continuous, interval-
level measure based on the Rasch measurement model and is appropriate for linear models, such as
correlation, regression, or analysis of variance. It is on the standard (logistic-unit) metric described in
Guide to Measuring Household Food Security, Revised 2000 (U.S. Department of Agriculture 2000) (for
households without children). Valid values range from 1.7 to 11.1, with higher values indicating more
severe food deprivation. The scale score is undefined for households that affirmed no adult-referenced
items and is coded -6 (see discussion of X9FSSCAL2 above).
X9FSCHSC is the children’s food security scale score. This is a measure of the severity of
food insecurity experienced by children in the household in the previous 12 months. It is a continuous,
interval-level measure based on the Rasch measurement model and is appropriate for linear models, such
as correlation, regression, or analysis of variance. It is on the standard (logistic-unit) metric described in
Measuring Children’s Food Security in U.S. Households, 1995–99 (U.S. Department of Agriculture
2002). Valid values range from 4.1 to 12.2, with higher values indicating more severe food deprivation.
The scale score is undefined for households that affirmed no child-referenced items and is coded -6 (see
discussion of X9FSSCAL2 above).
7.5.2.10.3 Food Security Status: Categorical Measures (X9FSSTAT2, X9FSADST2, and
X9FSCHST)
X9FSSTAT2 is a categorical measure of household food security status based on the
household’s food security raw score, X9FSRAW2. X9FSSTAT2 assigns households into one of three
ordered categories: food secure (raw scores 0-2), having low food security (raw scores 3-7), and having
very low food security (raw scores of 8 or more). The two categories “low food security” and “very low
food security” together make up the more general category, food insecurity. X9FSSTAT2 is appropriate
7-58for comparing percentages of households with food insecurity or very low food security across
subpopulations and can be used as a categorical variable in associative models.
X9FSADST2 is a categorical measure of adults’ food security status based on the
household’s adult food security raw score, X9FSADRA2. X9FSADST2 identifies households as food
secure (raw scores 0-2), having low food security among adults (raw scores 3-5), or having very low food
security among adults (raw scores of 6 or more). Users may combine the latter two categories as
indicating food insecurity among adults. This variable is appropriate for comparing percentages of
households with food insecurity among adults and very low food security among adults across
subpopulations.
X9FSCHST is a categorical measure of children’s food security status based on the
children’s food security raw score, X9FSCHRA. X9FSCHST identifies households as having only food
secure children (raw scores 0-1), having low food security among children (raw scores 2-4), or having
very low food security among children (raw scores 5-8). The two categories “low food security among
children” and “very low food security among children” together make up the more general category, food
insecurity among children (alternatively described as “households with food insecure children”).
X9FSCHST is appropriate for comparing percentages of households with food insecurity among children
and very low food security among children across subpopulations. When interpreting children’s food
security statistics, users should remember that these variables represent the most severe food insecurity
experienced by any child in the household and may not reflect experiences of the child in the
ECLS-K:2011 study if there are other children—especially older children—in the household.
7.5.3 Teacher Composite Variables
In addition to the teacher data flags discussed in section 7.4.3 above, there are several
composite variables on the file that use data from teachers. For example, there are composite variables
created from teacher reports about the child’s approaches to learning and self-control (X9TCHAPP,
X9TCHCON). These two variables are described in chapter 3 (section 3.4.1), along with other variables
derived from teacher reports of children’s social skills. Other variables that use teacher data are about the
child’s grade level (e.g., X9GRDLVL) and are discussed above in section 7.5.1 about the child
composites.
7-597.5.4 School Composite Variables
Variables describing children’s school characteristics were constructed using data from the
teacher, the school administrator, and the sample frame. Details on how these variables were created are
provided below.
A change in approach to school composite variables was implemented starting in spring
2014 and this approach was also used in spring 2015 and spring 2016. ECLS-K:2011 data were
prioritized over school master file39 data in assigning values to school composites. As a result, data from
the school administrator questionnaire were used for the current round or the most recent available prior
round before using current school master file data to assign composite values.
Because many children move from one school to another over the course of the study, the
construction of school composites (e.g., school type) can be challenging when current-round data are
missing or when items are not asked in the current round if the school submitted an SAQ in a prior round.
Using the school value for a child from a prior round can be erroneous due to children moving. As a
result, many school composites are constructed by combining data across years at the school level,
calculating the composite value, and then assigning that value to participating children currently enrolled
in the school.
7.5.4.1 School Type (X9SCTYP)
In the spring of 2016, the questionnaire given to administrators (SAQ) contained a question
on school type that was used in the creation of the spring school type composite (X9SCTYP). School
master file data were used if school responses were not available from any ECLS-K:2011 round.
X9SCTYP was created as follows: If question A5 in the SAQ (“Which of the following
characterizes your school?”) was answered as “a regular public school (not including magnet school or
school of choice)” (S9REGPSK); “a public magnet school” (S9MAGSKL); or “a charter school”
(S9CHRSKL); the school was coded as “public.” If the question was answered as “a Catholic school” of
any type (S9CATHOL, S9DIOCSK, S9PARSKL, or S9PRVORS), the school was coded as “Catholic.” If
the question was answered as “other private school, religious affiliation” (S9OTHREL), the school was
39 The school master file was created for the ECLS-K:2011 from the Common Core of Data (CCD) for public schools, the Private School
Universe Survey (PSS) for private schools, and other data sources. It is updated regularly as new files from those surveys become available.
7-60coded as “other religious.” Otherwise, if the question was answered as “private school, no religious
affiliation” (S9OTNAIS, S9OTHRNO), then the school was coded as “other private.”
If data about school type were missing from the SAQ for the current round or prior rounds,
information about school type from the school master file (which included FMS and frame data) was used
to create X9SCTYP.40
Homeschooled children have a code of -1 (not applicable) on X9SCTYP.41 Children who
changed schools and were not followed and children who were not located in the spring of 2016 have a
code of -9 (not ascertained) for X9SCTYP. The variable X9SCTYP is set to system missing for children
who were not participants in the spring 2016 round. In addition, nonparticipants have a value of
990000000 on the variable F9CCDLEA.
7.5.4.2 Public or Private School (X9PUBPRI)
X9PUBPRI is a broad indicator of school type with only two categories—public and private.
X9PUBPRI, which is derived from the more detailed school type variable X9SCTYP described above,
has valid values for the 12,346 cases that have either child assessment or parent interview data in round 9.
This composite was created as follows: X9PUBPRI is coded 1 (public) if school type
indicated in X9SCTYP is 4 (public). X9PUBPRI is coded 2 (private) if school type indicated in
X9SCTYP is 1, 2, or 3 (Catholic, other religious, or other private). If the school identification number for
spring 2016 indicated that the child was homeschooled, then X9PUBPRI is coded -1 (not applicable).
X9PUBPRI is coded -9 (not ascertained) if data on school type are not available in the spring 2016 school
master file. X9PUBPRI is set to system missing for children who did not participate in round 9.
7.5.4.3 School Enrollment (X9ENRLS)
There is a composite variable in the data file (X9ENRLS) that indicates total school
enrollment on October 1, 2015 (or the date nearest to that date for which the school administrator had data
40 X9SCTYP and the round 8 and round 7 versions of the composite, X8SCTYP and X7SCTYP, respectively, are constructed differently than
previous versions of the same composite. For example, for the round 6 version of the composite, X6SCTYP, if spring 2013 school administrator
data were missing, previous round composite values for school type (X4PUBPRI, X2PUBPRI) were used. If those data were missing, data from
the school master file were used.
41 These children were enrolled in a school at the time of sampling in the base year, but were homeschooled during the spring of 2016.
7-61available). This total school enrollment composite was created using the school enrollment variable from
the school administrator questionnaire (S9ANUMCH). If school administrator data on total school
enrollment were missing for spring 2016, enrollment data were obtained from the most recent round of
the study with nonmissing school administrator data about school enrollment. If those data were missing,
information from the Private School Universe Survey (PSS) for private schools and from the Common
Core of Data (CCD) public school universe data for public schools were used.42 In all other cases the
variable is coded -9 (not ascertained).
7.5.4.4 Percent Non-White Students in the School (X9RCETH)
The composite variable X9RCETH indicates the percentage of the student population that
was not White in the spring of 2016.43 The composite is derived from a question in the school
administrator questionnaire (question A9 in SAQ) that asked the number or percentage of students in the
school who were the following race/ethnicities: Hispanic/Latino of any race; American Indian or Alaska
Native, not Hispanic or Latino; Asian, not Hispanic or Latino; Black or African American, not Hispanic
or Latino; Native Hawaiian or other Pacific Islander, not Hispanic or Latino; White, not Hispanic or
Latino; or two or more races, not Hispanic or Latino. The composite was calculated by summing the
percentages for all categories except White, not Hispanic or Latino.
School administrators were allowed to report their answers to the student racial/ethnic
composition questions as either numbers or percentages. All answers provided as numbers were converted
to percentages using the total enrollment variable S9TOTENR as the denominator before computing the
composite variable.44 The sum of the calculated percentages for each race/ethnicity category was allowed
42 X9ENRLS and the round 8 and 7 versions of the composite, X8ENRLS and X7ENRLS, respectively, are constructed differently than previous
versions of the same composite. For example, for the round 6 version of the composite, X6ENRLS, if spring 2013 school administrator data were
missing, X6ENRLS was set using school master file data. If those data were missing, data from previous round composites (X4KENRLS,
X2KENRLS) were used.
43 This variable was S*MINOR in the ECLS-K. In the ECLS-K:2011, there is a different variable factored into the composite that indicates the
percentage of students classified as “two or more races, non-Hispanic or Latino” (S*MULTPT).
44 There were five recoding rules used for data with apparent errors:
1. If answers were reported as numbers and the total number of students in the school (S9TOTENR) was missing, the total from another question
about total enrollment (Q2b S9ANUMCH) was used if the difference between the summed total of students in different race/ethnicity groups and
the reported Q2a total was within +/-5 percent of 100 percent (95–105 percent). For example, if the number of students in each race/ethnicity
group in the school added to 501 students, but the total number of students by race (S9TOTENR) was missing, and total enrollment from
S9ANUMCH was 500 students, the sum of the number of students in the race/ethnicity categories (501) would be 100.2 percent of the value of
500 reported in S9ANUMCH. The value of 100.2 percent is within the 95-105 percent range of allowed errors, so S9ANUMCH is used as the
denominator for calculating the percentage of students in each race/ethnicity category.
2. If the method of reporting was mixed (some as numbers, others as percentages), the race/ethnicity percentages were coded as -9 (not
ascertained).
3. If percentages were recorded, with none of the above errors, and the summed total across categories was within +/-5 percent of 100 percent
(95–105 percent) of the value in S9TOTENR, any race/ethnicity categories that the school administrator left blank were recoded to 0.
7-62to be within +/- 5 percent of 100 percent to allow for minor reporting errors of numbers that did not add to
the reported total or percentages that did not add to 100 percent. In a few cases, this procedure resulted in
a total sum of percentages that was slightly over 100 percent. Totals greater than 100 percent are top-
coded to 100 percent.
A flag for each individual race/ethnicity variable indicating whether the school administrator
reported the information as a number or a percent is included in the data file.45 Because the composite is
calculated as a percent, these flags will not be needed by users unless they are interested in examining
how answers were reported. If the flag (S9ASIAFL S9HISPFL, S9BLACFL, S9WHITFL, S9AIANFL,
S9HAWPFL, and S9MULTFL) for each of the race/ethnicity variables (S9ASIAPT, S9HISPPT,
S9BLACPT, S9WHITPT, S9AIANPT, S9HAWPPT, and S9MULTPT) is equal to 1, that indicates the
information was reported by the school administrator as a percentage, or was reported as both a number
and a percentage. If the flag (S9ASIAFL S9HISPFL, S9BLACFL, S9WHITFL, S9AIANFL,
S9HAWPFL, and S9MULTFL) for each of the race/ethnicity variables (S9ASIAPT, S9HISPPT,
S9BLACPT, S9WHITPT, S9AIANPT, S9HAWPPT, and S9MULTPT) is equal to 2, that indicates the
information was reported by the school administrator as a number.
In some cases, the composite could not be derived from the school administrator
questionnaire responses because some data used to compute it were missing or the data collected from
administrators appeared to be in error (e.g., if school administrators reported both numbers and percents
that were not consistent with one another and it was unclear which data were correct). If the composite
could not be derived from the spring 2016 data, the percentage of non-White students in the school was
obtained from school administrator questionnaire responses from spring 2015, spring 2014, spring 2013,
spring 2012, or spring 2011.46 If those data were also missing, the percentage of non-White students in
the school was obtained from the CCD (for public schools) or the PSS (for private schools). If those data
were also missing, X9RCETH is coded -9 (not ascertained). If the study child was homeschooled in the
spring of 2016, X9RCETH is coded -1 (not applicable).
4. If the summed total of students in race/ethnicity categories was not +/-5 percent of 100 percent (95–105) percent of the sum reported in
S9TOTENR or not 95–105 percent of total enrollment from another question (Q2b S9ANUMCH), the individually reported percentages and
numbers were made -9 (not ascertained).
5. If numbers were reported, with none of the above errors, and the summed total across categories was within +/- 5 percent of the reported total,
any race/ethnicity categories that the school administrator left blank were recoded to 0.
45 In addition to flags for race/ethnicity variables, there is another flag that indicates whether school administrators reported average daily
attendance as a number or percent. The flag is S9ADAFLG (average daily attendance reported as number or percent). If school administrators
reported both a number and a percent, the flag is recorded as a percent.
46 X9RCETH and the round 8 and round 7 versions of the composite, X8RCETH and X7RCETH, respectively, are constructed differently than
previous versions of the same composite. For example, for the round 6 version of the composite, X6RCETH, if spring 2013 school administrator
data were missing, X6RCETH was set using school frame data. If those data were missing, data from previous round composites (X4RCETH,
X2KRCETH) were used.
7-637.5.4.5 Highest and Lowest Grade at the School (X9LOWGRD, X9HIGGRD)
Composite variables indicate the lowest grade taught at the school (X9LOWGRD) and the
highest grade taught at the school (X9HIGGRD). They are derived from information collected from the
school administrator during the spring 2016 data collection or from the spring of 2015, spring of 2014,
spring of 2013, spring of 2012, or the spring of 2011. Variables X9LOWGRD and X9HIGGRD were
created by first coding answers of “ungraded” in question A5 (“Mark all grade levels included in your
school”) as category 15 (ungraded) and then coding the lowest grade in the school and the highest grade
in the school, respectively. The grade level for children in transitional kindergarten, kindergarten, or pre-
first grade is coded as category 2 (kindergarten). For schools with missing data for school grade levels,
the composites X9HIGGRD and X9LOWGRD were set to the values reported in previous school
administrator data in spring 2015, spring 2014, spring 2013, spring 2012, or spring 2011. Data from the
school master file were used if information about the highest and lowest grade at the school was not
collected in school administrator variables for any round.47
7.5.4.6 Students Eligible for Free or Reduced-Price School Meals (X9FRMEAL_I)
The composite variable X9FRMEAL_I indicates the percent of students in the school who
were approved for free or reduced-price school meals (X9FRMEAL_I). This composite has valid values
for the 12,346 cases that have either child assessment or parent interview data in round 9. This composite
differs from the school meal composites created in for the spring of 2011 and the spring of 2012
(X2FLCH2_I, X2RLCH2_I, X4FMEAL_I, and X4RMEAL_I) because the spring 2016 school
administrator questionnaire, like the spring 2015 and spring 2014 school administrator questionnaires, did
not include questions on U.S. Department of Agriculture (USDA) program participation or the numbers
of students eligible for free and reduced priced meals (breakfast or lunch) that were used as the sources of
the composite variables for spring 2011 and spring 2012. However, in the spring of 2016 and in previous
rounds of the study, school administrators were asked for the percentage of children eligible for free or
reduced-price lunch. This question and several other sources of information were used to create
X9FRMEAL_I. Specifically, X9FRMEAL_I is derived from the percentage of children eligible for free
or reduced- price lunch reported by the school administrator during the spring 2016 data collection, or
47 X9LOWGRD and X9HIGGRD, and the round 8 and 7 versions of the composites, X8LOWGRN, X8HIGGRD, X7LOWGRD, and
X7HIGGRD, are constructed differently than previous versions of the same composites. For example, for the round 6 versions of the composites,
X6LOWGRD and X6HIGGRD, if spring 2013 school administrator data were missing, previous round composite (X4HIGGRD and
X4LOWGRD, X2HIGGRD, and X2LOWGRD) values were used to set the composites. If those data were missing, data from the school master
file were used.
7-64imputed if the item was missing from the SAQ, using information collected from school administrators in
the spring of 2015, spring of 2014, the spring of 2013, the spring of 2012, or the spring of 2011, frame
variables or hot deck imputation.48 For schools where no SAQ was received for spring 2016 (and
therefore SAQ missing values were not imputed), the composite was completed by assigning, in the
following order, a value from prior rounds of the study, the school master file, or hot deck imputation. 49
X9FRMEAL_I, based on school administrator data about children eligible for free or reduced-price lunch,
was imputed with information from previous rounds about students eligible for free or reduced-price
meals because children are approved for free or reduced-price meals generally, not just for lunch.
Children who were homeschooled have X9FRMEAL_I set to -1.
The percent of children reported by school administrators in spring 2016 to be eligible for
free or reduced-price lunch (S9PCTFLN_I) was used as the first source of data for X9FRMEAL_I. There
are three schools that appear to have reported a number of students rather than a percentage in
S9PCTFLN_I; their values were retained for the composite and a flag (X9FRMEALFLG) can be used to
identify them. A fourth school also has a value for X9FRMEAL_I that appears to be based on number of
students rather than a percentage. This school’s value comes from the school’s report in round 8
(S8PCTFLN_I). S9PCTFLN_I was imputed for all cases that had child assessment or parent interview
data in the spring 2016 round and a completed SAQ, but for which the administrator did not provide free
and reduced-price lunch information. Table 7-7 shows the level of missing data for the school
administrator variable for the percent of children who were eligible for free or reduced-price lunch
(S9PCTFLN) among the schools that had at least one child or parent respondent in the spring 2016 data
collection.
Table 7-7. Number and percent of public and private schools and study students with missing data for
the percent of children in the school eligible for free or reduced-price lunch (S9PCTFLN):
Spring 2016
School meal composite
Number
missing
Percent
missing
Number of students
in these schools
Percent of students
with missing values
Percent eligible for free or
reduced-price meal 74 3.3 350 3.4
SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class
of 2010–11 (ECLS-K:2011), spring 2016.
48 Both public schools and nonprofit private schools are eligible for the National School Lunch Program (NSLP).
49 X9FRMEAL_I and the round 8 and round 7 versions of the composite, X8FRMEAL_I and X7FRMEAL_I, are constructed differently than
previous versions of the same composite. For example, for the round 6 version of the composite, X6FRMEAL_I, data from the imputed spring
2013 school administrator questionnaire were used first to set the composite value, followed by variables in the following order of priority:
unimputed school administrator data from the most recent previous round of the study available, data from the school master file, the sum of the
spring 2012 composite for free school meals added to the spring 2012 composite for reduced-price school meals, and then the sum of the spring
2011 composite for free school meals added to the spring 2011 composite for reduced-price school meals. Finally, if X6FRMEAL_I did not have
an assigned value following each of the above steps, the remaining missing values were imputed using hot deck imputation at the composite
level.
7-65The imputation flag IFS9PCTFLN indicates whether the school administrator questionnaire
variable S9PCTFLN_I was longitudinally imputed using spring 2015, spring 2014, spring 2013, spring
2012, or spring 2011 data, was filled with data from the CCD, was imputed using the hot deck method, or
was not imputed. For cases with missing data on S9PCTFLN, longitudinal imputation was used first, if
possible, taking a value from school administrator data in a previous round for the same school in spring
2015 (S8PCTFLN_I), spring 2014 (S7PCTFLN_I), spring 2013 (S6PCTFLN_I), spring 2012
(S4PCTFLN), or spring 2011 (S2LUNCH). If historical survey data were not available, then data from
the CCD were used to impute for these missing S9PCTFLN_I values for public schools. The PSS does
not have data on school meals that can be used to compute an imputed value for S9PCTFLN_I. If CCD
data were not available, then the values of the meal composites from previous rounds were used to
compute an imputed value for S9PCTFLN_I, where available, with the imputed value computed as
X8FRMEAL_I, if this was available, X7FRMEAL_I, if this was available, X6FRMEAL_I, if this was
available, the sum of X4FMEAL_I and X4RMEAL_I, if these were available, and otherwise the sum of
X2FLCH2_I and X2RLCH2_I, if available.
If S9PCTFLN_I was still missing after data from previous rounds and the CCD were used, it
was imputed using the hot deck method described above in section 7.5.2.8. Hot-deck imputation was done
at the school level and the imputed value was then assigned to each child in the school. In hot-deck
imputation, a school with a non-missing value for a component has this value assigned or “donated” to a
similar school with a missing value for the component. Schools are similar if they belong in the same
imputation cell. Imputation cells were created using district poverty category (created from the district
poverty variable X9DISTPOV described in section 7.5.7), census region, school type, the percentage of
students in minority ethnic groups, whether the school received Title I funding, and school size (total
enrollment).
Cases that did not have any data from the school administrator questionnaire in the spring of
2016 did not have a value for S9PCTFLN_I to set the value of the composite X9FRMEAL_I, so other
sources were used to assign a value for the composite. X9FRMEAL_I was set to the percentage of
students in the child’s current school eligible for free or reduced-price lunch reported by the school
administrator in the spring of 2015 (S8PCTFLN_I), if those data were available, spring of 2014
(S7PCTFLN_I), if those data were available, spring of 2013 (S6PCTFLN_I), if those data were available,
or the spring of 2012 (S4PCTFLN), if those data were available. If spring 2012 data were not available
but data from the spring of 2011 (S2LUNCH) were, the 2011 data were used. Otherwise, if the school
master file had data for the school’s total enrollment, the number of children approved for free meals, and
the number of children approved for reduced-price meals, X9FRMEAL_I, was set to the percentage of
children approved for free meals plus the percentage of children approved for reduced-price meals.
7-66Finally, if X9FRMEAL_I did not have an assigned value following each of the above steps,
the remaining missing value was imputed using hot deck imputation at the composite level. The
imputation flag IFX9FRMEAL indicates whether X9FRMEAL_I was imputed using the hot deck
method, or was not imputed.
In some cases, the children’s schools are unknown because the child was unlocatable or the
child moved to a nonsampled county and was not followed into his/her new school, but a parent interview
was completed. In such cases, data were not imputed for X9FRMEAL_I because no information about the
school was available (e.g., public or private control, school size, or even if the child was enrolled in a
school). X9FRMEAL_I is coded as -9 for these cases.
7.5.4.7 Geographic Region and Locality of the Child’s School (X9REGION, X9LOCALE)
Composite variables indicating the geographic region (X9REGION) and locality type
(X9LOCALE) of the child’s school come from the PSS for private schools and the CCD for public
schools. For the spring 2016 geographic region composite, X9REGION, if the geographic region is
missing in the PSS and CCD files, then the state in which the school was located was used to assign
region. If those data are missing and the geographic region for the school was identified in an earlier
round, the composite was set to the value from the most recent round (as reported in X8REGION,
X7REGION, X6REGION, X4REGION, X2REGION, or X1REGION).50 Values for X9REGION are the
following:
1 = 2 = 3 = Northeast: CT, ME, MA, NH, RI, VT, NJ, NY, PA;
Midwest: IL, IN, MI, OH, WI, IA, KS, MN, MO, NE, ND, SD;
South: DE, DC, FL, GA, MD, NC, SC, VA, WV, AL, KY, MS, TN, AR, LA, OK, TX;
and
4 = West: AZ, CO, ID, MT, NV, NM, UT, WY, AK, CA, HI, OR, WA.
X9REGION is coded -9 (not ascertained) for children who were unlocatable or moved out of
a sampled county and were not followed to new schools in the spring of 2016, but for whom there are
50 X9REGION and the round 8 and round 7 versions of the composite, X8REGION and X7REGION, are constructed differently from all
previous versions of the same composite. Although X9REGION, X8REGION, and X7REGION use the same data sources that were used to
construct the composite in previous rounds, the order of the data sources used is different in these rounds than in previous rounds. For example,
for the round 6 version of the composite, X6REGION, the state in which the school was located was used as a final step in assigning the
composite value, if data from the CCD or PSS files and geographic location from a previous round (X4REGION, X2REGION, or X1REGION)
were not available.
7-67parent interview data. Children who were homeschooled in the spring of 2016 have a code of -1 on
X9REGION. X9REGION is set to system missing for those who did not participate in round 9.
For the spring 2016 school locality variable, X9LOCALE, the categories correspond to the
2006 NCES system for coding locale (https://nces.ed.gov/surveys/ruraled/definitions.asp). If data are not
available for the child’s school from the PSS or CCD, and locale data were available from an earlier
round, the composites were set to the value from the most recent round (X8LOCALE, X7LOCALE,
X6LOCALE, X4LOCALE, X2LOCALE, or X1LOCALE). Otherwise, the composites are coded -9 (not
ascertained). Some -9 (not ascertained) values for X9LOCALE are associated with cases in which
children who moved were unlocatable or moved out of a sampled county and were not followed to new
schools in spring 2016, but for whom there are parent interview data. Children who were homeschooled
in spring 2016 are coded as -1 on X9LOCALE. X9LOCALE is set to system missing for those who did
not participate in round 9. Values for X9LOCALE are the following:
11 - City, Large: Territory inside an urbanized area and inside a principal city with
population of 250,000 or more;
12 - City, Midsize: Territory inside an urbanized area and inside a principal city with
population less than 250,000 and greater than or equal to 100,000;
13 - City, Small: Territory inside an urbanized area and inside a principal city with
population less than 100,000;
21 - Suburb, Large: Territory outside a principal city and inside an urbanized area with
population of 250,000 or more;
22 - Suburb, Midsize: Territory outside a principal city and inside an urbanized area with
population less than 250,000 and greater than or equal to 100,000;
23 - Suburb, Small: Territory outside a principal city and inside an urbanized area with
population less than 100,000;
31 - Town, Fringe: Territory inside an urban cluster that is less than or equal to 10 miles
from an urbanized area;
32 - Town, Distant: Territory inside an urban cluster that is more than 10 miles and less
than or equal to 35 miles from an urbanized area;
33 - Town, Remote: Territory inside an urban cluster that is more than 35 miles from an
urbanized area;
41 - Rural, Fringe: Census-defined rural territory that is less than or equal to 5 miles from
an urbanized area, as well as rural territory that is less than or equal to 2.5 miles from an
urban cluster;
7-6842 - Rural, Distant: Census-defined rural territory that is more than 5 miles but less than or
equal to 25 miles from an urbanized area, as well as rural territory that is more than 2.5 miles
but less than or equal to 10 miles from an urban cluster; and
43 - Rural, Remote: Census-defined rural territory that is more than 25 miles from an
urbanized area and is also more than 10 miles from an urban cluster.
Some schools have different values for X*LOCALE between the base year and subsequent
rounds. The differences in values reflect changes in the PSS or CCD source data.
The classification of locale has undergone some changes since the ECLS-K study conducted
with children in the kindergarten class of 1998-99. Information on these changes is available on the
NCES website at https://nces.ed.gov/programs/edge/docs/EDGE_NCES_LOCALE_2015.pdf.
7.5.5 Field Management System (FMS) Composite Variables
Several composite variables were created from data stored in the FMS, which were obtained
from frame data as well as by field staff during visits to the schools and discussions with school staff.
7.5.5.1 School Year Start and End Dates (X9SCHBDD, X9SCHBMM, X9SCHBYY,
X9SCHEDD, X9SCHEMM, X9SCHEYY)
The composite variables indicating school year start and end dates, which are listed below,
were derived from information contained in the FMS.
 X9SCHBDD – X9 School Year Starting Date, Day;
 X9SCHBMM – X9 School Year Starting Date, Month;
 X9SCHBYY – X9 School Year Starting Date, Year;
 X9SCHEDD – X9 School Year Ending Date, Day;
 X9SCHEMM – X9 School Year Ending Date, Month; and
 X9SCHEYY – X9 School Year Ending Date, Year.
The composite variables for beginning and ending school dates are derived differently in spring 2016,
spring 2015, and spring 2014 than in previous rounds. In previous rounds of the study, the school
7-69administrator questionnaire data were used as the first source of data for creating the composites,
followed by the use of FMS data if the questionnaire data were missing. In spring 2016, spring 2015, and
spring 2014, the school administrator questionnaire did not include a question about beginning and ending
school dates, so the FMS was used to derive the composites.
7.5.5.2 Year-Round Schools (X9YRRND)
The year-round school composite variable is based on information obtained from the school
staff member who helps coordinate the data collection activities in the school (referred to as the school
coordinator) about whether a school is a year-round school. This composite has valid values for the
12,346 cases that have child assessment or parent interview data in round 9. The values for this composite
variable are 1 (year-round school) and 0 (not year-round school). If the child was homeschooled in the
spring of 2016, the composite is coded as -1 (not applicable). If these data were not obtained in the spring
of 2016 but information about being a year-round school was collected in an earlier round, the composite
was set to the value from the most recent round (X8YRRND, X7YRRND, X6YRRND, X4YRRND, or
X12YRRND).
7.5.6 School District Poverty (X9DISTPOV)
X9DISTPOV is a district-level indicator of the percentage of children age 5–17 in a school
district who are in poverty. It is derived from the 2015 Small Area Income & Poverty Estimates (SAIPE)
and is computed as the estimated number of children 5–17 years old in poverty divided by the estimated
population of children 5–17 years old in the district multiplied by 100 and rounded to 0 decimals. The
school district boundaries were based on the 2015 school district mapping survey that included school
districts as of January 1, 2016 and reflect district boundaries for the 2015–16 school year (U.S. Census
Bureau n.d.). There are 123 ECLS-K:2011 public schools with a missing value for X9DISTPOV because
the values were missing in the SAIPE source data.51
7.6 Methodological Variables
To facilitate methodological research, variables pertaining to aspects of the data collection
work were extracted from the FMS and included in the data file. These include disposition codes for the
51 Children who attended private school or were homeschooled were coded as -1 (inapplicable).
7-70child assessment and parent interview (F9CADISP and F9PIDISP, respectively); Federal Information
Processing Standards (FIPS) codes for school counties and states (F9FIPSCT and F9FIPSST,
respectively); school zip and census tract codes (F9SCHZIP and F9CENTRC, respectively); identifiers
for public schools (F9CCDLEA, F9CCDSID); an identifier for private schools (F9SCHPIN); identifiers
for parent interview work area (F9PWKARE); parent interviewer identification number (F9PINTVR); the
month the parent interview was conducted (F9INTVMM); the year the parent interview was conducted
(F9INTVYY); child assessment work area (F9CWKARE); and child assessor identification number
(F9CASSOR). A “work area” is the group of schools that each team leader was assigned. Team leaders
managed a group of one to four other individuals who worked as child assessors and parent interviewers
for the sampled cases in the work area. If a case was not assigned to an interviewer (e.g., a child who
moved and was not followed), then F9PINTVR is system missing. Similarly, if a case was not assigned to
an assessor, then F9CASSOR is system missing.
7-71This page intentionally left blank.8. ELECTRONIC CODEBOOK (ECB)
8.1 Introduction
This chapter provides specific instructions for using the Early Childhood Longitudinal Study,
Kindergarten Class of 2010–11 (ECLS-K:2011) Electronic Codebook (ECB). The functionality of the ECB,
which is the same throughout the ECLS studies, is fully described in the Help File for the ECLS-K:2011
longitudinal kindergarten–fifth grade (K–5) ECB. The information in the ECB’s Help File provides a
comprehensive tour through the ECB and addresses all of the functions and capabilities of the program.
These functions allow users to access the accompanying data catalog and view the data in various ways by
performing customized searches and extractions. Using the ECB, the data user can create SAS, SPSS for
Windows, and Stata syntax programs that can be run to generate an extract data file from the text
(ASCII) data file.
8.1.1 Hardware and Software Requirements
The ECB program is designed to run under Windows 95®, Windows 98®, Windows 2000®
,
Windows XP®
, or Windows NT® 4.0 on a Pentium-class or higher personal computer (PC). The ECB has
been successfully tested using current versions of Windows Vista and Windows 7. It has not been tested on
Windows 10. The ECB is not designed for use on Apple Macintosh systems, but Mac users can create a
data file using the file record layout.
The PC should have a minimum of 20 megabytes of available disk space. The program will
fit best visually on screens set to a desktop area of 1024 x 768 pixels. It will still work on other screen
settings, but it may not make the best use of the available screen space. If you have a Windows NT® or
earlier operating system, you can check or set your desktop area as follows:
1. Click the Windows Start button.
2. Select the Settings menu and then the Control Panel folder icon.
3. In the Control Panel window, click the Display icon.
4. Select the Settings tab.
5. Set the Desktop Area to 1024 x 768 pixels with the Desktop Area slidebar.
8-1If you have a Windows Vista or Windows 7® operating system, you can check or set your
desktop area as follows:
1. 2. 3. 4. 5. Click the Windows Start Button.
Select the Control Panel tab.
In the Control Panel window, click the Display icon.1
Select the Change display settings tab.
Set the Desktop Area2 to 1024 x 768 pixels with the Desktop Area slidebar.
As noted above, the ECB requires approximately 20 megabytes of available disk space on
your hard drive. If 20 megabytes of space is not available, you may wish to delete unnecessary files from
the drive to make space for the ECB.
8.2 Installing, Starting, and Exiting the ECB
The ECB is intended to be installed and run from within the Windows 95®, Windows 98®
,
Windows 2000®, Windows XP®
, Windows NT® 4.0, Windows Vista, or Windows 7® environment. The
sections in this chapter provide you with step-by-step instructions for installing the program on your PC,
starting the program, and exiting the program once you have completed your tasks.
8.2.1 Installing the ECB Program on Your Personal Computer
Program installation is initiated by running the “InstallECLSECB.exe” executable file.
How to Install the Program
1. Close all applications on your computer.
2. Run program “InstallECLSECB.exe.”
1 The Display icon is reached through the Appearance and Personalization section of the Control Panel on a Windows 7 operating system.
2 In Windows 7, the Desktop Area is called Resolution.
8-2Depending on your PC’s configuration, you may encounter warning messages during
installation. To respond, always keep the newer version of a file being copied and ignore any access
violations that occur during file copying.
If you are installing multiple ECBs (not different versions of the same ECB) on your PC, you
may receive a message warning that Setup is about to replace pre-existing files. To respond, always opt to
continue the installation although the default is to cancel the setup. When you get a follow-up message to
confirm whether the installation should be continued, press Yes to continue, although the default is No.
3. The screen shown in exhibit 8-1 indicates that the setup is being prepared.
Exhibit 8-1. InstallShield Wizard
4. You will be prompted to continue with the installation in the Welcome window shown
in exhibit 8-2. Click the Next button to continue.
Exhibit 8-2. Welcome window
8-35. When you continue, you will be prompted to choose a destination location for the
installation in the window shown in exhibit 8-3. If you wish to change the destination
location, click the Browse button to change the directory. Click the Next button when
the desired destination folder is shown.
Exhibit 8-3. Choose Destination Location
6. Exhibit 8-4. Setup Status
Setup will then start installing files. Exhibit 8-4 shows the setup status.
8-47. Once the installation is completed, the InstallShield Wizard Complete window shown
in exhibit 8-5 will appear. Click the Finish button to finish the process and return to
your PC’s desktop.
Exhibit 8-5. InstallShield Wizard Complete
8. The installation process should take about a minute, depending on the speed of the
computer on which the ECB is being installed.
8.2.2 How to Start the ECB
On the desktop screen, click the ECB desktop icon (exhibit 8-6a) shown below to initiate the
program. Alternatively, on the desktop screen, click the Start button and then point to Programs (exhibit 8-
6b). Click the ECB title to start the program. In Windows 7, click the Start button, click on All Programs,
and click the ECB title to start the program.
Exhibit 8-6a. Desktop icon
8-5Exhibit 8-6b. Desktop screen—click start
Exhibit 8-7. If you are a first-time user of the ECB, exhibit 8-7 will appear and ask if you are a new user.
First-time user dialog box
Click Yes if you are a first-time user. The ECB splash screen shown in exhibit 8-8 will appear.
8-6Exhibit 8-8. ECB splash screen
On the Select Catalog screen (exhibit 8-9), highlight the name of the catalog. (The
ECLS-K:2011 has only one catalog.)
Exhibit 8-9. Select Catalog screen
Click OK to open the main ECB screen, shown in exhibit 8-10.
8-7Exhibit 8-10. Main ECB screen
You are now ready to use the functions of the ECB as described in the ECB Help File.